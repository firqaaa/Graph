{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Utils"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def uniform(shape, scale=0.5, name=None):\n",
    "    \"\"\"Uniform init.\"\"\"\n",
    "    initial = tf.random.uniform(shape, minval=-scale, maxval=scale, dtype=tf.float64)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def glorot(shape, name=None):\n",
    "    \"\"\"Glorot & Bengio init.\"\"\"\n",
    "    init_range = np.sqrt(6.0/(shape[0]+shape[1]))\n",
    "    initial = tf.random.uniform(shape=shape, minval=-init_range, maxval=init_range, dtype=tf.float64)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def zeros(shape, name=None):\n",
    "    \"\"\"All zeros\"\"\"\n",
    "    initial = tf.zeros(shape, dtype=tf.float64)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def ones(shape, name=None):\n",
    "    \"\"\"All ones\"\"\"\n",
    "    initial = tf.ones(shape, dtype=tf.float64)\n",
    "    return tf.Variable(initial, name=name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def masked_softmax_cross_entropy(preds, labels, mask):\n",
    "    \"\"\"Softmax cross-entropy loss with masking\"\"\"\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    loss *= mask\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def masked_accuracy(preds, labels, mask):\n",
    "    \"\"\"Accuracy with masking\"\"\"\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1))\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    accuracy_all *= mask\n",
    "    return tf.reduce_mean(accuracy_all)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import eigsh\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def load_corpus(dataset_str):\n",
    "    \"\"\"\n",
    "    Loads input corpus from gcn/data directory\n",
    "    ind.dataset_str.x => the feature vectors of the training docs as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.tx => the feature vectors of the test docs as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training docs/words\n",
    "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.y => the one-hot labels of the labeled training docs as numpy.ndarray object;\n",
    "    ind.dataset_str.ty => the one-hot labels of the test docs as numpy.ndarray object;\n",
    "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
    "    ind.dataset_str.adj => adjacency matrix of word/doc nodes as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.train.index => the indices of training docs in original doc list.\n",
    "    All objects above must be saved using python pickle module.\n",
    "    :param dataset_str: Dataset name\n",
    "    :return: All data input files loaded (as well the training/test data).\n",
    "    \"\"\"\n",
    "\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'adj']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open('cleaned_data/' + dataset_str + '/graph/ind.' + dataset_str + '.' + names[i], 'rb') as f:\n",
    "            # with open(\"./data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, adj = tuple(objects)\n",
    "    # print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    labels = np.vstack((ally, ty))\n",
    "    # print(len(labels))\n",
    "\n",
    "    train_idx_orig = parse_index_file('cleaned_data/' + dataset_str + '/graph/' + dataset_str + '.train.index')\n",
    "    train_size = len(train_idx_orig)\n",
    "\n",
    "    val_size = train_size - x.shape[0]\n",
    "    test_size = tx.shape[0]\n",
    "\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y) + val_size)\n",
    "    idx_test = range(allx.shape[0], allx.shape[0] + test_size)\n",
    "\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size\n",
    "\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "\n",
    "    return sparse_to_tuple(features)\n",
    "\n",
    "\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "\n",
    "\n",
    "def construct_feed_dict(features, support, labels, labels_mask, placeholders):\n",
    "    \"\"\"Construct feed dictionary.\"\"\"\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['labels']: labels})\n",
    "    feed_dict.update({placeholders['labels_mask']: labels_mask})\n",
    "    feed_dict.update({placeholders['features']: features})\n",
    "    feed_dict.update({placeholders['support'][i]: support[i]\n",
    "                      for i in range(len(support))})\n",
    "    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def loadWord2Vec(filename):\n",
    "    \"\"\"Read Word Vectors\"\"\"\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    word_vector_map = {}\n",
    "    file = open(filename, 'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        if(len(row) > 2):\n",
    "            vocab.append(row[0])\n",
    "            vector = row[1:]\n",
    "            length = len(vector)\n",
    "            for i in range(length):\n",
    "                vector[i] = float(vector[i])\n",
    "            embd.append(vector)\n",
    "            word_vector_map[row[0]] = vector\n",
    "\n",
    "    print('Loaded Word Vectors!')\n",
    "    file.close()\n",
    "    return vocab, embd, word_vector_map\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocess"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\firqa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "# Remove words\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "#\n",
    "# if len(sys.argv) != 2:\n",
    "#     sys.exit(\"Use: python remove_words.py <dataset>\")\n",
    "\n",
    "dataset = \"R8\"\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "doc_content_list = []\n",
    "if dataset == 'CHINESE' or dataset == 'THUCTC':\n",
    "    with open('./cleaned_data/' + dataset + '/corpus/' + dataset + '.txt', 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            doc_content_list.append(line.strip())\n",
    "else:\n",
    "    with open('./cleaned_data/' + dataset + '/corpus/' + dataset + '.txt', 'rb') as f:\n",
    "        for line in f.readlines():\n",
    "            doc_content_list.append(line.strip().decode('latin1'))\n",
    "\n",
    "word_freq = {}  # to remove rare words\n",
    "\n",
    "for doc_content in doc_content_list:\n",
    "    if dataset == 'CHINESE' or dataset == 'THUCTC':\n",
    "        temp = doc_content\n",
    "    else:\n",
    "        temp = clean_str(doc_content)\n",
    "\n",
    "    words = temp.split()\n",
    "    for word in words:\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "\n",
    "clean_docs = []\n",
    "for doc_content in doc_content_list:\n",
    "    if dataset == 'CHINESE' or dataset == 'THUCTC':\n",
    "        temp = doc_content\n",
    "    else:\n",
    "        temp = clean_str(doc_content)\n",
    "    words = temp.split()\n",
    "    doc_words = []\n",
    "    for word in words:\n",
    "        # word not in stop_words and word_freq[word] >= 5\n",
    "        if dataset == 'mr':\n",
    "            doc_words.append(word)\n",
    "        elif word not in stop_words and word_freq[word] >= 5:\n",
    "            doc_words.append(word)\n",
    "\n",
    "    doc_str = ' '.join(doc_words).strip()\n",
    "    clean_docs.append(doc_str)\n",
    "\n",
    "clean_corpus_str = '\\n'.join(clean_docs)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/' + dataset + '_clean.txt', 'w') as f:\n",
    "    f.write(clean_corpus_str)\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size 7674\n",
      "adding...\n",
      "building...\n",
      "(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)\n"
     ]
    }
   ],
   "source": [
    "## Build Graph\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from math import log\n",
    "from sklearn import svm\n",
    "import scipy.sparse as sp\n",
    "from nltk.corpus import wordnet as wn\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "dataset = \"R8\"\n",
    "\n",
    "word_embeddings_dim = 300\n",
    "word_vector_map = {}\n",
    "\n",
    "# shuffling\n",
    "doc_name_list = []\n",
    "doc_train_list = []\n",
    "doc_test_list = []\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/' + dataset + '.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        doc_name_list.append(line.strip())\n",
    "        temp = line.split(\"\\t\")\n",
    "        if temp[1].find('test') != -1:\n",
    "            doc_test_list.append(line.strip())\n",
    "        elif temp[1].find('train') != -1:\n",
    "            doc_train_list.append(line.strip())\n",
    "\n",
    "doc_content_list = []\n",
    "with open('./cleaned_data/' + dataset + '/' + dataset + '_clean.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        doc_content_list.append(line.strip())\n",
    "\n",
    "train_ids = []\n",
    "for train_name in doc_train_list:\n",
    "    train_id = doc_name_list.index(train_name)\n",
    "    train_ids.append(train_id)\n",
    "\n",
    "random.shuffle(train_ids)\n",
    "\n",
    "train_ids_str = '\\n'.join(str(index) for index in train_ids)\n",
    "with open('./cleaned_data/' + dataset + '/graph/' + dataset + '.train.index', 'w') as f:\n",
    "    f.write(train_ids_str)\n",
    "\n",
    "test_ids = []\n",
    "for test_name in doc_test_list:\n",
    "    test_id = doc_name_list.index(test_name)\n",
    "    test_ids.append(test_id)\n",
    "\n",
    "random.shuffle(test_ids)\n",
    "\n",
    "test_ids_str = '\\n'.join(str(index) for index in test_ids)\n",
    "with open('./cleaned_data/' + dataset + '/graph/' + dataset + '.test.index', 'w') as f:\n",
    "    f.write(test_ids_str)\n",
    "\n",
    "ids = train_ids + test_ids\n",
    "\n",
    "print(\"data size {}\".format(len(ids)))\n",
    "\n",
    "print(\"adding...\")\n",
    "shuffle_doc_name_list = []\n",
    "shuffle_doc_words_list = []\n",
    "for id in ids:\n",
    "    shuffle_doc_name_list.append(doc_name_list[int(id)])\n",
    "    shuffle_doc_words_list.append(doc_content_list[int(id)])\n",
    "shuffle_doc_name_str = '\\n'.join(shuffle_doc_name_list)\n",
    "shuffle_doc_words_str = '\\n'.join(shuffle_doc_words_list)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/' + dataset + '_shuffle.txt', 'w') as f:\n",
    "    f.write(shuffle_doc_name_str)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/corpus/' + dataset + '_shuffle.txt', 'w') as f:\n",
    "    f.write(shuffle_doc_words_str)\n",
    "\n",
    "# build vocab\n",
    "print('building...')\n",
    "word_freq = {}\n",
    "word_set = set()\n",
    "for doc_words in shuffle_doc_words_list:\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_set.add(word)\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "\n",
    "vocab = list(word_set)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_doc_list = {}\n",
    "\n",
    "for i in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    appeared = set()\n",
    "    for word in words:\n",
    "        if word in appeared:\n",
    "            continue\n",
    "        if word in word_doc_list:\n",
    "            doc_list = word_doc_list[word]\n",
    "            doc_list.append(i)\n",
    "            word_doc_list[word] = doc_list\n",
    "        else:\n",
    "            word_doc_list[word] = [i]\n",
    "        appeared.add(word)\n",
    "\n",
    "word_doc_freq = {}\n",
    "for word, doc_list in word_doc_list.items():\n",
    "    word_doc_freq[word] = len(doc_list)\n",
    "\n",
    "word_id_map = {}\n",
    "for i in range(vocab_size):\n",
    "    word_id_map[vocab[i]] = i\n",
    "\n",
    "vocab_str = '\\n'.join(vocab)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/corpus/' + dataset + '_vocab.txt', 'w') as f:\n",
    "    f.write(vocab_str)\n",
    "\n",
    "# label list\n",
    "label_set = set()\n",
    "for doc_meta in shuffle_doc_name_list:\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label_set.add(temp[2])\n",
    "label_list = list(label_set)\n",
    "\n",
    "label_list_str = '\\n'.join(label_list)\n",
    "with open('./cleaned_data/' + dataset + '/corpus/' + dataset + '_labels.txt', 'w') as f:\n",
    "    f.write(label_list_str)\n",
    "\n",
    "# x: feature vectors of training docs, no initial features\n",
    "# slect 90% training set\n",
    "train_size = len(train_ids)\n",
    "val_size = int(0.1 * train_size)\n",
    "real_train_size = train_size - val_size\n",
    "\n",
    "real_train_doc_names = shuffle_doc_name_list[:real_train_size]\n",
    "real_train_doc_names_str = '\\n'.join(real_train_doc_names)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/graph/' + dataset + '.real_train.name', 'w') as f:\n",
    "    f.write(real_train_doc_names_str)\n",
    "\n",
    "row_x = []\n",
    "col_x = []\n",
    "data_x = []\n",
    "for i in range(real_train_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_x.append(i)\n",
    "        col_x.append(j)\n",
    "        data_x.append(doc_vec[j] / doc_len)\n",
    "\n",
    "x = sp.csr_matrix((data_x, (row_x, col_x)), shape=(\n",
    "    real_train_size, word_embeddings_dim\n",
    "))\n",
    "\n",
    "y = []\n",
    "for i in range(real_train_size):\n",
    "    doc_meta = shuffle_doc_name_list[i]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    y.append(one_hot)\n",
    "y = np.array(y)\n",
    "\n",
    "test_size = len(test_ids)\n",
    "\n",
    "row_tx = []\n",
    "col_tx = []\n",
    "data_tx = []\n",
    "for i in range(test_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i + train_size]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_tx.append(i)\n",
    "        col_tx.append(j)\n",
    "        data_tx.append(doc_vec[j] / doc_len)\n",
    "\n",
    "tx = sp.csr_matrix((data_tx, (row_tx, col_tx)),\n",
    "                   shape=(test_size, word_embeddings_dim))\n",
    "\n",
    "ty = []\n",
    "for i in range(test_size):\n",
    "    doc_meta = shuffle_doc_name_list[i + train_size]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    ty.append(one_hot)\n",
    "ty = np.array(ty)\n",
    "\n",
    "word_vectors = np.random.uniform(-0.01, 0.01,\n",
    "                                 (vocab_size, word_embeddings_dim))\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "    word = vocab[i]\n",
    "    if word in word_vector_map:\n",
    "        vector = word_vector_map[word]\n",
    "        word_vectors[i] = vector\n",
    "\n",
    "row_allx = []\n",
    "col_allx = []\n",
    "data_allx = []\n",
    "\n",
    "for i in range(train_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_allx.append(int(i))\n",
    "        col_allx.append(j)\n",
    "        data_allx.append(doc_vec[j] / doc_len)\n",
    "for i in range(vocab_size):\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_allx.append(int(i + train_size))\n",
    "        col_allx.append(j)\n",
    "        data_allx.append(word_vectors.item((i, j)))\n",
    "\n",
    "row_allx = np.array(row_allx)\n",
    "col_allx = np.array(col_allx)\n",
    "data_allx = np.array(data_allx)\n",
    "\n",
    "allx = sp.csr_matrix(\n",
    "    (data_allx, (row_allx, col_allx)), shape=(train_size + vocab_size, word_embeddings_dim)\n",
    ")\n",
    "\n",
    "ally = []\n",
    "for i in range(train_size):\n",
    "    doc_meta = shuffle_doc_name_list[i]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    ally.append(one_hot)\n",
    "\n",
    "for i in range(vocab_size):\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    ally.append(one_hot)\n",
    "\n",
    "ally = np.array(ally)\n",
    "\n",
    "print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
    "\n",
    "'''\n",
    "Doc word heterogeneous graph\n",
    "'''\n",
    "\n",
    "# word co-occurence with context windows\n",
    "window_size = 20\n",
    "windows = []\n",
    "\n",
    "for doc_words in shuffle_doc_words_list:\n",
    "    words = doc_words.split()\n",
    "    length = len(words)\n",
    "    if length <= window_size:\n",
    "        windows.append(words)\n",
    "    else:\n",
    "        for j in range(length - window_size + 1):\n",
    "            window = words[j: j + window_size]\n",
    "            windows.append(window)\n",
    "\n",
    "word_window_freq = {}\n",
    "for window in windows:\n",
    "    appeared = set()\n",
    "    for i in range(len(window)):\n",
    "        if window[i] in appeared:\n",
    "            continue\n",
    "        if window[i] in word_window_freq:\n",
    "            word_window_freq[window[i]] += 1\n",
    "        else:\n",
    "            word_window_freq[window[i]] = 1\n",
    "        appeared.add(window[i])\n",
    "\n",
    "word_pair_count = {}\n",
    "for window in windows:\n",
    "    for i in range(1, len(window)):\n",
    "        for j in range(0, i):\n",
    "            word_i = window[i]\n",
    "            word_i_id = word_id_map[word_i]\n",
    "            word_j = window[j]\n",
    "            word_j_id = word_id_map[word_j]\n",
    "            if word_i_id == word_j_id:\n",
    "                continue\n",
    "            word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n",
    "            # two orders\n",
    "            word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n",
    "\n",
    "row = []\n",
    "col = []\n",
    "weight = []\n",
    "\n",
    "# pmi as weights\n",
    "num_windows = len(windows)\n",
    "for key in word_pair_count:\n",
    "    temp = key.split(',')\n",
    "    i = int(temp[0])\n",
    "    j = int(temp[1])\n",
    "    count = word_pair_count[key]\n",
    "    word_freq_i = word_window_freq[vocab[i]]\n",
    "    word_freq_j = word_window_freq[vocab[j]]\n",
    "    pmi = log((1.0 * count / num_windows) /\n",
    "              (1.0 * word_freq_i * word_freq_j / (num_windows * num_windows)))\n",
    "    if pmi <= 0:\n",
    "        continue\n",
    "    row.append(train_size + i)\n",
    "    col.append(train_size + j)\n",
    "    weight.append(pmi)\n",
    "\n",
    "# doc word frequency\n",
    "doc_word_freq = {}\n",
    "\n",
    "for doc_id in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[doc_id]\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_id = word_id_map[word]\n",
    "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
    "        if doc_word_str in doc_word_freq:\n",
    "            doc_word_freq[doc_word_str] += 1\n",
    "        else:\n",
    "            doc_word_freq[doc_word_str] = 1\n",
    "\n",
    "for i in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_word_set = set()\n",
    "    for word in words:\n",
    "        if word in doc_word_set:\n",
    "            continue\n",
    "        j = word_id_map[word]\n",
    "        key = str(i) + ',' + str(j)\n",
    "        freq = doc_word_freq[key]\n",
    "        if i < train_size:\n",
    "            row.append(i)\n",
    "        else:\n",
    "            row.append(i + vocab_size)\n",
    "        col.append(train_size + j)\n",
    "        idf = log(1.0 * len(shuffle_doc_words_list) /\n",
    "                  word_doc_freq[vocab[j]])\n",
    "        weight.append(freq * idf)\n",
    "        doc_word_set.add(word)\n",
    "\n",
    "node_size = train_size + vocab_size + test_size\n",
    "adj = sp.csr_matrix((weight, (row, col)), shape=(node_size, node_size))\n",
    "\n",
    "# dump objects\n",
    "with open('./cleaned_data/' + dataset + '/graph/ind.' + dataset + '.x', 'wb') as f:\n",
    "    pkl.dump(x, f)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/graph/ind.' + dataset + '.y', 'wb') as f:\n",
    "    pkl.dump(y, f)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/graph/ind.' + dataset + '.tx', 'wb') as f:\n",
    "    pkl.dump(tx, f)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/graph/ind.' + dataset + '.ty', 'wb') as f:\n",
    "    pkl.dump(ty, f)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/graph/ind.' + dataset + '.allx', 'wb') as f:\n",
    "    pkl.dump(allx, f)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/graph/ind.' + dataset + '.ally', 'wb') as f:\n",
    "    pkl.dump(ally, f)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/graph/ind.' + dataset + '.adj', 'wb') as f:\n",
    "    pkl.dump(adj, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Layers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# global unique layer ID dictionary for layer name assignment\n",
    "_LAYER_UIDS = {}\n",
    "\n",
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"Helper function, assign unique layer IDs\"\"\"\n",
    "    if layer_name not in _LAYER_UIDS:\n",
    "        _LAYER_UIDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_UIDS[layer_name] += 1\n",
    "        return _LAYER_UIDS[layer_name]\n",
    "\n",
    "def sparse_dropout(x, rate, noise_shape):\n",
    "    \"\"\"Dropout for sparse tensors.\"\"\"\n",
    "    random_tensor = rate\n",
    "    random_tensor += tf.random.uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse.retain(x, dropout_mask)\n",
    "    return pre_out * (1./(rate))\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
    "    if sparse:\n",
    "        res = tf.sparse.sparse_dense_matmul(x, y)\n",
    "    else:\n",
    "        res = tf.matmul(x, y)\n",
    "    return res\n",
    "\n",
    "class GraphConvolution(layers.Layer):\n",
    "    \"\"\"Graph Convolutional Layer\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, num_features_nonzero,\n",
    "                 dropout=0.,\n",
    "                 is_sparse_inputs=False,\n",
    "                 activation=tf.nn.relu,\n",
    "                 bias=False,\n",
    "                 featureless=False, **kwargs):\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.is_sparse_inputs = is_sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "        self.num_features_nonzero = num_features_nonzero\n",
    "        self.embedding = None\n",
    "\n",
    "        self.weights_ = []\n",
    "        for i in range(1):\n",
    "            w = self.add_variable('weight' + str(i), [input_dim, output_dim])\n",
    "            self.weights_.append(w)\n",
    "        if self.bias:\n",
    "            self.bias = self.add_variable('bias', [output_dim])\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x, support_ = inputs\n",
    "        # dropout\n",
    "        if training is not False and self.is_sparse_inputs:\n",
    "            x = sparse_dropout(x, self.dropout, self.num_features_nonzero)\n",
    "        elif training is not False:\n",
    "            x = tf.nn.dropout(x, self.dropout)\n",
    "\n",
    "        # convolve\n",
    "        supports = list()\n",
    "        for i in range(len(support_)):\n",
    "            if not self.featureless:\n",
    "                pre_sup = dot(x, self.weights_[i], sparse=self.is_sparse_inputs)\n",
    "            else:\n",
    "                pre_sup = self.weights_[i]\n",
    "\n",
    "            support = dot(support_[i], pre_sup, sparse=True)\n",
    "            supports.append(support)\n",
    "        output = tf.add_n(supports)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.bias\n",
    "\n",
    "        self.embedding = output\n",
    "        return self.activation(output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "class CONFIG(object):\n",
    "    \"\"\"docstring for CONFIG\"\"\"\n",
    "    def __init__(self):\n",
    "        super(CONFIG, self).__init__()\n",
    "\n",
    "        self.dataset = 'R8'\n",
    "        self.learning_rate = 0.02\n",
    "        self.epochs = 100\n",
    "        self.hidden1 = 200\n",
    "        self.dropout = 0.5\n",
    "        self.weight_decay = 0.\n",
    "        self.early_stopping = 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "cfg = CONFIG()\n",
    "\n",
    "class GCN(keras.Model):\n",
    "    def __init__(self, input_dim, output_dim, num_features_nonzero, **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        print('input dim: ', input_dim)\n",
    "        print('output dim: ', output_dim)\n",
    "\n",
    "        self.layers_ = []\n",
    "        self.layers_.append(GraphConvolution(\n",
    "            input_dim=self.input_dim,\n",
    "            output_dim=cfg.hidden1,\n",
    "            num_features_nonzero=num_features_nonzero,\n",
    "            activation=tf.nn.relu,\n",
    "            dropout=cfg.dropout,\n",
    "            is_sparse_inputs=True\n",
    "        ))\n",
    "\n",
    "        self.layers_.append(GraphConvolution(\n",
    "            input_dim=cfg.hidden1,\n",
    "            output_dim=self.output_dim,\n",
    "            num_features_nonzero=num_features_nonzero,\n",
    "            activation=lambda x: x,\n",
    "            dropout=cfg.dropout\n",
    "        ))\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        x, label, mask, support = inputs\n",
    "        outputs = [x]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            hidden = layer((outputs[-1], support), training)\n",
    "            outputs.append(hidden)\n",
    "        output = outputs[-1]\n",
    "\n",
    "        # Weight decay loss\n",
    "        loss = tf.zeros([])\n",
    "        for var in self.layers_[0].trainable_variables:\n",
    "            loss += cfg.weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        loss += masked_softmax_cross_entropy(output, label, mask)\n",
    "        acc = masked_accuracy(output, label, mask)\n",
    "\n",
    "        return tf.argmax(output, 1), loss, acc\n",
    "\n",
    "    def predict(self, **kwargs):\n",
    "        return tf.nn.softmax(self.outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\firqa\\AppData\\Local\\Temp\\ipykernel_3644\\281618087.py:19: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return np.array(mask, dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dim:  15362\n",
      "output dim:  8\n",
      "WARNING:tensorflow:From C:\\Users\\firqa\\AppData\\Local\\Temp\\ipykernel_3644\\3256699056.py:53: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:Layer gcn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch: 0001 train_loss= 2.07935 train_acc= 0.14280 val_loss= 2.06279 val_acc= 0.64963 time= 11.22156\n",
      "Epoch: 0002 train_loss= 2.06161 train_acc= 0.68220 val_loss= 2.02519 val_acc= 0.65693 time= 6.29237\n",
      "Epoch: 0003 train_loss= 2.02310 train_acc= 0.69395 val_loss= 1.96431 val_acc= 0.64599 time= 6.41804\n",
      "Epoch: 0004 train_loss= 1.96074 train_acc= 0.67551 val_loss= 1.88068 val_acc= 0.62774 time= 5.83391\n",
      "Epoch: 0005 train_loss= 1.87142 train_acc= 0.65830 val_loss= 1.77767 val_acc= 0.61496 time= 5.59638\n",
      "Epoch: 0006 train_loss= 1.76158 train_acc= 0.63743 val_loss= 1.66257 val_acc= 0.59307 time= 5.32658\n",
      "Epoch: 0007 train_loss= 1.64523 train_acc= 0.63277 val_loss= 1.54857 val_acc= 0.57847 time= 4.91110\n",
      "Epoch: 0008 train_loss= 1.51183 train_acc= 0.60280 val_loss= 1.44885 val_acc= 0.57482 time= 4.66165\n",
      "Epoch: 0009 train_loss= 1.41004 train_acc= 0.59652 val_loss= 1.37120 val_acc= 0.56752 time= 4.82403\n",
      "Epoch: 0010 train_loss= 1.30788 train_acc= 0.60037 val_loss= 1.31339 val_acc= 0.55109 time= 4.61376\n",
      "Epoch: 0011 train_loss= 1.25780 train_acc= 0.59145 val_loss= 1.26858 val_acc= 0.54015 time= 4.78838\n",
      "Epoch: 0012 train_loss= 1.20841 train_acc= 0.57687 val_loss= 1.22728 val_acc= 0.54197 time= 4.59281\n",
      "Epoch: 0013 train_loss= 1.14774 train_acc= 0.57444 val_loss= 1.18296 val_acc= 0.56752 time= 4.25721\n",
      "Epoch: 0014 train_loss= 1.10909 train_acc= 0.60158 val_loss= 1.13250 val_acc= 0.60949 time= 4.80479\n",
      "Epoch: 0015 train_loss= 1.04374 train_acc= 0.63804 val_loss= 1.07646 val_acc= 0.65876 time= 4.31420\n",
      "Epoch: 0016 train_loss= 0.99594 train_acc= 0.68888 val_loss= 1.01715 val_acc= 0.72445 time= 4.45544\n",
      "Epoch: 0017 train_loss= 0.94209 train_acc= 0.74539 val_loss= 0.95862 val_acc= 0.76460 time= 4.60756\n",
      "Epoch: 0018 train_loss= 0.88988 train_acc= 0.77213 val_loss= 0.90472 val_acc= 0.77007 time= 4.36983\n",
      "Epoch: 0019 train_loss= 0.84609 train_acc= 0.78104 val_loss= 0.85709 val_acc= 0.76095 time= 4.92920\n",
      "Epoch: 0020 train_loss= 0.79901 train_acc= 0.78388 val_loss= 0.81638 val_acc= 0.75912 time= 4.97419\n",
      "Epoch: 0021 train_loss= 0.75690 train_acc= 0.77983 val_loss= 0.78198 val_acc= 0.75547 time= 4.97931\n",
      "Epoch: 0022 train_loss= 0.72738 train_acc= 0.77699 val_loss= 0.75279 val_acc= 0.75182 time= 6.82655\n",
      "Epoch: 0023 train_loss= 0.70272 train_acc= 0.77780 val_loss= 0.72745 val_acc= 0.75182 time= 6.33061\n",
      "Epoch: 0024 train_loss= 0.67598 train_acc= 0.77881 val_loss= 0.70480 val_acc= 0.75730 time= 4.62408\n",
      "Epoch: 0025 train_loss= 0.66335 train_acc= 0.78347 val_loss= 0.68387 val_acc= 0.77007 time= 6.64483\n",
      "Epoch: 0026 train_loss= 0.64043 train_acc= 0.78975 val_loss= 0.66399 val_acc= 0.78285 time= 5.27358\n",
      "Epoch: 0027 train_loss= 0.61016 train_acc= 0.80697 val_loss= 0.64475 val_acc= 0.79927 time= 5.77215\n",
      "Epoch: 0028 train_loss= 0.59739 train_acc= 0.81669 val_loss= 0.62589 val_acc= 0.80839 time= 4.95918\n",
      "Epoch: 0029 train_loss= 0.58348 train_acc= 0.83249 val_loss= 0.60731 val_acc= 0.82117 time= 4.82992\n",
      "Epoch: 0030 train_loss= 0.55851 train_acc= 0.84707 val_loss= 0.58901 val_acc= 0.84307 time= 5.65626\n",
      "Epoch: 0031 train_loss= 0.54159 train_acc= 0.86510 val_loss= 0.57098 val_acc= 0.84307 time= 5.45412\n",
      "Epoch: 0032 train_loss= 0.52481 train_acc= 0.86267 val_loss= 0.55352 val_acc= 0.85949 time= 6.03407\n",
      "Epoch: 0033 train_loss= 0.50547 train_acc= 0.87118 val_loss= 0.53676 val_acc= 0.85949 time= 5.27012\n",
      "Epoch: 0034 train_loss= 0.48498 train_acc= 0.87766 val_loss= 0.52071 val_acc= 0.86861 time= 8.36497\n",
      "Epoch: 0035 train_loss= 0.47599 train_acc= 0.87300 val_loss= 0.50534 val_acc= 0.87226 time= 8.60959\n",
      "Epoch: 0036 train_loss= 0.45613 train_acc= 0.88110 val_loss= 0.49051 val_acc= 0.87409 time= 5.26985\n",
      "Epoch: 0037 train_loss= 0.44223 train_acc= 0.87968 val_loss= 0.47605 val_acc= 0.88139 time= 5.51379\n",
      "Epoch: 0038 train_loss= 0.42243 train_acc= 0.89022 val_loss= 0.46202 val_acc= 0.88321 time= 5.90442\n",
      "Epoch: 0039 train_loss= 0.40605 train_acc= 0.88961 val_loss= 0.44838 val_acc= 0.88504 time= 5.00567\n",
      "Epoch: 0040 train_loss= 0.39382 train_acc= 0.90055 val_loss= 0.43511 val_acc= 0.88504 time= 4.80465\n",
      "Epoch: 0041 train_loss= 0.38072 train_acc= 0.90359 val_loss= 0.42215 val_acc= 0.88869 time= 4.52164\n",
      "Epoch: 0042 train_loss= 0.36753 train_acc= 0.90743 val_loss= 0.40953 val_acc= 0.89051 time= 4.11845\n",
      "Epoch: 0043 train_loss= 0.35720 train_acc= 0.90541 val_loss= 0.39719 val_acc= 0.89234 time= 4.80231\n",
      "Epoch: 0044 train_loss= 0.33991 train_acc= 0.91067 val_loss= 0.38523 val_acc= 0.89234 time= 4.31466\n",
      "Epoch: 0045 train_loss= 0.32270 train_acc= 0.91776 val_loss= 0.37375 val_acc= 0.89781 time= 4.50890\n",
      "Epoch: 0046 train_loss= 0.31830 train_acc= 0.91736 val_loss= 0.36269 val_acc= 0.90146 time= 4.75601\n",
      "Epoch: 0047 train_loss= 0.30971 train_acc= 0.92809 val_loss= 0.35196 val_acc= 0.90146 time= 4.12678\n",
      "Epoch: 0048 train_loss= 0.29504 train_acc= 0.92789 val_loss= 0.34161 val_acc= 0.90511 time= 4.42837\n",
      "Epoch: 0049 train_loss= 0.27782 train_acc= 0.93316 val_loss= 0.33164 val_acc= 0.90511 time= 4.77805\n",
      "Epoch: 0050 train_loss= 0.27035 train_acc= 0.94227 val_loss= 0.32210 val_acc= 0.90876 time= 4.27661\n",
      "Epoch: 0051 train_loss= 0.25624 train_acc= 0.93863 val_loss= 0.31296 val_acc= 0.91058 time= 4.44996\n",
      "Epoch: 0052 train_loss= 0.24505 train_acc= 0.94632 val_loss= 0.30421 val_acc= 0.91241 time= 4.82277\n",
      "Epoch: 0053 train_loss= 0.23902 train_acc= 0.94713 val_loss= 0.29573 val_acc= 0.91423 time= 4.19899\n",
      "Epoch: 0054 train_loss= 0.23063 train_acc= 0.95017 val_loss= 0.28753 val_acc= 0.91606 time= 4.54808\n",
      "Epoch: 0055 train_loss= 0.22453 train_acc= 0.95098 val_loss= 0.27973 val_acc= 0.91788 time= 5.07760\n",
      "Epoch: 0056 train_loss= 0.21640 train_acc= 0.95078 val_loss= 0.27219 val_acc= 0.91788 time= 4.57503\n",
      "Epoch: 0057 train_loss= 0.20506 train_acc= 0.95584 val_loss= 0.26532 val_acc= 0.92153 time= 4.86034\n",
      "Epoch: 0058 train_loss= 0.19799 train_acc= 0.95625 val_loss= 0.25865 val_acc= 0.92153 time= 4.86035\n",
      "Epoch: 0059 train_loss= 0.19013 train_acc= 0.96212 val_loss= 0.25238 val_acc= 0.92336 time= 4.12720\n",
      "Epoch: 0060 train_loss= 0.19321 train_acc= 0.95301 val_loss= 0.24627 val_acc= 0.92336 time= 4.33315\n",
      "Epoch: 0061 train_loss= 0.16923 train_acc= 0.96395 val_loss= 0.24035 val_acc= 0.92518 time= 4.99230\n",
      "Epoch: 0062 train_loss= 0.17382 train_acc= 0.96212 val_loss= 0.23456 val_acc= 0.92518 time= 4.59719\n",
      "Epoch: 0063 train_loss= 0.16202 train_acc= 0.96192 val_loss= 0.22898 val_acc= 0.92883 time= 4.82713\n",
      "Epoch: 0064 train_loss= 0.15759 train_acc= 0.96536 val_loss= 0.22366 val_acc= 0.92883 time= 4.60098\n",
      "Epoch: 0065 train_loss= 0.14939 train_acc= 0.96455 val_loss= 0.21868 val_acc= 0.93066 time= 4.24723\n",
      "Epoch: 0066 train_loss= 0.14737 train_acc= 0.96435 val_loss= 0.21442 val_acc= 0.93066 time= 4.73870\n",
      "Epoch: 0067 train_loss= 0.13849 train_acc= 0.96982 val_loss= 0.21105 val_acc= 0.93613 time= 4.90206\n",
      "Epoch: 0068 train_loss= 0.13944 train_acc= 0.96577 val_loss= 0.20838 val_acc= 0.93431 time= 8.36281\n",
      "Epoch: 0069 train_loss= 0.13111 train_acc= 0.96860 val_loss= 0.20566 val_acc= 0.93613 time= 6.79228\n",
      "Epoch: 0070 train_loss= 0.12647 train_acc= 0.97002 val_loss= 0.20225 val_acc= 0.93796 time= 5.42198\n",
      "Epoch: 0071 train_loss= 0.12110 train_acc= 0.96881 val_loss= 0.19805 val_acc= 0.93613 time= 6.36162\n",
      "Epoch: 0072 train_loss= 0.12123 train_acc= 0.96982 val_loss= 0.19325 val_acc= 0.93796 time= 4.66850\n",
      "Epoch: 0073 train_loss= 0.12069 train_acc= 0.97002 val_loss= 0.18869 val_acc= 0.93796 time= 4.19873\n",
      "Epoch: 0074 train_loss= 0.11246 train_acc= 0.97306 val_loss= 0.18467 val_acc= 0.94161 time= 4.69353\n",
      "Epoch: 0075 train_loss= 0.10693 train_acc= 0.97691 val_loss= 0.18108 val_acc= 0.94161 time= 4.41102\n",
      "Epoch: 0076 train_loss= 0.10515 train_acc= 0.97468 val_loss= 0.17788 val_acc= 0.94708 time= 4.75549\n",
      "Epoch: 0077 train_loss= 0.10180 train_acc= 0.97428 val_loss= 0.17505 val_acc= 0.94708 time= 4.45777\n",
      "Epoch: 0078 train_loss= 0.10454 train_acc= 0.97650 val_loss= 0.17233 val_acc= 0.94708 time= 5.18155\n",
      "Epoch: 0079 train_loss= 0.09552 train_acc= 0.97914 val_loss= 0.17036 val_acc= 0.94526 time= 7.58435\n",
      "Epoch: 0080 train_loss= 0.09385 train_acc= 0.97873 val_loss= 0.16893 val_acc= 0.94708 time= 6.27949\n",
      "Epoch: 0081 train_loss= 0.09610 train_acc= 0.97711 val_loss= 0.16809 val_acc= 0.94708 time= 5.69048\n",
      "Epoch: 0082 train_loss= 0.08935 train_acc= 0.97914 val_loss= 0.16677 val_acc= 0.94708 time= 5.82973\n",
      "Epoch: 0083 train_loss= 0.08889 train_acc= 0.97772 val_loss= 0.16582 val_acc= 0.94708 time= 5.27856\n",
      "Epoch: 0084 train_loss= 0.08582 train_acc= 0.97934 val_loss= 0.16466 val_acc= 0.94890 time= 5.63827\n",
      "Epoch: 0085 train_loss= 0.08142 train_acc= 0.98015 val_loss= 0.16349 val_acc= 0.94708 time= 6.10079\n",
      "Epoch: 0086 train_loss= 0.08136 train_acc= 0.98055 val_loss= 0.16208 val_acc= 0.94708 time= 5.43106\n",
      "Epoch: 0087 train_loss= 0.08026 train_acc= 0.98197 val_loss= 0.15998 val_acc= 0.94708 time= 5.73457\n",
      "Epoch: 0088 train_loss= 0.07857 train_acc= 0.98096 val_loss= 0.15745 val_acc= 0.94708 time= 5.71505\n",
      "Epoch: 0089 train_loss= 0.07684 train_acc= 0.98298 val_loss= 0.15474 val_acc= 0.95073 time= 6.46310\n",
      "Epoch: 0090 train_loss= 0.07129 train_acc= 0.98380 val_loss= 0.15198 val_acc= 0.95073 time= 6.36195\n",
      "Epoch: 0091 train_loss= 0.07572 train_acc= 0.97954 val_loss= 0.14944 val_acc= 0.95255 time= 5.38954\n",
      "Epoch: 0092 train_loss= 0.07080 train_acc= 0.98238 val_loss= 0.14784 val_acc= 0.95620 time= 6.01340\n",
      "Epoch: 0093 train_loss= 0.06885 train_acc= 0.98481 val_loss= 0.14700 val_acc= 0.95620 time= 5.49342\n",
      "Epoch: 0094 train_loss= 0.06797 train_acc= 0.98258 val_loss= 0.14656 val_acc= 0.95620 time= 5.68918\n",
      "Epoch: 0095 train_loss= 0.06791 train_acc= 0.98562 val_loss= 0.14641 val_acc= 0.95438 time= 6.54753\n",
      "Epoch: 0096 train_loss= 0.07050 train_acc= 0.98380 val_loss= 0.14658 val_acc= 0.95438 time= 5.71046\n",
      "Epoch: 0097 train_loss= 0.06322 train_acc= 0.98380 val_loss= 0.14567 val_acc= 0.95255 time= 5.61488\n",
      "Epoch: 0098 train_loss= 0.06271 train_acc= 0.98440 val_loss= 0.14567 val_acc= 0.95255 time= 5.72431\n",
      "Epoch: 0099 train_loss= 0.06170 train_acc= 0.98521 val_loss= 0.14610 val_acc= 0.95255 time= 6.19220\n",
      "Epoch: 0100 train_loss= 0.05606 train_acc= 0.98623 val_loss= 0.14638 val_acc= 0.95255 time= 6.75840\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "cfg = CONFIG()\n",
    "\n",
    "# set random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "dataset = \"R8\"\n",
    "cfg.dataset = dataset\n",
    "\n",
    "# Load data\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size = load_corpus(cfg.dataset)\n",
    "\n",
    "features = sp.identity(features.shape[0])  # featureless\n",
    "features = preprocess_features(features)\n",
    "\n",
    "\n",
    "support = [preprocess_adj(adj)]\n",
    "\n",
    "\n",
    "t_features = tf.SparseTensor(*features)\n",
    "t_y_train = tf.convert_to_tensor(y_train)\n",
    "t_y_val = tf.convert_to_tensor(y_val)\n",
    "t_y_test = tf.convert_to_tensor(y_test)\n",
    "tm_train_mask = tf.convert_to_tensor(train_mask)\n",
    "\n",
    "tm_val_mask = tf.convert_to_tensor(val_mask)\n",
    "tm_test_mask = tf.convert_to_tensor(test_mask)\n",
    "\n",
    "t_support = []\n",
    "for i in range(len(support)):\n",
    "    t_support.append(tf.cast(tf.SparseTensor(*support[i]), dtype=tf.float64))\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = GCN(input_dim=features[2][1], output_dim=y_train.shape[1], num_features_nonzero=features[1].shape)\n",
    "\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "optimizer = optimizers.Adam(lr=cfg.learning_rate)\n",
    "\n",
    "cost_val = []\n",
    "\n",
    "for epoch in range(cfg.epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    with tf.GradientTape() as tape:\n",
    "        _, loss, acc = model((t_features, t_y_train, tm_train_mask, t_support))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    _, val_loss, val_acc = model((t_features, t_y_val, tm_val_mask, t_support), training=False)\n",
    "    cost_val.append(val_loss)\n",
    "\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(loss),\n",
    "          \"train_acc=\", \"{:.5f}\".format(acc), \"val_loss=\", \"{:.5f}\".format(val_loss),\n",
    "          \"val_acc=\", \"{:.5f}\".format(val_acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch > cfg.early_stopping and cost_val[-1] > np.mean(cost_val[-(cfg.early_stopping+1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Eval"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: cost= 0.11123 accuracy= 0.97350 time= 2.08307\n",
      "Average Test Precision, Recall and F1-Score...\n",
      "(0.9735038830516217, 0.9735038830516217, 0.9735038830516217, None)\n"
     ]
    }
   ],
   "source": [
    "def evaluate(features, y, mask, support):\n",
    "    t = time.time()\n",
    "    pred, test_loss, test_acc = model((features, y, mask, support), training=False)\n",
    "    return test_loss, test_acc, pred, np.argmax(y, axis=1), time.time() - t\n",
    "\n",
    "test_cost, test_acc, pred, labels, test_duration = evaluate(t_features, t_y_test, tm_test_mask, t_support)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost), \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))\n",
    "\n",
    "test_pred = []\n",
    "test_labels = []\n",
    "\n",
    "for i in range(len(test_mask)):\n",
    "    if test_mask[i]:\n",
    "        test_pred.append(pred[i])\n",
    "        test_labels.append(labels[i])\n",
    "\n",
    "print(\"Average Test Precision, Recall and F1-Score...\")\n",
    "print(metrics.precision_recall_fscore_support(test_labels, test_pred, average='micro'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storing layer 1 embeddings...\n",
      "finish...\n",
      "storing layer 2 embeddings...\n",
      "finish...\n"
     ]
    }
   ],
   "source": [
    "print('storing layer 1 embeddings...')\n",
    "\n",
    "embeddings_1 = model.layers_[0].embedding\n",
    "\n",
    "word_embeddings = embeddings_1[train_size: adj.shape[0] - test_size]\n",
    "train_doc_embeddings = embeddings_1[:train_size]  # include val docs\n",
    "test_doc_embeddings = embeddings_1[adj.shape[0] - test_size:]\n",
    "\n",
    "\n",
    "doc_vectors = []\n",
    "doc_id = 0\n",
    "for i in range(train_size):\n",
    "    doc_vector = train_doc_embeddings[i]\n",
    "    doc_vector_str = ' '.join([str(tf.keras.backend.get_value(x)) for x in doc_vector])\n",
    "    doc_vectors.append('doc_' + str(doc_id) + ' ' + doc_vector_str)\n",
    "    doc_id += 1\n",
    "\n",
    "for i in range(test_size):\n",
    "    doc_vector = test_doc_embeddings[i]\n",
    "    doc_vector_str = ' '.join([str(tf.keras.backend.get_value(x)) for x in doc_vector])\n",
    "    doc_vectors.append('doc_' + str(doc_id) + ' ' + doc_vector_str)\n",
    "    doc_id += 1\n",
    "\n",
    "doc_embeddings_str = '\\n'.join(doc_vectors)\n",
    "f = open('./cleaned_data/' + cfg.dataset + '/' + cfg.dataset +  '_1_vectors.txt', 'w')\n",
    "f.write(doc_embeddings_str)\n",
    "f.close()\n",
    "\n",
    "print(\"finish...\")\n",
    "\n",
    "\n",
    "\n",
    "print('storing layer 2 embeddings...')\n",
    "embeddings_2 = model.layers_[1].embedding\n",
    "\n",
    "word_embeddings = embeddings_2[train_size: adj.shape[0] - test_size]\n",
    "train_doc_embeddings = embeddings_2[:train_size]  # include val docs\n",
    "test_doc_embeddings = embeddings_2[adj.shape[0] - test_size:]\n",
    "\n",
    "doc_vectors = []\n",
    "doc_id = 0\n",
    "for i in range(train_size):\n",
    "    doc_vector = train_doc_embeddings[i]\n",
    "    doc_vector_str = ' '.join([str(tf.keras.backend.get_value(x)) for x in doc_vector])\n",
    "    doc_vectors.append('doc_' + str(doc_id) + ' ' + doc_vector_str)\n",
    "    doc_id += 1\n",
    "\n",
    "for i in range(test_size):\n",
    "    doc_vector = test_doc_embeddings[i]\n",
    "    doc_vector_str = ' '.join([str(tf.keras.backend.get_value(x)) for x in doc_vector])\n",
    "    doc_vectors.append('doc_' + str(doc_id) + ' ' + doc_vector_str)\n",
    "    doc_id += 1\n",
    "\n",
    "doc_embeddings_str = '\\n'.join(doc_vectors)\n",
    "f = open('./cleaned_data/' + cfg.dataset + '/' + cfg.dataset +  '_2_vectors.txt', 'w')\n",
    "f.write(doc_embeddings_str)\n",
    "f.close()\n",
    "\n",
    "print(\"finish...\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pygeo",
   "language": "python",
   "display_name": "pygeo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}