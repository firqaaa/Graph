{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "imdb = pd.read_csv('./IMDB Dataset.csv')\n",
    "imdb = shuffle(imdb, random_state=0)\n",
    "\n",
    "def rm_tab(text):\n",
    "    return re.sub(\"[\\t ]+\", \" \", text)\n",
    "\n",
    "imdb['review'] = imdb['review'].apply(rm_tab)\n",
    "\n",
    "train = imdb[:5000]\n",
    "test = imdb[5000:6000]\n",
    "\n",
    "# token_length = []\n",
    "# for i in imdb['review']:\n",
    "#     token_length.append(len(i.split(\" \")))\n",
    "#\n",
    "# print(np.sum(token_length) / len(token_length))\n",
    "\n",
    "if (os.path.exists(\"./data/imdb/raw/\")) ^ (os.path.exists(\"./data/imdb/processed\")):\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(\"./data/imdb/raw/\", exist_ok=True)\n",
    "    # os.makedirs(\"./data/imdb/processed/\", exist_ok=True)\n",
    "\n",
    "if os.path.exists(\"./config/imdb/\"):\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(\"./config/imdb/\")\n",
    "\n",
    "with open('./data/imdb/raw/train.txt', \"w\", encoding='utf-8') as output_file:\n",
    "    [output_file.write(\"\".join(train.iloc[i, :]['review'] + '\\t' + train.iloc[i, :]['sentiment']) + '\\n') for i in range(len(train))]\n",
    "    output_file.close()\n",
    "\n",
    "with open('./data/imdb/raw/test.txt', \"w\", encoding='utf-8') as output_file:\n",
    "    [output_file.write(\"\".join(test.iloc[i, :]['review'] + '\\t' + test.iloc[i, :]['sentiment']) + '\\n') for i in range(len(test))]\n",
    "    output_file.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from graph4nlp.pytorch.data.dataset import Text2LabelDataset\n",
    "\n",
    "class IMDBDataset(Text2LabelDataset):\n",
    "    @property\n",
    "    def raw_file_names(self) -> dict:\n",
    "        \"\"\"3 reserved keys: 'train', 'val' (optional), 'test'. Represent the split of dataset.\"\"\"\n",
    "        return {\"train\": \"train.txt\", \"test\": \"test.txt\"}\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> dict:\n",
    "        \"\"\"At least 3 reserved keys should be filled: 'vocab', 'data' and 'label'.\"\"\"\n",
    "        return {\"vocab\": \"vocab.pt\", \"data\": \"data.pt\", \"label\": \"label.pt\"}\n",
    "\n",
    "    def download(self):\n",
    "        return\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            root_dir,\n",
    "            topology_subdir,\n",
    "            graph_name,\n",
    "            static_or_dynamic=\"static\",\n",
    "            topology_builder=None,\n",
    "            dynamic_init_graph_name=None,\n",
    "            dynamic_init_topology_builder=None,\n",
    "            dynamic_init_topology_aux_args=None,\n",
    "            pretrained_word_emb_name=\"840B\",\n",
    "            pretrained_word_emb_url=None,\n",
    "            pretrained_word_emb_cache_dir=None,\n",
    "            max_word_vocab_size=None,\n",
    "            min_word_vocab_freq=1,\n",
    "            tokenizer=nltk.RegexpTokenizer(\" \", gaps=True).tokenize,\n",
    "            word_emb_size=None,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super(IMDBDataset, self).__init__(\n",
    "            graph_name,\n",
    "            root_dir=root_dir,\n",
    "            static_or_dynamic=static_or_dynamic,\n",
    "            topology_builder=topology_builder,\n",
    "            topology_subdir=topology_subdir,\n",
    "            dynamic_init_graph_name=dynamic_init_graph_name,\n",
    "            dynamic_init_topology_builder=dynamic_init_topology_builder,\n",
    "            dynamic_init_topology_aux_args=dynamic_init_topology_aux_args,\n",
    "            pretrained_word_emb_name=pretrained_word_emb_name,\n",
    "            pretrained_word_emb_url=pretrained_word_emb_url,\n",
    "            pretrained_word_emb_cache_dir=pretrained_word_emb_cache_dir,\n",
    "            max_word_vocab_size=max_word_vocab_size,\n",
    "            min_word_vocab_freq=min_word_vocab_freq,\n",
    "            tokenizer=tokenizer,\n",
    "            word_emb_size=word_emb_size,\n",
    "            **kwargs\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import yaml\n",
    "import torch\n",
    "import datetime\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from graph4nlp.pytorch.modules.evaluation.accuracy import Accuracy\n",
    "from graph4nlp.pytorch.modules.graph_construction import (\n",
    "    NodeEmbeddingBasedGraphConstruction,\n",
    "    NodeEmbeddingBasedRefinedGraphConstruction\n",
    ")\n",
    "from graph4nlp.pytorch.modules.graph_embedding_initialization.embedding_construction import (\n",
    "    WordEmbedding\n",
    ")\n",
    "from graph4nlp.pytorch.modules.graph_embedding_initialization.graph_embedding_initialization import (\n",
    "    GraphEmbeddingInitialization\n",
    ")\n",
    "from graph4nlp.pytorch.modules.graph_embedding_learning import GAT, GGNN, GraphSAGE\n",
    "from graph4nlp.pytorch.modules.loss.general_loss import GeneralLoss\n",
    "from graph4nlp.pytorch.modules.prediction.classification.graph_classification import FeedForwardNN\n",
    "from graph4nlp.pytorch.modules.utils import constants as Constants\n",
    "from graph4nlp.pytorch.modules.utils.generic_utils import EarlyStopping, grid, to_cuda\n",
    "from graph4nlp.pytorch.modules.utils.logger import Logger"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab, label_model, config):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.config = config\n",
    "        self.vocab_model = vocab\n",
    "        self.label_model = label_model\n",
    "        self.graph_name = self.config[\"graph_construction_args\"][\"graph_construction_share\"][\n",
    "            \"graph_name\"\n",
    "        ]\n",
    "        assert not (\n",
    "                self.graph_name in (\"node_emb\", \"node_emb_refined\") and config[\"gnn\"] == \"gat\"\n",
    "        ), \"dynamic graph construction does not support GAT\"\n",
    "\n",
    "        embedding_style = {\n",
    "            \"single_token_item\": True if self.graph_name != \"ie\" else False,\n",
    "            \"emb_strategy\": config.get(\"emb_strategy\", \"w2v_bilstm\"),\n",
    "            \"num_rnn_layers\": 1,\n",
    "            \"bert_model_name\": config.get(\"bert_model_name\", \"bert-base-uncased\"),\n",
    "            \"bert_lower_case\": True,\n",
    "        }\n",
    "\n",
    "        self.graph_initializer = GraphEmbeddingInitialization(\n",
    "            word_vocab=self.vocab_model.in_word_vocab,\n",
    "            embedding_style=embedding_style,\n",
    "            hidden_size=config[\"num_hidden\"],\n",
    "            word_dropout=config[\"word_dropout\"],\n",
    "            rnn_dropout=config[\"rnn_dropout\"],\n",
    "            fix_word_emb=not config[\"no_fix_word_emb\"],\n",
    "            fix_bert_emb=not config.get(\"no_fix_bert_emb\", False),\n",
    "        )\n",
    "\n",
    "        use_edge_weight = False\n",
    "        if self.graph_name == \"node_emb\":\n",
    "            self.graph_topology = NodeEmbeddingBasedGraphConstruction(\n",
    "                sim_metric_type=config[\"gl_metric_type\"],\n",
    "                num_heads=config[\"gl_num_heads\"],\n",
    "                top_k_neigh=config[\"gl_top_k\"],\n",
    "                epsilon_neigh=config[\"gl_epsilon\"],\n",
    "                smoothness_ratio=config[\"gl_smoothness_ratio\"],\n",
    "                connectivity_ratio=config[\"gl_connectivity_ratio\"],\n",
    "                sparsity_ratio=config[\"gl_sparsity_ratio\"],\n",
    "                input_size=config[\"num_hidden\"],\n",
    "                hidden_size=config[\"gl_num_hidden\"],\n",
    "            )\n",
    "            use_edge_weight = True\n",
    "        elif self.graph_name == \"node_emb_refined\":\n",
    "            self.graph_topology = NodeEmbeddingBasedRefinedGraphConstruction(\n",
    "                config[\"init_adj_alpha\"],\n",
    "                sim_metric_type=config[\"gl_metric_type\"],\n",
    "                num_heads=config[\"gl_num_heads\"],\n",
    "                top_k_neigh=config[\"gl_top_k\"],\n",
    "                epsilon_neigh=config[\"gl_epsilon\"],\n",
    "                smoothness_ratio=config[\"gl_smoothness_ratio\"],\n",
    "                connectivity_ratio=config[\"gl_connectivity_ratio\"],\n",
    "                sparsity_ratio=config[\"gl_sparsity_ratio\"],\n",
    "                input_size=config[\"num_hidden\"],\n",
    "                hidden_size=config[\"gl_num_hidden\"],\n",
    "            )\n",
    "            use_edge_weight = True\n",
    "\n",
    "        if \"w2v\" in self.graph_initializer.embedding_layer.word_emb_layers:\n",
    "            self.word_emb = self.graph_initializer.embedding_layer.word_emb_layers[\n",
    "                \"w2v\"\n",
    "            ].word_emb_layer\n",
    "        else:\n",
    "            self.word_emb = WordEmbedding(\n",
    "                self.vocab_model.in_word_vocab.embeddings.shape[0],\n",
    "                self.vocab_model.in_word_vocab.embeddings.shape[1],\n",
    "                pretrained_word_emb=self.vocab_model.in_word_vocab.embeddings,\n",
    "                fix_emb=not config[\"no_fix_word_emb\"],\n",
    "            ).word_emb_layer\n",
    "\n",
    "        if config[\"gnn\"] == \"gat\":\n",
    "            heads = [config[\"gat_num_heads\"]] * (config[\"gnn_num_layers\"] - 1) + [\n",
    "                config[\"gat_num_out_heads\"]\n",
    "            ]\n",
    "            self.gnn = GAT(\n",
    "                config[\"gnn_num_layers\"],\n",
    "                config[\"num_hidden\"],\n",
    "                config[\"num_hidden\"],\n",
    "                config[\"num_hidden\"],\n",
    "                heads,\n",
    "                direction_option=config[\"gnn_direction_option\"],\n",
    "                feat_drop=config[\"gnn_dropout\"],\n",
    "                attn_drop=config[\"gat_attn_dropout\"],\n",
    "                negative_slope=config[\"gat_negative_slope\"],\n",
    "                residual=config[\"gat_residual\"],\n",
    "                activation=F.elu,\n",
    "                allow_zero_in_degree=True,\n",
    "            )\n",
    "        elif config[\"gnn\"] == \"graphsage\":\n",
    "            self.gnn = GraphSAGE(\n",
    "                config[\"gnn_num_layers\"],\n",
    "                config[\"num_hidden\"],\n",
    "                config[\"num_hidden\"],\n",
    "                config[\"num_hidden\"],\n",
    "                config[\"graphsage_aggreagte_type\"],\n",
    "                direction_option=config[\"gnn_direction_option\"],\n",
    "                feat_drop=config[\"gnn_dropout\"],\n",
    "                bias=True,\n",
    "                norm=None,\n",
    "                activation=F.relu,\n",
    "                use_edge_weight=use_edge_weight,\n",
    "            )\n",
    "        elif config[\"gnn\"] == \"ggnn\":\n",
    "            self.gnn = GGNN(\n",
    "                config[\"gnn_num_layers\"],\n",
    "                config[\"num_hidden\"],\n",
    "                config[\"num_hidden\"],\n",
    "                config[\"num_hidden\"],\n",
    "                feat_drop=config[\"gnn_dropout\"],\n",
    "                direction_option=config[\"gnn_direction_option\"],\n",
    "                bias=True,\n",
    "                use_edge_weight=use_edge_weight,\n",
    "            )\n",
    "        else:\n",
    "            raise RuntimeError(\"Unknown gnn type: {}\".format(config[\"gnn\"]))\n",
    "\n",
    "        self.clf = FeedForwardNN(\n",
    "            2 * config[\"num_hidden\"]\n",
    "            if config[\"gnn_direction_option\"] == \"bi_sep\"\n",
    "            else config[\"num_hidden\"],\n",
    "            config[\"num_classes\"],\n",
    "            [config[\"num_hidden\"]],\n",
    "            graph_pool_type=config[\"graph_pooling\"],\n",
    "            dim=config[\"num_hidden\"],\n",
    "            use_linear_proj=config[\"max_pool_linear_proj\"],\n",
    "        )\n",
    "\n",
    "        self.loss = GeneralLoss(\"CrossEntropy\")\n",
    "\n",
    "    def forward(self, graph_list, tgt=None, require_loss=True):\n",
    "        # graph embedding initialization\n",
    "        batch_gd = self.graph_initializer(graph_list)\n",
    "\n",
    "        # run dynamic graph construction if turned on\n",
    "        if hasattr(self, \"graph_topology\") and hasattr(self.graph_topology, \"dynamic_topology\"):\n",
    "            batch_gd = self.graph_topology.dynamic_topology(batch_gd)\n",
    "\n",
    "        # run GNN\n",
    "        self.gnn(batch_gd)\n",
    "\n",
    "        # run graph classifier\n",
    "        self.clf(batch_gd)\n",
    "        logits = batch_gd.graph_attributes[\"logits\"]\n",
    "\n",
    "        if require_loss:\n",
    "            loss = self.loss(logits, tgt)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "    def inference_forward(self, collate_data):\n",
    "        return self.forward(collate_data[\"graph_data\"], require_loss=False)\n",
    "\n",
    "    def post_process(self, logits, label_names):\n",
    "        logits_list = []\n",
    "\n",
    "        for idx in range(len(logits)):\n",
    "            logits_list.append(logits[idx].cpu().clone().numpy())\n",
    "\n",
    "        pred_tags = [label_names[pred.argmax()] for pred in logits_list]\n",
    "        return pred_tags\n",
    "\n",
    "    @classmethod\n",
    "    def load_checkpoint(cls, model_path):\n",
    "        \"\"\"The API to load the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model_path : str\n",
    "            The saved model path.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Class\n",
    "        \"\"\"\n",
    "        return torch.load(model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class ModelHandler:\n",
    "    def __init__(self, config):\n",
    "        super(ModelHandler, self).__init__()\n",
    "        self.config = config\n",
    "        self.logger = Logger(\n",
    "            self.config[\"out_dir\"],\n",
    "            config={k: v for k, v in self.config.items() if k != \"device\"},\n",
    "            overwrite=True,\n",
    "        )\n",
    "        self.logger.write(self.config[\"out_dir\"])\n",
    "        self._build_dataloader()\n",
    "        self._build_model()\n",
    "        self._build_optimizer()\n",
    "        self._build_evaluation()\n",
    "\n",
    "    def _build_dataloader(self):\n",
    "        self.graph_name = self.config[\"graph_construction_args\"][\"graph_construction_share\"][\n",
    "            \"graph_name\"\n",
    "        ]\n",
    "        topology_subdir = \"{}_graph\".format(self.graph_name)\n",
    "        if self.graph_name == \"node_emb_refined\":\n",
    "            topology_subdir += \"_{}\".format(\n",
    "                self.config[\"graph_construction_args\"][\"graph_construction_private\"][\n",
    "                    \"dynamic_init_graph_name\"\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        dataset = IMDBDataset(\n",
    "            root_dir=self.config[\"graph_construction_args\"][\"graph_construction_share\"][\"root_dir\"],\n",
    "            topology_subdir=topology_subdir,\n",
    "            graph_name=self.graph_name,\n",
    "            dynamic_init_graph_name=self.config[\"graph_construction_args\"][\n",
    "                \"graph_construction_private\"\n",
    "            ][\"dynamic_init_graph_name\"],\n",
    "            dynamic_init_topology_aux_args={\"dummy_param\": 0},\n",
    "            pretrained_word_emb_name=self.config[\"pretrained_word_emb_name\"],\n",
    "            merge_strategy=self.config[\"graph_construction_args\"][\"graph_construction_private\"][\n",
    "                \"merge_strategy\"\n",
    "            ],\n",
    "            edge_strategy=self.config[\"graph_construction_args\"][\"graph_construction_private\"][\n",
    "                \"edge_strategy\"\n",
    "            ],\n",
    "            min_word_vocab_freq=self.config.get(\"min_word_freq\", 1),\n",
    "            word_emb_size=self.config.get(\"word_emb_size\", 300),\n",
    "            seed=self.config[\"seed\"],\n",
    "            thread_number=self.config[\"graph_construction_args\"][\"graph_construction_share\"][\n",
    "                \"thread_number\"\n",
    "            ],\n",
    "            port=self.config[\"graph_construction_args\"][\"graph_construction_share\"][\"port\"],\n",
    "            timeout=self.config[\"graph_construction_args\"][\"graph_construction_share\"][\"timeout\"],\n",
    "            reused_label_model=None,\n",
    "        )\n",
    "\n",
    "        self.train_dataloader = DataLoader(\n",
    "            dataset.train,\n",
    "            batch_size=self.config[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            num_workers=self.config[\"num_workers\"],\n",
    "            collate_fn=dataset.collate_fn,\n",
    "        )\n",
    "        if not hasattr(dataset, \"val\"):\n",
    "            dataset.val = dataset.test\n",
    "        self.val_dataloader = DataLoader(\n",
    "            dataset.val,\n",
    "            batch_size=self.config[\"batch_size\"],\n",
    "            shuffle=False,\n",
    "            num_workers=self.config[\"num_workers\"],\n",
    "            collate_fn=dataset.collate_fn,\n",
    "        )\n",
    "        self.test_dataloader = DataLoader(\n",
    "            dataset.test,\n",
    "            batch_size=self.config[\"batch_size\"],\n",
    "            shuffle=False,\n",
    "            num_workers=self.config[\"num_workers\"],\n",
    "            collate_fn=dataset.collate_fn,\n",
    "        )\n",
    "        self.vocab_model = dataset.vocab_model\n",
    "        self.label_model = dataset.label_model\n",
    "        self.config[\"num_classes\"] = self.label_model.num_classes\n",
    "        self.num_train = len(dataset.train)\n",
    "        self.num_val = len(dataset.val)\n",
    "        self.num_test = len(dataset.test)\n",
    "        print(\n",
    "            \"Train size: {}, Val size: {}, Test size: {}\".format(\n",
    "                self.num_train, self.num_val, self.num_test\n",
    "            )\n",
    "        )\n",
    "        self.logger.write(\n",
    "            \"Train size: {}, Val size: {}, Test size: {}\".format(\n",
    "                self.num_train, self.num_val, self.num_test\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def _build_model(self):\n",
    "        self.model = TextClassifier(self.vocab_model, self.label_model, self.config).to(\n",
    "            self.config[\"device\"]\n",
    "        )\n",
    "\n",
    "    def _build_optimizer(self):\n",
    "        parameters = [p for p in self.model.parameters() if p.requires_grad]\n",
    "        self.optimizer = optim.Adam(parameters, lr=self.config[\"lr\"])\n",
    "        self.stopper = EarlyStopping(\n",
    "            os.path.join(\n",
    "                self.config[\"out_dir\"],\n",
    "                self.config.get(\"model_ckpt_name\", Constants._SAVED_WEIGHTS_FILE),\n",
    "            ),\n",
    "            patience=self.config[\"patience\"],\n",
    "        )\n",
    "        self.scheduler = ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode=\"max\",\n",
    "            factor=self.config[\"lr_reduce_factor\"],\n",
    "            patience=self.config[\"lr_patience\"],\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    def _build_evaluation(self):\n",
    "        self.metric = Accuracy([\"accuracy\"])\n",
    "\n",
    "    def train(self):\n",
    "        dur = []\n",
    "        for epoch in range(self.config[\"epochs\"]):\n",
    "            self.model.train()\n",
    "            train_loss = []\n",
    "            train_acc = []\n",
    "            t0 = time.time()\n",
    "            for data in self.train_dataloader:\n",
    "                tgt = to_cuda(data[\"tgt_tensor\"], self.config[\"device\"])\n",
    "                data[\"graph_data\"] = data[\"graph_data\"].to(self.config[\"device\"])\n",
    "                logits, loss = self.model(data[\"graph_data\"], tgt, require_loss=True)\n",
    "\n",
    "                # add graph regularization loss if available\n",
    "                if data[\"graph_data\"].graph_attributes.get(\"graph_reg\", None) is not None:\n",
    "                    loss = loss + data[\"graph_data\"].graph_attributes[\"graph_reg\"]\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "                pred = torch.max(logits, dim=-1)[1].cpu()\n",
    "                train_acc.append(\n",
    "                    self.metric.calculate_scores(ground_truth=tgt.cpu(), predict=pred.cpu())[0]\n",
    "                )\n",
    "                dur.append(time.time() - t0)\n",
    "\n",
    "            val_acc = self.evaluate(self.val_dataloader)\n",
    "            self.scheduler.step(val_acc)\n",
    "            print(\n",
    "                \"Epoch: [{} / {}] | Time: {:.2f}s | Loss: {:.4f} |\"\n",
    "                \"Train Acc: {:.4f} | Val Acc: {:.4f}\".format(\n",
    "                    epoch + 1,\n",
    "                    self.config[\"epochs\"],\n",
    "                    np.mean(dur),\n",
    "                    np.mean(train_loss),\n",
    "                    np.mean(train_acc),\n",
    "                    val_acc,\n",
    "                )\n",
    "            )\n",
    "            self.logger.write(\n",
    "                \"Epoch: [{} / {}] | Time: {:.2f}s | Loss: {:.4f} |\"\n",
    "                \"Train Acc: {:.4f} | Val Acc: {:.4f}\".format(\n",
    "                    epoch + 1,\n",
    "                    self.config[\"epochs\"],\n",
    "                    np.mean(dur),\n",
    "                    np.mean(train_loss),\n",
    "                    np.mean(train_acc),\n",
    "                    val_acc,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if self.stopper.step(val_acc, self.model):\n",
    "                break\n",
    "\n",
    "        return self.stopper.best_score\n",
    "\n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_collect = []\n",
    "            gt_collect = []\n",
    "            for data in dataloader:\n",
    "                tgt = to_cuda(data[\"tgt_tensor\"], self.config[\"device\"])\n",
    "                data[\"graph_data\"] = data[\"graph_data\"].to(self.config[\"device\"])\n",
    "                logits = self.model(data[\"graph_data\"], require_loss=False)\n",
    "                pred_collect.append(logits)\n",
    "                gt_collect.append(tgt)\n",
    "\n",
    "            pred_collect = torch.max(torch.cat(pred_collect, 0), dim=-1)[1].cpu()\n",
    "            gt_collect = torch.cat(gt_collect, 0).cpu()\n",
    "            score = self.metric.calculate_scores(ground_truth=gt_collect, predict=pred_collect)[0]\n",
    "\n",
    "            return score\n",
    "\n",
    "    def test(self):\n",
    "        # restored best saved model\n",
    "        self.model = TextClassifier.load_checkpoint(self.stopper.save_model_path)\n",
    "\n",
    "        t0 = time.time()\n",
    "        acc = self.evaluate(self.test_dataloader)\n",
    "        dur = time.time() - t0\n",
    "        print(\n",
    "            \"Test examples: {} | Time: {:.2f}s |  Test Acc: {:.4f}\".format(self.num_test, dur, acc)\n",
    "        )\n",
    "        self.logger.write(\n",
    "            \"Test examples: {} | Time: {:.2f}s |  Test Acc: {:.4f}\".format(self.num_test, dur, acc)\n",
    "        )\n",
    "\n",
    "        return acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def print_config(config):\n",
    "    print('**************** MODEL CONFIGURATION ****************')\n",
    "    for key in sorted(config.keys()):\n",
    "        val = config[key]\n",
    "        keystr = '{}'.format(key) + (' ' * (24 - len(key)))\n",
    "        print('{} -->   {}'.format(keystr, val))\n",
    "    print('**************** MODEL CONFIGURATION ****************')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************** MODEL CONFIGURATION ****************\n",
      "batch_size               -->   50\n",
      "dataset                  -->   imdb\n",
      "emb_strategy             -->   w2v_bilstm\n",
      "epochs                   -->   500\n",
      "gat_attn_dropout         -->   None\n",
      "gat_negative_slope       -->   None\n",
      "gat_num_heads            -->   None\n",
      "gat_num_out_heads        -->   None\n",
      "gat_residual             -->   False\n",
      "gl_connectivity_ratio    -->   None\n",
      "gl_epsilon               -->   None\n",
      "gl_metric_type           -->   None\n",
      "gl_num_heads             -->   1\n",
      "gl_num_hidden            -->   300\n",
      "gl_smoothness_ratio      -->   None\n",
      "gl_sparsity_ratio        -->   None\n",
      "gl_top_k                 -->   None\n",
      "gnn                      -->   graphsage\n",
      "gnn_direction_option     -->   bi_fuse\n",
      "gnn_dropout              -->   0.4\n",
      "gnn_num_layers           -->   1\n",
      "gpu                      -->   -1\n",
      "graph_construction_args  -->   {'graph_construction_share': {'graph_name': 'dependency', 'root_dir': './data/imdb', 'thread_number': 10, 'port': 9000, 'timeout': 15000}, 'graph_construction_private': {'edge_strategy': 'homogeneous', 'merge_strategy': 'tailhead', 'sequential_link': True, 'as_node': False, 'dynamic_init_graph_name': None}}\n",
      "graph_pooling            -->   avg_pool\n",
      "graphsage_aggreagte_type -->   lstm\n",
      "init_adj_alpha           -->   None\n",
      "lr                       -->   0.001\n",
      "lr_patience              -->   2\n",
      "lr_reduce_factor         -->   0.5\n",
      "max_pool_linear_proj     -->   False\n",
      "no_cuda                  -->   False\n",
      "no_fix_word_emb          -->   False\n",
      "num_hidden               -->   300\n",
      "num_workers              -->   0\n",
      "out_dir                  -->   out/imdb/graphsage_bi_fuse_dependency_ckpt\n",
      "patience                 -->   10\n",
      "pretrained_word_emb_name -->   840B\n",
      "rnn_dropout              -->   0.1\n",
      "seed                     -->   1234\n",
      "val_split_ratio          -->   0.2\n",
      "word_dropout             -->   0.4\n",
      "**************** MODEL CONFIGURATION ****************\n"
     ]
    }
   ],
   "source": [
    "# config setup\n",
    "import platform, multiprocessing\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    multiprocessing.set_start_method('spawn')\n",
    "\n",
    "config_file = 'D:\\\\Notebook\\\\Graph\\\\config\\\\imdb\\\\graphsage_bi_fuse_static_dependency_v2.yaml'\n",
    "config = yaml.load(open(config_file, 'r'), Loader=yaml.FullLoader)\n",
    "print_config(config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Using CUDA ]\n",
      "\n",
      "out/imdb/graphsage_bi_fuse_dependency_ckpt_1659658536.232536\n"
     ]
    }
   ],
   "source": [
    "# run model\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(config['seed'])\n",
    "torch.manual_seed(config['seed'])\n",
    "\n",
    "if not config[\"no_cuda\"] and torch.cuda.is_available():\n",
    "    print(\"[ Using CUDA ]\")\n",
    "    config[\"device\"] = torch.device(\"cuda\" if config[\"gpu\"] < 0 else \"cuda:%d\" % config[\"gpu\"])\n",
    "    torch.cuda.manual_seed(config[\"seed\"])\n",
    "    torch.cuda.manual_seed_all(config[\"seed\"])\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    cudnn.benchmark = False\n",
    "else:\n",
    "    config[\"device\"] = torch.device(\"cpu\")\n",
    "\n",
    "ts = datetime.datetime.now().timestamp()\n",
    "config['out_dir'] += '_{}'.format(ts)\n",
    "print('\\n' + config['out_dir'])\n",
    "\n",
    "runner = ModelHandler(config)\n",
    "t0 = time.time()\n",
    "\n",
    "val_acc = runner.train()\n",
    "test_acc = runner.test()\n",
    "\n",
    "runtime = time.time() - t0\n",
    "print('Total runtime: {:.2f}s'.format(runtime))\n",
    "runner.logger.write('Total runtime: {:.2f}s\\n'.format(runtime))\n",
    "runner.logger.close()\n",
    "print('val acc: {}, test acc: {}'.format(val_acc, test_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pygeo",
   "language": "python",
   "display_name": "pygeo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}