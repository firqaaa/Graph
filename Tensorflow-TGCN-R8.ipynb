{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Utils"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def uniform(shape, scale=0.5, name=None):\n",
    "    \"\"\"Uniform init.\"\"\"\n",
    "    initial = tf.random.uniform(shape, minval=-scale, maxval=scale, dtype=tf.float64)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def glorot(shape, name=None):\n",
    "    \"\"\"Glorot & Bengio init.\"\"\"\n",
    "    init_range = np.sqrt(6.0/(shape[0]+shape[1]))\n",
    "    initial = tf.random.uniform(shape=shape, minval=-init_range, maxval=init_range, dtype=tf.float64)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def zeros(shape, name=None):\n",
    "    \"\"\"All zeros\"\"\"\n",
    "    initial = tf.zeros(shape, dtype=tf.float64)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def ones(shape, name=None):\n",
    "    \"\"\"All ones\"\"\"\n",
    "    initial = tf.ones(shape, dtype=tf.float64)\n",
    "    return tf.Variable(initial, name=name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def masked_softmax_cross_entropy(preds, labels, mask):\n",
    "    \"\"\"Softmax cross-entropy loss with masking\"\"\"\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    loss *= mask\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def masked_accuracy(preds, labels, mask):\n",
    "    \"\"\"Accuracy with masking\"\"\"\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1))\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    accuracy_all *= mask\n",
    "    return tf.reduce_mean(accuracy_all)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import eigsh\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def load_corpus(dataset_str):\n",
    "    \"\"\"\n",
    "    Loads input corpus from gcn/data directory\n",
    "    ind.dataset_str.x => the feature vectors of the training docs as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.tx => the feature vectors of the test docs as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training docs/words\n",
    "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.y => the one-hot labels of the labeled training docs as numpy.ndarray object;\n",
    "    ind.dataset_str.ty => the one-hot labels of the test docs as numpy.ndarray object;\n",
    "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
    "    ind.dataset_str.adj => adjacency matrix of word/doc nodes as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.train.index => the indices of training docs in original doc list.\n",
    "    All objects above must be saved using python pickle module.\n",
    "    :param dataset_str: Dataset name\n",
    "    :return: All data input files loaded (as well the training/test data).\n",
    "    \"\"\"\n",
    "\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'adj']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open('cleaned_data/' + dataset_str + '/graph/ind.' + dataset_str + '.' + names[i], 'rb') as f:\n",
    "            # with open(\"./data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, adj = tuple(objects)\n",
    "    # print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    labels = np.vstack((ally, ty))\n",
    "    # print(len(labels))\n",
    "\n",
    "    train_idx_orig = parse_index_file('cleaned_data/' + dataset_str + '/graph/' + dataset_str + '.train.index')\n",
    "    train_size = len(train_idx_orig)\n",
    "\n",
    "    val_size = train_size - x.shape[0]\n",
    "    test_size = tx.shape[0]\n",
    "\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y) + val_size)\n",
    "    idx_test = range(allx.shape[0], allx.shape[0] + test_size)\n",
    "\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size\n",
    "\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "\n",
    "    return sparse_to_tuple(features)\n",
    "\n",
    "\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "\n",
    "\n",
    "def construct_feed_dict(features, support, labels, labels_mask, placeholders):\n",
    "    \"\"\"Construct feed dictionary.\"\"\"\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['labels']: labels})\n",
    "    feed_dict.update({placeholders['labels_mask']: labels_mask})\n",
    "    feed_dict.update({placeholders['features']: features})\n",
    "    feed_dict.update({placeholders['support'][i]: support[i]\n",
    "                      for i in range(len(support))})\n",
    "    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def loadWord2Vec(filename):\n",
    "    \"\"\"Read Word Vectors\"\"\"\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    word_vector_map = {}\n",
    "    file = open(filename, 'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        if(len(row) > 2):\n",
    "            vocab.append(row[0])\n",
    "            vector = row[1:]\n",
    "            length = len(vector)\n",
    "            for i in range(length):\n",
    "                vector[i] = float(vector[i])\n",
    "            embd.append(vector)\n",
    "            word_vector_map[row[0]] = vector\n",
    "\n",
    "    print('Loaded Word Vectors!')\n",
    "    file.close()\n",
    "    return vocab, embd, word_vector_map\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\firqa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "# Remove words\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "#\n",
    "# if len(sys.argv) != 2:\n",
    "#     sys.exit(\"Use: python remove_words.py <dataset>\")\n",
    "\n",
    "dataset = \"R8\"\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "doc_content_list = []\n",
    "if dataset == 'CHINESE' or dataset == 'THUCTC':\n",
    "    with open('./cleaned_data/' + dataset + '/corpus/' + dataset + '.txt', 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            doc_content_list.append(line.strip())\n",
    "else:\n",
    "    with open('./cleaned_data/' + dataset + '/corpus/' + dataset + '.txt', 'rb') as f:\n",
    "        for line in f.readlines():\n",
    "            doc_content_list.append(line.strip().decode('latin1'))\n",
    "\n",
    "word_freq = {}  # to remove rare words\n",
    "\n",
    "for doc_content in doc_content_list:\n",
    "    if dataset == 'CHINESE' or dataset == 'THUCTC':\n",
    "        temp = doc_content\n",
    "    else:\n",
    "        temp = clean_str(doc_content)\n",
    "\n",
    "    words = temp.split()\n",
    "    for word in words:\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "\n",
    "clean_docs = []\n",
    "for doc_content in doc_content_list:\n",
    "    if dataset == 'CHINESE' or dataset == 'THUCTC':\n",
    "        temp = doc_content\n",
    "    else:\n",
    "        temp = clean_str(doc_content)\n",
    "    words = temp.split()\n",
    "    doc_words = []\n",
    "    for word in words:\n",
    "        # word not in stop_words and word_freq[word] >= 5\n",
    "        if dataset == 'mr':\n",
    "            doc_words.append(word)\n",
    "        elif word not in stop_words and word_freq[word] >= 5:\n",
    "            doc_words.append(word)\n",
    "\n",
    "    doc_str = ' '.join(doc_words).strip()\n",
    "    clean_docs.append(doc_str)\n",
    "\n",
    "clean_corpus_str = '\\n'.join(clean_docs)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/' + dataset + '_clean.txt', 'w') as f:\n",
    "    f.write(clean_corpus_str)\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size 7674\n",
      "adding...\n",
      "building...\n",
      "(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)\n"
     ]
    }
   ],
   "source": [
    "## Build Graph\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from math import log\n",
    "from sklearn import svm\n",
    "import scipy.sparse as sp\n",
    "from nltk.corpus import wordnet as wn\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "dataset = \"R8\"\n",
    "\n",
    "word_embeddings_dim = 300\n",
    "word_vector_map = {}\n",
    "\n",
    "# shuffling\n",
    "doc_name_list = []\n",
    "doc_train_list = []\n",
    "doc_test_list = []\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/' + dataset + '.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        doc_name_list.append(line.strip())\n",
    "        temp = line.split(\"\\t\")\n",
    "        if temp[1].find('test') != -1:\n",
    "            doc_test_list.append(line.strip())\n",
    "        elif temp[1].find('train') != -1:\n",
    "            doc_train_list.append(line.strip())\n",
    "\n",
    "doc_content_list = []\n",
    "with open('./cleaned_data/' + dataset + '/' + dataset + '_clean.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        doc_content_list.append(line.strip())\n",
    "\n",
    "train_ids = []\n",
    "for train_name in doc_train_list:\n",
    "    train_id = doc_name_list.index(train_name)\n",
    "    train_ids.append(train_id)\n",
    "\n",
    "random.shuffle(train_ids)\n",
    "\n",
    "train_ids_str = '\\n'.join(str(index) for index in train_ids)\n",
    "with open('./cleaned_data/' + dataset + '/graph/' + dataset + '.train.index', 'w') as f:\n",
    "    f.write(train_ids_str)\n",
    "\n",
    "test_ids = []\n",
    "for test_name in doc_test_list:\n",
    "    test_id = doc_name_list.index(test_name)\n",
    "    test_ids.append(test_id)\n",
    "\n",
    "random.shuffle(test_ids)\n",
    "\n",
    "test_ids_str = '\\n'.join(str(index) for index in test_ids)\n",
    "with open('./cleaned_data/' + dataset + '/graph/' + dataset + '.test.index', 'w') as f:\n",
    "    f.write(test_ids_str)\n",
    "\n",
    "ids = train_ids + test_ids\n",
    "\n",
    "print(\"data size {}\".format(len(ids)))\n",
    "\n",
    "print(\"adding...\")\n",
    "shuffle_doc_name_list = []\n",
    "shuffle_doc_words_list = []\n",
    "for id in ids:\n",
    "    shuffle_doc_name_list.append(doc_name_list[int(id)])\n",
    "    shuffle_doc_words_list.append(doc_content_list[int(id)])\n",
    "shuffle_doc_name_str = '\\n'.join(shuffle_doc_name_list)\n",
    "shuffle_doc_words_str = '\\n'.join(shuffle_doc_words_list)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/' + dataset + '_shuffle.txt', 'w') as f:\n",
    "    f.write(shuffle_doc_name_str)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/corpus/' + dataset + '_shuffle.txt', 'w') as f:\n",
    "    f.write(shuffle_doc_words_str)\n",
    "\n",
    "# build vocab\n",
    "print('building...')\n",
    "word_freq = {}\n",
    "word_set = set()\n",
    "for doc_words in shuffle_doc_words_list:\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_set.add(word)\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "\n",
    "vocab = list(word_set)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_doc_list = {}\n",
    "\n",
    "for i in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    appeared = set()\n",
    "    for word in words:\n",
    "        if word in appeared:\n",
    "            continue\n",
    "        if word in word_doc_list:\n",
    "            doc_list = word_doc_list[word]\n",
    "            doc_list.append(i)\n",
    "            word_doc_list[word] = doc_list\n",
    "        else:\n",
    "            word_doc_list[word] = [i]\n",
    "        appeared.add(word)\n",
    "\n",
    "word_doc_freq = {}\n",
    "for word, doc_list in word_doc_list.items():\n",
    "    word_doc_freq[word] = len(doc_list)\n",
    "\n",
    "word_id_map = {}\n",
    "for i in range(vocab_size):\n",
    "    word_id_map[vocab[i]] = i\n",
    "\n",
    "vocab_str = '\\n'.join(vocab)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/corpus/' + dataset + '_vocab.txt', 'w') as f:\n",
    "    f.write(vocab_str)\n",
    "\n",
    "# label list\n",
    "label_set = set()\n",
    "for doc_meta in shuffle_doc_name_list:\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label_set.add(temp[2])\n",
    "label_list = list(label_set)\n",
    "\n",
    "label_list_str = '\\n'.join(label_list)\n",
    "with open('./cleaned_data/' + dataset + '/corpus/' + dataset + '_labels.txt', 'w') as f:\n",
    "    f.write(label_list_str)\n",
    "\n",
    "# x: feature vectors of training docs, no initial features\n",
    "# slect 90% training set\n",
    "train_size = len(train_ids)\n",
    "val_size = int(0.1 * train_size)\n",
    "real_train_size = train_size - val_size\n",
    "\n",
    "real_train_doc_names = shuffle_doc_name_list[:real_train_size]\n",
    "real_train_doc_names_str = '\\n'.join(real_train_doc_names)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/graph/' + dataset + '.real_train.name', 'w') as f:\n",
    "    f.write(real_train_doc_names_str)\n",
    "\n",
    "row_x = []\n",
    "col_x = []\n",
    "data_x = []\n",
    "for i in range(real_train_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_x.append(i)\n",
    "        col_x.append(j)\n",
    "        data_x.append(doc_vec[j] / doc_len)\n",
    "\n",
    "x = sp.csr_matrix((data_x, (row_x, col_x)), shape=(\n",
    "    real_train_size, word_embeddings_dim\n",
    "))\n",
    "\n",
    "y = []\n",
    "for i in range(real_train_size):\n",
    "    doc_meta = shuffle_doc_name_list[i]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    y.append(one_hot)\n",
    "y = np.array(y)\n",
    "\n",
    "test_size = len(test_ids)\n",
    "\n",
    "row_tx = []\n",
    "col_tx = []\n",
    "data_tx = []\n",
    "for i in range(test_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i + train_size]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_tx.append(i)\n",
    "        col_tx.append(j)\n",
    "        data_tx.append(doc_vec[j] / doc_len)\n",
    "\n",
    "tx = sp.csr_matrix((data_tx, (row_tx, col_tx)),\n",
    "                   shape=(test_size, word_embeddings_dim))\n",
    "\n",
    "ty = []\n",
    "for i in range(test_size):\n",
    "    doc_meta = shuffle_doc_name_list[i + train_size]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    ty.append(one_hot)\n",
    "ty = np.array(ty)\n",
    "\n",
    "word_vectors = np.random.uniform(-0.01, 0.01,\n",
    "                                 (vocab_size, word_embeddings_dim))\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "    word = vocab[i]\n",
    "    if word in word_vector_map:\n",
    "        vector = word_vector_map[word]\n",
    "        word_vectors[i] = vector\n",
    "\n",
    "row_allx = []\n",
    "col_allx = []\n",
    "data_allx = []\n",
    "\n",
    "for i in range(train_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_allx.append(int(i))\n",
    "        col_allx.append(j)\n",
    "        data_allx.append(doc_vec[j] / doc_len)\n",
    "for i in range(vocab_size):\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_allx.append(int(i + train_size))\n",
    "        col_allx.append(j)\n",
    "        data_allx.append(word_vectors.item((i, j)))\n",
    "\n",
    "row_allx = np.array(row_allx)\n",
    "col_allx = np.array(col_allx)\n",
    "data_allx = np.array(data_allx)\n",
    "\n",
    "allx = sp.csr_matrix(\n",
    "    (data_allx, (row_allx, col_allx)), shape=(train_size + vocab_size, word_embeddings_dim)\n",
    ")\n",
    "\n",
    "ally = []\n",
    "for i in range(train_size):\n",
    "    doc_meta = shuffle_doc_name_list[i]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    ally.append(one_hot)\n",
    "\n",
    "for i in range(vocab_size):\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    ally.append(one_hot)\n",
    "\n",
    "ally = np.array(ally)\n",
    "\n",
    "print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
    "\n",
    "'''\n",
    "Doc word heterogeneous graph\n",
    "'''\n",
    "\n",
    "# word co-occurence with context windows\n",
    "window_size = 20\n",
    "windows = []\n",
    "\n",
    "for doc_words in shuffle_doc_words_list:\n",
    "    words = doc_words.split()\n",
    "    length = len(words)\n",
    "    if length <= window_size:\n",
    "        windows.append(words)\n",
    "    else:\n",
    "        for j in range(length - window_size + 1):\n",
    "            window = words[j: j + window_size]\n",
    "            windows.append(window)\n",
    "\n",
    "word_window_freq = {}\n",
    "for window in windows:\n",
    "    appeared = set()\n",
    "    for i in range(len(window)):\n",
    "        if window[i] in appeared:\n",
    "            continue\n",
    "        if window[i] in word_window_freq:\n",
    "            word_window_freq[window[i]] += 1\n",
    "        else:\n",
    "            word_window_freq[window[i]] = 1\n",
    "        appeared.add(window[i])\n",
    "\n",
    "word_pair_count = {}\n",
    "for window in windows:\n",
    "    for i in range(1, len(window)):\n",
    "        for j in range(0, i):\n",
    "            word_i = window[i]\n",
    "            word_i_id = word_id_map[word_i]\n",
    "            word_j = window[j]\n",
    "            word_j_id = word_id_map[word_j]\n",
    "            if word_i_id == word_j_id:\n",
    "                continue\n",
    "            word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n",
    "            # two orders\n",
    "            word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n",
    "\n",
    "row = []\n",
    "col = []\n",
    "weight = []\n",
    "\n",
    "# pmi as weights\n",
    "num_windows = len(windows)\n",
    "for key in word_pair_count:\n",
    "    temp = key.split(',')\n",
    "    i = int(temp[0])\n",
    "    j = int(temp[1])\n",
    "    count = word_pair_count[key]\n",
    "    word_freq_i = word_window_freq[vocab[i]]\n",
    "    word_freq_j = word_window_freq[vocab[j]]\n",
    "    pmi = log((1.0 * count / num_windows) /\n",
    "              (1.0 * word_freq_i * word_freq_j / (num_windows * num_windows)))\n",
    "    if pmi <= 0:\n",
    "        continue\n",
    "    row.append(train_size + i)\n",
    "    col.append(train_size + j)\n",
    "    weight.append(pmi)\n",
    "\n",
    "# doc word frequency\n",
    "doc_word_freq = {}\n",
    "\n",
    "for doc_id in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[doc_id]\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_id = word_id_map[word]\n",
    "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
    "        if doc_word_str in doc_word_freq:\n",
    "            doc_word_freq[doc_word_str] += 1\n",
    "        else:\n",
    "            doc_word_freq[doc_word_str] = 1\n",
    "\n",
    "for i in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_word_set = set()\n",
    "    for word in words:\n",
    "        if word in doc_word_set:\n",
    "            continue\n",
    "        j = word_id_map[word]\n",
    "        key = str(i) + ',' + str(j)\n",
    "        freq = doc_word_freq[key]\n",
    "        if i < train_size:\n",
    "            row.append(i)\n",
    "        else:\n",
    "            row.append(i + vocab_size)\n",
    "        col.append(train_size + j)\n",
    "        idf = log(1.0 * len(shuffle_doc_words_list) /\n",
    "                  word_doc_freq[vocab[j]])\n",
    "        weight.append(freq * idf)\n",
    "        doc_word_set.add(word)\n",
    "\n",
    "node_size = train_size + vocab_size + test_size\n",
    "adj = sp.csr_matrix((weight, (row, col)), shape=(node_size, node_size))\n",
    "\n",
    "# dump objects\n",
    "with open('./cleaned_data/' + dataset + '/graph/ind.' + dataset + '.x', 'wb') as f:\n",
    "    pkl.dump(x, f)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/graph/ind.' + dataset + '.y', 'wb') as f:\n",
    "    pkl.dump(y, f)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/graph/ind.' + dataset + '.tx', 'wb') as f:\n",
    "    pkl.dump(tx, f)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/graph/ind.' + dataset + '.ty', 'wb') as f:\n",
    "    pkl.dump(ty, f)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/graph/ind.' + dataset + '.allx', 'wb') as f:\n",
    "    pkl.dump(allx, f)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/graph/ind.' + dataset + '.ally', 'wb') as f:\n",
    "    pkl.dump(ally, f)\n",
    "\n",
    "with open('./cleaned_data/' + dataset + '/graph/ind.' + dataset + '.adj', 'wb') as f:\n",
    "    pkl.dump(adj, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Layers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# global unique layer ID dictionary for layer name assignment\n",
    "_LAYER_UIDS = {}\n",
    "\n",
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"Helper function, assign unique layer IDs\"\"\"\n",
    "    if layer_name not in _LAYER_UIDS:\n",
    "        _LAYER_UIDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_UIDS[layer_name] += 1\n",
    "        return _LAYER_UIDS[layer_name]\n",
    "\n",
    "def sparse_dropout(x, rate, noise_shape):\n",
    "    \"\"\"Dropout for sparse tensors.\"\"\"\n",
    "    random_tensor = rate\n",
    "    random_tensor += tf.random.uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse.retain(x, dropout_mask)\n",
    "    return pre_out * (1./(rate))\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
    "    if sparse:\n",
    "        res = tf.sparse.sparse_dense_matmul(x, y)\n",
    "    else:\n",
    "        res = tf.matmul(x, y)\n",
    "    return res\n",
    "\n",
    "class GraphConvolution(layers.Layer):\n",
    "    \"\"\"Graph Convolutional Layer\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, num_features_nonzero,\n",
    "                 dropout=0.,\n",
    "                 is_sparse_inputs=False,\n",
    "                 activation=tf.nn.relu,\n",
    "                 bias=False,\n",
    "                 featureless=False, **kwargs):\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.is_sparse_inputs = is_sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "        self.num_features_nonzero = num_features_nonzero\n",
    "        self.embedding = None\n",
    "\n",
    "        self.weights_ = []\n",
    "        for i in range(1):\n",
    "            w = self.add_variable('weight' + str(i), [input_dim, output_dim])\n",
    "            self.weights_.append(w)\n",
    "        if self.bias:\n",
    "            self.bias = self.add_variable('bias', [output_dim])\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x, support_ = inputs\n",
    "        # dropout\n",
    "        if training is not False and self.is_sparse_inputs:\n",
    "            x = sparse_dropout(x, self.dropout, self.num_features_nonzero)\n",
    "        elif training is not False:\n",
    "            x = tf.nn.dropout(x, self.dropout)\n",
    "\n",
    "        # convolve\n",
    "        supports = list()\n",
    "        for i in range(len(support_)):\n",
    "            if not self.featureless:\n",
    "                pre_sup = dot(x, self.weights_[i], sparse=self.is_sparse_inputs)\n",
    "            else:\n",
    "                pre_sup = self.weights_[i]\n",
    "\n",
    "            support = dot(support_[i], pre_sup, sparse=True)\n",
    "            supports.append(support)\n",
    "        output = tf.add_n(supports)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.bias\n",
    "\n",
    "        self.embedding = output\n",
    "        return self.activation(output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class CONFIG(object):\n",
    "    \"\"\"docstring for CONFIG\"\"\"\n",
    "    def __init__(self):\n",
    "        super(CONFIG, self).__init__()\n",
    "\n",
    "        self.dataset = 'R8'\n",
    "        self.learning_rate = 0.02\n",
    "        self.epochs = 100\n",
    "        self.hidden1 = 200\n",
    "        self.dropout = 0.5\n",
    "        self.weight_decay = 0.\n",
    "        self.early_stopping = 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "cfg = CONFIG()\n",
    "\n",
    "class GCN(keras.Model):\n",
    "    def __init__(self, input_dim, output_dim, num_features_nonzero, **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        print('input dim: ', input_dim)\n",
    "        print('output dim: ', output_dim)\n",
    "\n",
    "        self.layers_ = []\n",
    "        self.layers_.append(GraphConvolution(\n",
    "            input_dim=self.input_dim,\n",
    "            output_dim=cfg.hidden1,\n",
    "            num_features_nonzero=num_features_nonzero,\n",
    "            activation=tf.nn.relu,\n",
    "            dropout=cfg.dropout,\n",
    "            is_sparse_inputs=True\n",
    "        ))\n",
    "\n",
    "        self.layers_.append(GraphConvolution(\n",
    "            input_dim=cfg.hidden1,\n",
    "            output_dim=self.output_dim,\n",
    "            num_features_nonzero=num_features_nonzero,\n",
    "            activation=lambda x: x,\n",
    "            dropout=cfg.dropout\n",
    "        ))\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        x, label, mask, support = inputs\n",
    "        outputs = [x]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            hidden = layer((outputs[-1], support), training)\n",
    "            outputs.append(hidden)\n",
    "        output = outputs[-1]\n",
    "\n",
    "        # Weight decay loss\n",
    "        loss = tf.zeros([])\n",
    "        for var in self.layers_[0].trainable_variables:\n",
    "            loss += cfg.weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        loss += masked_softmax_cross_entropy(output, label, mask)\n",
    "        acc = masked_accuracy(output, label, mask)\n",
    "\n",
    "        return tf.argmax(output, 1), loss, acc\n",
    "\n",
    "    def predict(self, **kwargs):\n",
    "        return tf.nn.softmax(self.outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\firqa\\AppData\\Local\\Temp\\ipykernel_704\\281618087.py:19: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return np.array(mask, dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dim:  15362\n",
      "output dim:  8\n",
      "WARNING:tensorflow:From C:\\Users\\firqa\\AppData\\Local\\Temp\\ipykernel_704\\3256699056.py:53: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:Layer gcn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch: 0001 train_loss= 2.07953 train_acc= 0.08284 val_loss= 2.06356 val_acc= 0.58942 time= 8.95331\n",
      "Epoch: 0002 train_loss= 2.06251 train_acc= 0.60806 val_loss= 2.02666 val_acc= 0.58759 time= 5.44461\n",
      "Epoch: 0003 train_loss= 2.02229 train_acc= 0.61211 val_loss= 1.96687 val_acc= 0.56204 time= 5.07563\n",
      "Epoch: 0004 train_loss= 1.96505 train_acc= 0.61049 val_loss= 1.88384 val_acc= 0.54745 time= 4.99724\n",
      "Epoch: 0005 train_loss= 1.87247 train_acc= 0.56350 val_loss= 1.78033 val_acc= 0.53467 time= 4.28908\n",
      "Epoch: 0006 train_loss= 1.77009 train_acc= 0.55297 val_loss= 1.66267 val_acc= 0.53285 time= 4.27607\n",
      "Epoch: 0007 train_loss= 1.65315 train_acc= 0.53838 val_loss= 1.54221 val_acc= 0.53102 time= 5.03658\n",
      "Epoch: 0008 train_loss= 1.51158 train_acc= 0.53535 val_loss= 1.43213 val_acc= 0.53102 time= 4.49088\n",
      "Epoch: 0009 train_loss= 1.40207 train_acc= 0.53535 val_loss= 1.34308 val_acc= 0.53102 time= 4.07147\n",
      "Epoch: 0010 train_loss= 1.30320 train_acc= 0.53656 val_loss= 1.27677 val_acc= 0.53650 time= 4.27019\n",
      "Epoch: 0011 train_loss= 1.23957 train_acc= 0.54041 val_loss= 1.22743 val_acc= 0.54927 time= 5.73988\n",
      "Epoch: 0012 train_loss= 1.19226 train_acc= 0.56796 val_loss= 1.18522 val_acc= 0.58577 time= 5.54507\n",
      "Epoch: 0013 train_loss= 1.13772 train_acc= 0.62406 val_loss= 1.14304 val_acc= 0.63321 time= 4.52060\n",
      "Epoch: 0014 train_loss= 1.09548 train_acc= 0.66761 val_loss= 1.09714 val_acc= 0.69343 time= 3.77287\n",
      "Epoch: 0015 train_loss= 1.05119 train_acc= 0.70286 val_loss= 1.04720 val_acc= 0.72628 time= 6.82092\n",
      "Epoch: 0016 train_loss= 1.00053 train_acc= 0.75512 val_loss= 0.99475 val_acc= 0.75547 time= 6.05436\n",
      "Epoch: 0017 train_loss= 0.93646 train_acc= 0.76848 val_loss= 0.94238 val_acc= 0.76095 time= 4.88773\n",
      "Epoch: 0018 train_loss= 0.89595 train_acc= 0.78347 val_loss= 0.89242 val_acc= 0.76460 time= 4.73543\n",
      "Epoch: 0019 train_loss= 0.84610 train_acc= 0.78226 val_loss= 0.84670 val_acc= 0.76277 time= 4.90353\n",
      "Epoch: 0020 train_loss= 0.79821 train_acc= 0.78489 val_loss= 0.80636 val_acc= 0.76277 time= 6.07219\n",
      "Epoch: 0021 train_loss= 0.75284 train_acc= 0.78489 val_loss= 0.77152 val_acc= 0.76277 time= 5.94075\n",
      "Epoch: 0022 train_loss= 0.71754 train_acc= 0.78388 val_loss= 0.74152 val_acc= 0.76642 time= 4.61970\n",
      "Epoch: 0023 train_loss= 0.69245 train_acc= 0.78570 val_loss= 0.71510 val_acc= 0.77190 time= 6.05992\n",
      "Epoch: 0024 train_loss= 0.66401 train_acc= 0.78570 val_loss= 0.69116 val_acc= 0.78285 time= 4.78898\n",
      "Epoch: 0025 train_loss= 0.65030 train_acc= 0.79583 val_loss= 0.66864 val_acc= 0.79562 time= 5.16301\n",
      "Epoch: 0026 train_loss= 0.62418 train_acc= 0.80859 val_loss= 0.64682 val_acc= 0.80474 time= 5.24033\n",
      "Epoch: 0027 train_loss= 0.59474 train_acc= 0.82479 val_loss= 0.62536 val_acc= 0.81934 time= 4.83762\n",
      "Epoch: 0028 train_loss= 0.58077 train_acc= 0.83958 val_loss= 0.60403 val_acc= 0.83029 time= 4.77326\n",
      "Epoch: 0029 train_loss= 0.57096 train_acc= 0.84120 val_loss= 0.58285 val_acc= 0.83577 time= 4.43415\n",
      "Epoch: 0030 train_loss= 0.54373 train_acc= 0.84221 val_loss= 0.56207 val_acc= 0.84672 time= 4.30790\n",
      "Epoch: 0031 train_loss= 0.53092 train_acc= 0.85538 val_loss= 0.54177 val_acc= 0.85401 time= 5.31055\n",
      "Epoch: 0032 train_loss= 0.50960 train_acc= 0.86125 val_loss= 0.52223 val_acc= 0.85949 time= 4.85716\n",
      "Epoch: 0033 train_loss= 0.48989 train_acc= 0.86510 val_loss= 0.50359 val_acc= 0.86131 time= 4.79939\n",
      "Epoch: 0034 train_loss= 0.47190 train_acc= 0.86753 val_loss= 0.48586 val_acc= 0.86314 time= 5.39821\n",
      "Epoch: 0035 train_loss= 0.45571 train_acc= 0.87503 val_loss= 0.46898 val_acc= 0.86861 time= 4.39832\n",
      "Epoch: 0036 train_loss= 0.44145 train_acc= 0.87827 val_loss= 0.45280 val_acc= 0.87591 time= 4.89425\n",
      "Epoch: 0037 train_loss= 0.42438 train_acc= 0.88313 val_loss= 0.43723 val_acc= 0.88139 time= 5.11414\n",
      "Epoch: 0038 train_loss= 0.40942 train_acc= 0.88900 val_loss= 0.42208 val_acc= 0.88321 time= 4.36575\n",
      "Epoch: 0039 train_loss= 0.38844 train_acc= 0.89427 val_loss= 0.40715 val_acc= 0.89051 time= 4.49438\n",
      "Epoch: 0040 train_loss= 0.37703 train_acc= 0.89994 val_loss= 0.39240 val_acc= 0.89416 time= 4.97655\n",
      "Epoch: 0041 train_loss= 0.36826 train_acc= 0.90541 val_loss= 0.37807 val_acc= 0.90146 time= 4.89950\n",
      "Epoch: 0042 train_loss= 0.35619 train_acc= 0.90743 val_loss= 0.36414 val_acc= 0.90876 time= 4.94609\n",
      "Epoch: 0043 train_loss= 0.34506 train_acc= 0.90905 val_loss= 0.35071 val_acc= 0.91423 time= 5.58640\n",
      "Epoch: 0044 train_loss= 0.31888 train_acc= 0.92161 val_loss= 0.33771 val_acc= 0.91971 time= 6.46499\n",
      "Epoch: 0045 train_loss= 0.31639 train_acc= 0.92404 val_loss= 0.32517 val_acc= 0.92336 time= 6.41050\n",
      "Epoch: 0046 train_loss= 0.30408 train_acc= 0.92222 val_loss= 0.31309 val_acc= 0.92518 time= 5.60181\n",
      "Epoch: 0047 train_loss= 0.29512 train_acc= 0.92890 val_loss= 0.30173 val_acc= 0.93796 time= 6.61010\n",
      "Epoch: 0048 train_loss= 0.28435 train_acc= 0.93397 val_loss= 0.29048 val_acc= 0.93978 time= 4.92978\n",
      "Epoch: 0049 train_loss= 0.26973 train_acc= 0.93822 val_loss= 0.27959 val_acc= 0.94526 time= 4.21750\n",
      "Epoch: 0050 train_loss= 0.26653 train_acc= 0.93863 val_loss= 0.26900 val_acc= 0.94526 time= 4.22261\n",
      "Epoch: 0051 train_loss= 0.25088 train_acc= 0.94166 val_loss= 0.25882 val_acc= 0.94890 time= 4.34254\n",
      "Epoch: 0052 train_loss= 0.24096 train_acc= 0.94632 val_loss= 0.24921 val_acc= 0.95073 time= 4.50104\n",
      "Epoch: 0053 train_loss= 0.23100 train_acc= 0.95179 val_loss= 0.23990 val_acc= 0.95438 time= 4.20454\n",
      "Epoch: 0054 train_loss= 0.22213 train_acc= 0.95017 val_loss= 0.23101 val_acc= 0.95438 time= 4.01170\n",
      "Epoch: 0055 train_loss= 0.21131 train_acc= 0.94754 val_loss= 0.22255 val_acc= 0.95620 time= 5.49482\n",
      "Epoch: 0056 train_loss= 0.20519 train_acc= 0.94977 val_loss= 0.21453 val_acc= 0.95803 time= 4.70616\n",
      "Epoch: 0057 train_loss= 0.19314 train_acc= 0.95362 val_loss= 0.20692 val_acc= 0.95985 time= 3.87065\n",
      "Epoch: 0058 train_loss= 0.19077 train_acc= 0.95422 val_loss= 0.19933 val_acc= 0.95620 time= 4.31395\n",
      "Epoch: 0059 train_loss= 0.18663 train_acc= 0.95665 val_loss= 0.19209 val_acc= 0.95620 time= 4.16638\n",
      "Epoch: 0060 train_loss= 0.17925 train_acc= 0.95625 val_loss= 0.18530 val_acc= 0.95985 time= 6.65492\n",
      "Epoch: 0061 train_loss= 0.17084 train_acc= 0.95929 val_loss= 0.17887 val_acc= 0.96168 time= 6.38923\n",
      "Epoch: 0062 train_loss= 0.16538 train_acc= 0.96151 val_loss= 0.17280 val_acc= 0.96168 time= 4.78771\n",
      "Epoch: 0063 train_loss= 0.15488 train_acc= 0.96314 val_loss= 0.16717 val_acc= 0.96350 time= 4.55093\n",
      "Epoch: 0064 train_loss= 0.15153 train_acc= 0.96334 val_loss= 0.16174 val_acc= 0.96533 time= 4.57330\n",
      "Epoch: 0065 train_loss= 0.14809 train_acc= 0.96617 val_loss= 0.15662 val_acc= 0.96715 time= 4.07619\n",
      "Epoch: 0066 train_loss= 0.13621 train_acc= 0.96962 val_loss= 0.15169 val_acc= 0.96898 time= 4.31237\n",
      "Epoch: 0067 train_loss= 0.13698 train_acc= 0.96820 val_loss= 0.14692 val_acc= 0.97080 time= 4.59981\n",
      "Epoch: 0068 train_loss= 0.13419 train_acc= 0.96638 val_loss= 0.14222 val_acc= 0.96898 time= 3.98491\n",
      "Epoch: 0069 train_loss= 0.12958 train_acc= 0.96739 val_loss= 0.13792 val_acc= 0.96898 time= 4.30721\n",
      "Epoch: 0070 train_loss= 0.12283 train_acc= 0.96921 val_loss= 0.13420 val_acc= 0.96715 time= 4.55858\n",
      "Epoch: 0071 train_loss= 0.11988 train_acc= 0.96962 val_loss= 0.13085 val_acc= 0.96715 time= 4.13736\n",
      "Epoch: 0072 train_loss= 0.11856 train_acc= 0.96941 val_loss= 0.12782 val_acc= 0.97080 time= 5.05947\n",
      "Epoch: 0073 train_loss= 0.11043 train_acc= 0.97164 val_loss= 0.12466 val_acc= 0.96898 time= 5.35168\n",
      "Epoch: 0074 train_loss= 0.10867 train_acc= 0.97164 val_loss= 0.12172 val_acc= 0.96898 time= 4.49153\n",
      "Epoch: 0075 train_loss= 0.10427 train_acc= 0.97326 val_loss= 0.11908 val_acc= 0.97080 time= 4.44220\n",
      "Epoch: 0076 train_loss= 0.10301 train_acc= 0.97488 val_loss= 0.11644 val_acc= 0.97263 time= 4.63578\n",
      "Epoch: 0077 train_loss= 0.09894 train_acc= 0.97509 val_loss= 0.11431 val_acc= 0.97810 time= 3.81148\n",
      "Epoch: 0078 train_loss= 0.09986 train_acc= 0.97488 val_loss= 0.11243 val_acc= 0.97628 time= 5.10694\n",
      "Epoch: 0079 train_loss= 0.09648 train_acc= 0.97630 val_loss= 0.11063 val_acc= 0.97628 time= 5.08421\n",
      "Epoch: 0080 train_loss= 0.09431 train_acc= 0.97630 val_loss= 0.10810 val_acc= 0.97628 time= 4.82907\n",
      "Epoch: 0081 train_loss= 0.09206 train_acc= 0.97833 val_loss= 0.10576 val_acc= 0.97263 time= 4.98158\n",
      "Epoch: 0082 train_loss= 0.08967 train_acc= 0.97934 val_loss= 0.10357 val_acc= 0.97628 time= 5.55326\n",
      "Epoch: 0083 train_loss= 0.08327 train_acc= 0.98055 val_loss= 0.10171 val_acc= 0.97810 time= 4.91779\n",
      "Epoch: 0084 train_loss= 0.08746 train_acc= 0.97812 val_loss= 0.10009 val_acc= 0.97628 time= 4.28067\n",
      "Epoch: 0085 train_loss= 0.08368 train_acc= 0.97974 val_loss= 0.09876 val_acc= 0.97628 time= 4.54183\n",
      "Epoch: 0086 train_loss= 0.08160 train_acc= 0.98116 val_loss= 0.09771 val_acc= 0.97628 time= 4.07812\n",
      "Epoch: 0087 train_loss= 0.08024 train_acc= 0.98076 val_loss= 0.09643 val_acc= 0.97445 time= 4.18858\n",
      "Epoch: 0088 train_loss= 0.07921 train_acc= 0.98055 val_loss= 0.09486 val_acc= 0.97445 time= 3.88005\n",
      "Epoch: 0089 train_loss= 0.07582 train_acc= 0.98298 val_loss= 0.09340 val_acc= 0.97628 time= 4.12355\n",
      "Epoch: 0090 train_loss= 0.07497 train_acc= 0.98562 val_loss= 0.09228 val_acc= 0.97628 time= 4.18535\n",
      "Epoch: 0091 train_loss= 0.07243 train_acc= 0.98339 val_loss= 0.09120 val_acc= 0.97445 time= 3.85890\n",
      "Epoch: 0092 train_loss= 0.06747 train_acc= 0.98420 val_loss= 0.09027 val_acc= 0.97445 time= 3.96943\n",
      "Epoch: 0093 train_loss= 0.07006 train_acc= 0.98157 val_loss= 0.08876 val_acc= 0.97810 time= 5.36446\n",
      "Epoch: 0094 train_loss= 0.06744 train_acc= 0.98319 val_loss= 0.08734 val_acc= 0.97993 time= 6.36825\n",
      "Epoch: 0095 train_loss= 0.06785 train_acc= 0.98521 val_loss= 0.08649 val_acc= 0.98175 time= 4.97228\n",
      "Epoch: 0096 train_loss= 0.06953 train_acc= 0.98359 val_loss= 0.08577 val_acc= 0.97628 time= 5.01218\n",
      "Epoch: 0097 train_loss= 0.06679 train_acc= 0.98420 val_loss= 0.08541 val_acc= 0.97993 time= 5.11142\n",
      "Epoch: 0098 train_loss= 0.06177 train_acc= 0.98440 val_loss= 0.08520 val_acc= 0.97993 time= 5.14196\n",
      "Epoch: 0099 train_loss= 0.06277 train_acc= 0.98400 val_loss= 0.08480 val_acc= 0.97810 time= 4.39170\n",
      "Epoch: 0100 train_loss= 0.06545 train_acc= 0.98440 val_loss= 0.08351 val_acc= 0.97993 time= 5.13257\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "cfg = CONFIG()\n",
    "\n",
    "# set random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "dataset = \"R8\"\n",
    "cfg.dataset = dataset\n",
    "\n",
    "# Load data\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size = load_corpus(cfg.dataset)\n",
    "\n",
    "features = sp.identity(features.shape[0])  # featureless\n",
    "features = preprocess_features(features)\n",
    "\n",
    "\n",
    "support = [preprocess_adj(adj)]\n",
    "\n",
    "\n",
    "t_features = tf.SparseTensor(*features)\n",
    "t_y_train = tf.convert_to_tensor(y_train)\n",
    "t_y_val = tf.convert_to_tensor(y_val)\n",
    "t_y_test = tf.convert_to_tensor(y_test)\n",
    "tm_train_mask = tf.convert_to_tensor(train_mask)\n",
    "\n",
    "tm_val_mask = tf.convert_to_tensor(val_mask)\n",
    "tm_test_mask = tf.convert_to_tensor(test_mask)\n",
    "\n",
    "t_support = []\n",
    "for i in range(len(support)):\n",
    "    t_support.append(tf.cast(tf.SparseTensor(*support[i]), dtype=tf.float64))\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = GCN(input_dim=features[2][1], output_dim=y_train.shape[1], num_features_nonzero=features[1].shape)\n",
    "\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "optimizer = optimizers.Adam(lr=cfg.learning_rate)\n",
    "\n",
    "cost_val = []\n",
    "\n",
    "for epoch in range(cfg.epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    with tf.GradientTape() as tape:\n",
    "        _, loss, acc = model((t_features, t_y_train, tm_train_mask, t_support))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    _, val_loss, val_acc = model((t_features, t_y_val, tm_val_mask, t_support), training=False)\n",
    "    cost_val.append(val_loss)\n",
    "\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(loss),\n",
    "          \"train_acc=\", \"{:.5f}\".format(acc), \"val_loss=\", \"{:.5f}\".format(val_loss),\n",
    "          \"val_acc=\", \"{:.5f}\".format(val_acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch > cfg.early_stopping and cost_val[-1] > np.mean(cost_val[-(cfg.early_stopping+1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Eval"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: cost= 0.11133 accuracy= 0.97305 time= 1.46225\n",
      "Average Test Precision, Recall and F1-Score...\n",
      "(0.9730470534490635, 0.9730470534490635, 0.9730470534490635, None)\n"
     ]
    }
   ],
   "source": [
    "def evaluate(features, y, mask, support):\n",
    "    t = time.time()\n",
    "    pred, test_loss, test_acc = model((features, y, mask, support), training=False)\n",
    "    return test_loss, test_acc, pred, np.argmax(y, axis=1), time.time() - t\n",
    "\n",
    "test_cost, test_acc, pred, labels, test_duration = evaluate(t_features, t_y_test, tm_test_mask, t_support)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost), \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))\n",
    "\n",
    "test_pred = []\n",
    "test_labels = []\n",
    "\n",
    "for i in range(len(test_mask)):\n",
    "    if test_mask[i]:\n",
    "        test_pred.append(pred[i])\n",
    "        test_labels.append(labels[i])\n",
    "\n",
    "print(\"Average Test Precision, Recall and F1-Score...\")\n",
    "print(metrics.precision_recall_fscore_support(test_labels, test_pred, average='micro'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storing layer 1 embeddings...\n",
      "finish...\n",
      "storing layer 2 embeddings...\n",
      "finish...\n"
     ]
    }
   ],
   "source": [
    "print('storing layer 1 embeddings...')\n",
    "\n",
    "embeddings_1 = model.layers_[0].embedding\n",
    "\n",
    "word_embeddings = embeddings_1[train_size: adj.shape[0] - test_size]\n",
    "train_doc_embeddings = embeddings_1[:train_size]  # include val docs\n",
    "test_doc_embeddings = embeddings_1[adj.shape[0] - test_size:]\n",
    "\n",
    "\n",
    "doc_vectors = []\n",
    "doc_id = 0\n",
    "for i in range(train_size):\n",
    "    doc_vector = train_doc_embeddings[i]\n",
    "    doc_vector_str = ' '.join([str(tf.keras.backend.get_value(x)) for x in doc_vector])\n",
    "    doc_vectors.append('doc_' + str(doc_id) + ' ' + doc_vector_str)\n",
    "    doc_id += 1\n",
    "\n",
    "for i in range(test_size):\n",
    "    doc_vector = test_doc_embeddings[i]\n",
    "    doc_vector_str = ' '.join([str(tf.keras.backend.get_value(x)) for x in doc_vector])\n",
    "    doc_vectors.append('doc_' + str(doc_id) + ' ' + doc_vector_str)\n",
    "    doc_id += 1\n",
    "\n",
    "doc_embeddings_str = '\\n'.join(doc_vectors)\n",
    "f = open('./cleaned_data/' + cfg.dataset + '/' + cfg.dataset +  '_1_vectors.txt', 'w')\n",
    "f.write(doc_embeddings_str)\n",
    "f.close()\n",
    "\n",
    "print(\"finish...\")\n",
    "\n",
    "\n",
    "\n",
    "print('storing layer 2 embeddings...')\n",
    "embeddings_2 = model.layers_[1].embedding\n",
    "\n",
    "word_embeddings = embeddings_2[train_size: adj.shape[0] - test_size]\n",
    "train_doc_embeddings = embeddings_2[:train_size]  # include val docs\n",
    "test_doc_embeddings = embeddings_2[adj.shape[0] - test_size:]\n",
    "\n",
    "doc_vectors = []\n",
    "doc_id = 0\n",
    "for i in range(train_size):\n",
    "    doc_vector = train_doc_embeddings[i]\n",
    "    doc_vector_str = ' '.join([str(tf.keras.backend.get_value(x)) for x in doc_vector])\n",
    "    doc_vectors.append('doc_' + str(doc_id) + ' ' + doc_vector_str)\n",
    "    doc_id += 1\n",
    "\n",
    "for i in range(test_size):\n",
    "    doc_vector = test_doc_embeddings[i]\n",
    "    doc_vector_str = ' '.join([str(tf.keras.backend.get_value(x)) for x in doc_vector])\n",
    "    doc_vectors.append('doc_' + str(doc_id) + ' ' + doc_vector_str)\n",
    "    doc_id += 1\n",
    "\n",
    "doc_embeddings_str = '\\n'.join(doc_vectors)\n",
    "f = open('./cleaned_data/' + cfg.dataset + '/' + cfg.dataset +  '_2_vectors.txt', 'w')\n",
    "f.write(doc_embeddings_str)\n",
    "f.close()\n",
    "\n",
    "print(\"finish...\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\firqa\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\manifold\\_t_sne.py:795: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\firqa\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\manifold\\_t_sne.py:805: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "dataset = \"R8\"\n",
    "length = \"2\"\n",
    "\n",
    "f = open('./cleaned_data/' + dataset + '/graph/' + dataset + '.train.index', 'r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "train_size = len(lines)\n",
    "\n",
    "\n",
    "f = open('./cleaned_data/' + dataset + '/' + dataset + '_shuffle.txt', 'r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "target_names = set()\n",
    "labels = []\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    temp = line.split('\\t')\n",
    "    labels.append(temp[2])\n",
    "    target_names.add(temp[2])\n",
    "\n",
    "target_names = list(target_names)\n",
    "\n",
    "f = open('./cleaned_data/' + dataset + '/' + dataset + '_' + length + '_vectors.txt', 'r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "docs = []\n",
    "for line in lines:\n",
    "    temp = line.strip().split()\n",
    "    values_str_list = temp[1:]\n",
    "    values = [float(x) for x in values_str_list]\n",
    "    docs.append(values)\n",
    "\n",
    "fea = docs[train_size:]  # int(train_size * 0.9)\n",
    "label = labels[train_size:]  # int(train_size * 0.9)\n",
    "label = np.array(label)\n",
    "\n",
    "fea = TSNE(n_components=2).fit_transform(fea)\n",
    "cls = np.unique(label)\n",
    "\n",
    "\n",
    "fea_num = [fea[label == i] for i in cls]\n",
    "for i, f in enumerate(fea_num):\n",
    "    if cls[i] in range(10):\n",
    "        plt.scatter(f[:, 0], f[:, 1], label=cls[i], marker='+')\n",
    "    else:\n",
    "        plt.scatter(f[:, 0], f[:, 1], label=cls[i])\n",
    "\n",
    "\n",
    "plt.legend(ncol=5, loc='upper center', bbox_to_anchor=(0.48, -0.08), fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.savefig(dataset+'_'+length+'_vis.png', format='png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pygeo",
   "language": "python",
   "display_name": "pygeo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}