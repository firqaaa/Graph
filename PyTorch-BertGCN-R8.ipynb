{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Utils"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import eigsh\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def load_data(dataset_str):\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(\"data/ind.{}.test.index\".format(dataset_str))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "    print_log(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
    "\n",
    "    if dataset_str == \"citeseer\":\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(\n",
    "            min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "    idx_test = test_idx_range.tolist()\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y)+500)\n",
    "\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "def load_corpus(dataset_str):\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'adj']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"./data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, adj = tuple(objects)\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    labels = np.vstack((ally, ty))\n",
    "\n",
    "    train_idx_orig = parse_index_file(\n",
    "        \"./data/{}.train.index\".format(dataset_str)\n",
    "    )\n",
    "    train_size = len(train_idx_orig)\n",
    "\n",
    "    val_size = train_size - x.shape[0]\n",
    "    test_size = tx.shape[0]\n",
    "\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y) + val_size)\n",
    "    idx_test = range(allx.shape[0], allx.shape[0] + test_size)\n",
    "\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "    return sparse_mx\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalized feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return features.A\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symetrically normalize adjacency matrix\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    return adj_normalized.A\n",
    "\n",
    "def construct_feed_dict(features, support, labels, labels_mask, placeholders):\n",
    "    \"\"\"Construct feed dictionary\"\"\"\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['labels']: labels})\n",
    "    feed_dict.update({placeholders['labels_mask']: labels_mask})\n",
    "    feed_dict.update({placeholders['features']: features})\n",
    "    feed_dict.update({placeholders['support'][i]: support[i] for i in range(len(len(support)))})\n",
    "    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n",
    "    return feed_dict\n",
    "\n",
    "def chebyshev_polynomials(adj, k):\n",
    "    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrix (tuple representation)\"\"\"\n",
    "    print_log(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\n",
    "\n",
    "    adj_normalized = normalize_adj(adj)\n",
    "    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n",
    "    largest_eigval, _ = eigsh(laplacian, 1, which='LM')\n",
    "    scaled_laplacian = (2. / largest_eigval[0]) * laplacian - sp.eye(adj.shape[0])\n",
    "\n",
    "    t_k = list()\n",
    "    t_k.append(sp.eye(adj.shape[0]).A)\n",
    "    t_k.append(scaled_laplacian.A)\n",
    "\n",
    "    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n",
    "        s_lap = sp.csr_matrix(scaled_lap, copy=True)\n",
    "        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n",
    "\n",
    "    for i in range(2, k+1):\n",
    "        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n",
    "\n",
    "    return t_k\n",
    "\n",
    "def loadWord2Vec(filename):\n",
    "    \"\"\"Read Word Vectors\"\"\"\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    word_vector_map = {}\n",
    "    file = open(filename, 'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        if (len(row) > 2):\n",
    "            vocab.append(row[0])\n",
    "            vector = row[1:]\n",
    "            length = len(vector)\n",
    "            for i in range(length):\n",
    "                vector[i] = float(vector[i])\n",
    "            embd.append(vector)\n",
    "            word_vector_map[row[0]] = vector\n",
    "    print_log('Loaded Word Vectors!')\n",
    "    file.close()\n",
    "    return vocab, embd, word_vector_map\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def print_log(msg='', end='\\n'):\n",
    "    now = datetime.datetime.now()\n",
    "    t = str(now.year) + '/' + str(now.month) + '/' + str(now.day) + ' '\\\n",
    "        + str(now.hour).zfill(2) + ':' + str(now.minute).zfill(2) + ':' + str(now.second).zfill(2)\n",
    "\n",
    "    if isinstance(msg, str):\n",
    "        lines = msg.split('\\n')\n",
    "    else:\n",
    "        lines = [msg]\n",
    "\n",
    "    for line in lines:\n",
    "        if line == lines[-1]:\n",
    "            print('[' + t + '] ' + str(line), end=end)\n",
    "        else:\n",
    "            print('[' + t + '] ' + str(line))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build Graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)\n"
     ]
    }
   ],
   "source": [
    "# build graph\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from math import log\n",
    "from sklearn import svm\n",
    "import scipy.sparse as sp\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "datasets = ['20ng', 'R8', 'R52', 'ohsumed', 'mr']\n",
    "dataset = \"R8\"\n",
    "\n",
    "if dataset not in datasets:\n",
    "    sys.exit(\"wrong dataset name\")\n",
    "\n",
    "word_embedding_dim = 300\n",
    "word_vector_map = {}\n",
    "\n",
    "# Shuffling\n",
    "doc_name_list = []\n",
    "doc_train_list = []\n",
    "doc_test_list = []\n",
    "\n",
    "with open(\"./data/\" + dataset + '.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        doc_name_list.append(line.strip())\n",
    "        temp = line.split(\"\\t\")\n",
    "        if temp[1].find(\"test\") != -1:\n",
    "            doc_test_list.append(line.strip())\n",
    "        elif temp[1].find('train') != -1:\n",
    "            doc_train_list.append(line.strip())\n",
    "\n",
    "doc_content_list = []\n",
    "with open(\"./data/corpus/\" + dataset + '.clean.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        doc_content_list.append(line.strip())\n",
    "\n",
    "train_ids = []\n",
    "for train_name in doc_train_list:\n",
    "    train_id = doc_name_list.index(train_name)\n",
    "    train_ids.append(train_id)\n",
    "# print(train_ids)\n",
    "random.shuffle(train_ids)\n",
    "\n",
    "train_ids_str = \"\\n\".join(str(index) for index in train_ids)\n",
    "with open(\"./data/\" + dataset + '.train.index', 'w') as f:\n",
    "    f.write(train_ids_str)\n",
    "\n",
    "test_ids = []\n",
    "for test_name in doc_test_list:\n",
    "    test_id = doc_name_list.index(test_name)\n",
    "    test_ids.append(test_id)\n",
    "# print(test_ids)\n",
    "random.shuffle(test_ids)\n",
    "\n",
    "test_ids_str = \"\\n\".join(str(index) for index in test_ids)\n",
    "with open(\"./data/\" + dataset + \".test.index\", \"w\") as f:\n",
    "    f.write(test_ids_str)\n",
    "\n",
    "ids = train_ids + test_ids\n",
    "# print(ids)\n",
    "# print(len(ids))\n",
    "\n",
    "shuffle_doc_name_list = []\n",
    "shuffle_doc_words_list = []\n",
    "for id in ids:\n",
    "    shuffle_doc_name_list.append(doc_name_list[int(id)])\n",
    "    shuffle_doc_words_list.append(doc_content_list[int(id)])\n",
    "shuffle_doc_name_str = \"\\n\".join(shuffle_doc_name_list)\n",
    "shuffle_doc_words_str = \"\\n\".join(shuffle_doc_words_list)\n",
    "\n",
    "with open(\"./data/\" + dataset + \"_shuffle.txt\", \"w\") as f:\n",
    "    f.write(shuffle_doc_name_str)\n",
    "\n",
    "with open(\"./data/corpus/\" + dataset + \"_shuffle.txt\", \"w\") as f:\n",
    "    f.write(shuffle_doc_words_str)\n",
    "\n",
    "# build vocab\n",
    "word_freq = {}\n",
    "word_set = set()\n",
    "for doc_words in shuffle_doc_words_list:\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_set.add(word)\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "\n",
    "vocab = list(word_set)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_doc_list = {}\n",
    "\n",
    "for i in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    appeared = set()\n",
    "    for word in words:\n",
    "        if word in appeared:\n",
    "            continue\n",
    "        if word in word_doc_list:\n",
    "            doc_list = word_doc_list[word]\n",
    "            doc_list.append(i)\n",
    "            word_doc_list[word] = doc_list\n",
    "        else:\n",
    "            word_doc_list[word] = [i]\n",
    "        appeared.add(word)\n",
    "\n",
    "word_doc_freq = {}\n",
    "for word, doc_list in word_doc_list.items():\n",
    "    word_doc_freq[word] = len(doc_list)\n",
    "\n",
    "word_id_map = {}\n",
    "for i in range(vocab_size):\n",
    "    word_id_map[vocab[i]] = i\n",
    "\n",
    "vocab_str = \"\\n\".join(vocab)\n",
    "\n",
    "with open(\"./data/corpus/\" + dataset + \"_vocab.txt\", \"w\") as f:\n",
    "    f.write(vocab_str)\n",
    "\n",
    "# Label List\n",
    "label_set = set()\n",
    "for doc_meta in shuffle_doc_name_list:\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label_set.add(temp[2])\n",
    "label_list = list(label_set)\n",
    "\n",
    "label_list_str = '\\n'.join(label_list)\n",
    "with open('./data/corpus/' + dataset + '_labels.txt', 'w') as f:\n",
    "    f.write(label_list_str)\n",
    "\n",
    "train_size = len(train_ids)\n",
    "val_size = int(0.1 * train_size)\n",
    "real_train_size = train_size - val_size\n",
    "\n",
    "real_train_doc_names = shuffle_doc_name_list[:real_train_size]\n",
    "real_train_doc_names_str = '\\n'.join(real_train_doc_names)\n",
    "\n",
    "with open('./data/' + dataset + '.real_train.name', 'w') as f:\n",
    "    f.write(real_train_doc_names_str)\n",
    "\n",
    "row_x = []\n",
    "col_x = []\n",
    "data_x = []\n",
    "for i in range(real_train_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embedding_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embedding_dim):\n",
    "        row_x.append(i)\n",
    "        col_x.append(j)\n",
    "        data_x.append(doc_vec[j] / doc_len)\n",
    "\n",
    "x = sp.csr_matrix((data_x, (row_x, col_x)), shape=(real_train_size, word_embedding_dim))\n",
    "\n",
    "y = []\n",
    "for i in range(real_train_size):\n",
    "    doc_meta = shuffle_doc_name_list[i]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    y.append(one_hot)\n",
    "y = np.array(y)\n",
    "# print(y)\n",
    "\n",
    "# tx: feature vectors of test docs, no initial features\n",
    "test_size = len(test_ids)\n",
    "\n",
    "row_tx = []\n",
    "col_tx = []\n",
    "data_tx = []\n",
    "for i in range(test_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embedding_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i + train_size]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embedding_dim):\n",
    "        row_tx.append(i)\n",
    "        col_tx.append(j)\n",
    "        data_tx.append(doc_vec[j] / doc_len)\n",
    "\n",
    "tx = sp.csr_matrix((data_tx, (row_tx, col_tx)),\n",
    "                   shape=(test_size, word_embedding_dim))\n",
    "\n",
    "ty = []\n",
    "for i in range(test_size):\n",
    "    doc_meta = shuffle_doc_name_list[i + train_size]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    ty.append(one_hot)\n",
    "ty = np.array(ty)\n",
    "# print(ty)\n",
    "\n",
    "# allx: the the feature vectors of both labeled and unlabeled training instances\n",
    "# (a superset of x)\n",
    "# unlabeled training instances -> words\n",
    "\n",
    "word_vectors = np.random.uniform(-0.01, 0.01, (vocab_size, word_embedding_dim))\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "    word = vocab[i]\n",
    "    if word in word_vector_map:\n",
    "        vector = word_vector_map[word]\n",
    "        word_vectors[i] = vector\n",
    "\n",
    "row_allx = []\n",
    "col_allx = []\n",
    "data_allx = []\n",
    "\n",
    "for i in range(train_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embedding_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "    for j in range(word_embedding_dim):\n",
    "        row_allx.append(int(i))\n",
    "        col_allx.append(j)\n",
    "        data_allx.append(doc_vec[j] / doc_len)\n",
    "for i in range(vocab_size):\n",
    "    for j in range(word_embedding_dim):\n",
    "        row_allx.append(int(i + train_size))\n",
    "        col_allx.append(j)\n",
    "        data_allx.append(word_vectors.item((i, j)))\n",
    "\n",
    "row_allx = np.array(row_allx)\n",
    "col_allx = np.array(col_allx)\n",
    "data_allx = np.array(data_allx)\n",
    "\n",
    "allx = sp.csr_matrix((data_allx, (row_allx, col_allx)), shape=(train_size + vocab_size, word_embedding_dim))\n",
    "\n",
    "ally = []\n",
    "for i in range(train_size):\n",
    "    doc_meta = shuffle_doc_name_list[i]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    ally.append(one_hot)\n",
    "\n",
    "for i in range(vocab_size):\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    ally.append(one_hot)\n",
    "\n",
    "ally = np.array(ally)\n",
    "print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
    "\n",
    "'''\n",
    "Doc word heterogeneous graph\n",
    "'''\n",
    "\n",
    "# word co-occurence with context windows\n",
    "window_size = 20\n",
    "windows = []\n",
    "\n",
    "for doc_words in shuffle_doc_words_list:\n",
    "    words = doc_words.split()\n",
    "    length = len(words)\n",
    "    if length <= window_size:\n",
    "        windows.append(words)\n",
    "    else:\n",
    "        for j in range(length - window_size + 1):\n",
    "            window = words[j: j + window_size]\n",
    "            windows.append(window)\n",
    "\n",
    "word_window_freq = {}\n",
    "for window in windows:\n",
    "    appeared = set()\n",
    "    for i in range(len(window)):\n",
    "        if window[i] in appeared:\n",
    "            continue\n",
    "        if window[i] in word_window_freq:\n",
    "            word_window_freq[window[i]] += 1\n",
    "        else:\n",
    "            word_window_freq[window[i]] = 1\n",
    "        appeared.add(window[i])\n",
    "\n",
    "word_pair_count = {}\n",
    "for window in windows:\n",
    "    for i in range(1, len(window)):\n",
    "        for j in range(0, i):\n",
    "            word_i = window[i]\n",
    "            word_i_id = word_id_map[word_i]\n",
    "            word_j = window[j]\n",
    "            word_j_id = word_id_map[word_j]\n",
    "            if word_i_id == word_j_id:\n",
    "                continue\n",
    "            word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n",
    "            # two orders\n",
    "            word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n",
    "\n",
    "row = []\n",
    "col = []\n",
    "weight = []\n",
    "\n",
    "# pmi as weight\n",
    "num_window = len(windows)\n",
    "\n",
    "for key in word_pair_count:\n",
    "    temp = key.split(',')\n",
    "    i = int(temp[0])\n",
    "    j = int(temp[1])\n",
    "    count = word_pair_count[key]\n",
    "    word_freq_i = word_window_freq[vocab[i]]\n",
    "    word_freq_j = word_window_freq[vocab[j]]\n",
    "    pmi = log((1.0 * count / num_window) /\n",
    "              (1.0 * word_freq_i * word_freq_j / (num_window * num_window)))\n",
    "    if pmi <= 0:\n",
    "        continue\n",
    "    row.append(train_size + i)\n",
    "    col.append(train_size + j)\n",
    "    weight.append(pmi)\n",
    "\n",
    "# word vector cosine similarity as weight\n",
    "\n",
    "# doc word frequency\n",
    "doc_word_freq = {}\n",
    "\n",
    "for doc_id in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[doc_id]\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_id = word_id_map[word]\n",
    "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
    "        if doc_word_str in doc_word_freq:\n",
    "            doc_word_freq[doc_word_str] += 1\n",
    "        else:\n",
    "            doc_word_freq[doc_word_str] = 1\n",
    "\n",
    "for i in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_word_set = set()\n",
    "    for word in words:\n",
    "        if word in doc_word_set:\n",
    "            continue\n",
    "        j = word_id_map[word]\n",
    "        key = str(i) + ',' + str(j)\n",
    "        freq = doc_word_freq[key]\n",
    "        if i < train_size:\n",
    "            row.append(i)\n",
    "        else:\n",
    "            row.append(i + vocab_size)\n",
    "        col.append(train_size + j)\n",
    "        idf = log(1.0 * len(shuffle_doc_words_list) /\n",
    "                  word_doc_freq[vocab[j]])\n",
    "        weight.append(freq * idf)\n",
    "        doc_word_set.add(word)\n",
    "\n",
    "node_size = train_size + vocab_size + test_size\n",
    "adj = sp.csr_matrix(\n",
    "    (weight, (row, col)), shape=(node_size, node_size))\n",
    "\n",
    "# dump objects\n",
    "with open(\"./data/ind.{}.x\".format(dataset), 'wb') as f:\n",
    "    pkl.dump(x, f)\n",
    "\n",
    "with open(\"./data/ind.{}.y\".format(dataset), 'wb') as f:\n",
    "    pkl.dump(y, f)\n",
    "\n",
    "with open(\"./data/ind.{}.tx\".format(dataset), 'wb') as f:\n",
    "    pkl.dump(tx, f)\n",
    "\n",
    "with open(\"./data/ind.{}.ty\".format(dataset), 'wb') as f:\n",
    "    pkl.dump(ty, f)\n",
    "\n",
    "with open(\"./data/ind.{}.allx\".format(dataset), 'wb') as f:\n",
    "    pkl.dump(allx, f)\n",
    "\n",
    "with open(\"./data/ind.{}.ally\".format(dataset), 'wb') as f:\n",
    "    pkl.dump(ally, f)\n",
    "\n",
    "with open(\"./data/ind.{}.adj\".format(dataset), 'wb') as f:\n",
    "    pkl.dump(adj, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "from dgl.base import DGLError\n",
    "from dgl import function as fn\n",
    "from dgl.utils import expand_as_pair\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "\n",
    "class GraphConvEdgeWeight(GraphConv):\n",
    "\n",
    "    def forward(self, graph, feat, weight=None, edge_weights=None):\n",
    "        with graph.local_scope():\n",
    "            if not self._allow_zero_in_degree:\n",
    "                if (graph.in_degree() == 0).any():\n",
    "                    raise DGLError('There are 0-in-degree nodes in the graph, '\n",
    "                                   'output for those nodes will be invalid. '\n",
    "                                   'This is harmful for some applications, '\n",
    "                                   'causing silent performance regression. '\n",
    "                                   'Adding self-loop on the input graph by '\n",
    "                                   'calling `g = dgl.add_self_loop(g)` will resolve '\n",
    "                                   'the issue. Setting ``allow_zero_in_degree`` '\n",
    "                                   'to be `True` when constructing this module will '\n",
    "                                   'suppress the check and let the code run.')\n",
    "\n",
    "            # (BarclayII) For RGCN on heterogeneous graphs we need to support GCN on bipartite.\n",
    "            feat_src, feat_dst = expand_as_pair(feat, graph)\n",
    "            if self._norm == 'both':\n",
    "                degs = graph.out_degrees().float().clamp(min=1)\n",
    "                norm = th.pow(degs, -0.5)\n",
    "                shp = norm.shape + (1, ) * (feat_src.dim() - 1)\n",
    "                norm = th.reshape(norm, shp)\n",
    "                feat_src = feat_src * norm\n",
    "\n",
    "            if weight is not None:\n",
    "                if self.weight is not None:\n",
    "                    raise DGLError('External weight is provided while at the same time the'\n",
    "                                   ' module has defined its own weight parameter. Please'\n",
    "                                   ' create the module with flag weight=False.')\n",
    "            else:\n",
    "                weight = self.weight\n",
    "\n",
    "            if self._in_feats > self._out_feats:\n",
    "                # mult W first to reduce the feature size for aggregation\n",
    "                if weight is not None:\n",
    "                    feat_src = th.matmul(feat_src, weight)\n",
    "                graph.srcdata['h'] = feat_src\n",
    "                if edge_weights is None:\n",
    "                    graph.update_all(fn.copy_src(src='h', out='m'),\n",
    "                                     fn.sum(msg='m', out='h'))\n",
    "                else:\n",
    "                    graph.edata['a'] = edge_weights\n",
    "                    graph.update_all(fn.u_mul_e('h', 'a', 'm'),\n",
    "                                     fn.sum(msg='m', out='h'))\n",
    "                rst = graph.dstdata['h']\n",
    "            else:\n",
    "                # aggregate first then mult W\n",
    "                graph.srcdata['h'] = feat_src\n",
    "                if edge_weights is None:\n",
    "                    graph.update_all(fn.copy_src(src='h', out='m'),\n",
    "                                     fn.sum(msg='m', out='h'))\n",
    "                else:\n",
    "                    graph.edata['a'] = edge_weights\n",
    "                    graph.update_all(fn.u_mul_e('h', 'a', 'm'),\n",
    "                                     fn.sum(msg='m', out='h'))\n",
    "                rst = graph.dstdata['h']\n",
    "                if weight is not None:\n",
    "                    rst = th.matmul(rst, weight)\n",
    "\n",
    "            if self._norm != 'none':\n",
    "                degs = graph.is_degrees().float().clamp(min=1)\n",
    "                if self._norm == 'both':\n",
    "                    norm = th.pow(degs, -0.5)\n",
    "                else:\n",
    "                    norm = 1.0 / degs\n",
    "                shp = norm.shape + (1,) * (feat_dst.dim() - 1)\n",
    "                norm = th.reshape(norm, shp)\n",
    "                rst = rst * norm\n",
    "\n",
    "            if self.bias is not None:\n",
    "                rst = rst + self.bias\n",
    "\n",
    "            if self._activation is not None:\n",
    "                rst = self._activation(rst)\n",
    "\n",
    "            return rst"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_classes, n_layers, activation, dropout, normalization='none'):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        # input layer\n",
    "        self.layers.append(GraphConv(in_feats, n_hidden, activation=activation, norm=normalization))\n",
    "        # hidden layer\n",
    "        for i in range(n_layers -1):\n",
    "            self.layers.append(GraphConv(n_hidden, n_hidden, activation=activation, norm=normalization))\n",
    "        # output layer\n",
    "        self.layers.append(GraphConv(n_hidden, n_classes, norm=normalization))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, features, g, edge_weight):\n",
    "        h = features\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                h = self.dropout(h)\n",
    "            h = layer(g, h, edge_weights=edge_weight)\n",
    "        return h"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import dgl.function as fn\n",
    "from dgl.nn import GATConv\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, num_layers, in_dim, num_hidden, num_classes, heads, activation, feat_drop=0, attn_drop=0, negative_slope=0.2, residual=False):\n",
    "        super(GAT, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        self.activation = activation\n",
    "        # input projection (no residual)\n",
    "        self.gat_layers.append(GATConv(\n",
    "            in_dim, num_hidden, heads[0],\n",
    "            feat_drop, attn_drop, negative_slope, False, self.activation\n",
    "        ))\n",
    "        # hidden layers\n",
    "        for l in range(1, num_layers):\n",
    "            # due to multi-head, the in-dim = num_hidden * num_heads\n",
    "            self.gat_layers.append(GATConv(\n",
    "                num_hidden * heads[l-1], num_hidden, heads[l],\n",
    "                feat_drop, attn_drop, negative_slope, residual, self.activation\n",
    "            ))\n",
    "        # output projection\n",
    "        self.gat_layers.append(GATConv(\n",
    "            num_hidden * heads[-2], num_classes, heads[-1],\n",
    "            feat_drop, attn_drop, negative_slope, residual, None\n",
    "        ))\n",
    "\n",
    "    def forward(self, inputs, g):\n",
    "        h = inputs\n",
    "        for l in range(self.num_layers):\n",
    "            h = self.gat_layers[l](g, h).flatten(1)\n",
    "        # output projection\n",
    "        logits = self.gat_layers[-1](g, h).mean(1)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='roberta_base', nb_class=20):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.nb_class = nb_class\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "        self.bert_model = AutoModel.from_pretrained(pretrained_model)\n",
    "        self.feat_dim = list(self.bert_model.modules())[-2].out_features\n",
    "        self.classifier = nn.Linear(self.feat_dim, nb_class)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        cls_feats = self.bert_model(input_ids, attention_mask)[0][:, 0]\n",
    "        cls_logit = self.classifier(cls_feats)\n",
    "        return cls_logit\n",
    "\n",
    "class BertGCN(nn.Module):\n",
    "    def __init__(self, pretrained_model='roberta_base', nb_class=20, m=0.7, gcn_layers=2, n_hidden=200, dropout=0.5):\n",
    "        super(BertGCN, self).__init__()\n",
    "        self.m = m\n",
    "        self.nb_class = nb_class\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "        self.bert_model = AutoModel.from_pretrained(pretrained_model)\n",
    "        self.feat_dim = list(self.bert_model.modules())[-2].out_features\n",
    "        self.classifier = nn.Linear(self.feat_dim, nb_class)\n",
    "        self.gcn = GCN(\n",
    "            in_feats=self.feat_dim,\n",
    "            n_hidden=n_hidden,\n",
    "            n_classes=nb_class,\n",
    "            n_layers=gcn_layers-1,\n",
    "            activation=F.elu,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, g, idx):\n",
    "        input_ids, attention_mask = g.ndata['input_ids'][idx], g.ndata['attention_mask'][idx]\n",
    "        if self.training:\n",
    "            cls_feats = self.bert_model(input_ids, attention_mask)[0][:, 0]\n",
    "            g.ndata['cls_feats'][idx] = cls_feats\n",
    "        else:\n",
    "            cls_feats = g.ndata['cls_feats'][idx]\n",
    "        cls_logit = self.classifier(cls_feats)\n",
    "        cls_pred = nn.Softmax(dim=1)(cls_logit)\n",
    "        gcn_logit = self.gcn(g.ndata['cls_feats'], g, g.edata['edge_weight'])[idx]\n",
    "        gcn_pred = nn.Softmax(dim=1)(gcn_logit)\n",
    "        pred = (gcn_pred+1e-10) * self.m + cls_pred * (1 - self.m)\n",
    "        pred = torch.log(pred)\n",
    "        return pred\n",
    "\n",
    "class BertGAT(nn.Module):\n",
    "    def __init__(self, pretrained_model='roberta_base', nb_class=20, m=0.7, gcn_layers=2, heads=8, n_hidden=32, dropout=0.5):\n",
    "        super(BertGAT, self).__init__()\n",
    "        self.m = m\n",
    "        self.nb_class = nb_class\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "        self.bert_model = AutoModel.from_pretrained(pretrained_model)\n",
    "        self.feat_dim = list(self.bert_model.modules())[-2].out_features\n",
    "        self.classifier = nn.Linear(self.feat_dim, nb_class)\n",
    "        self.gcn = GAT(\n",
    "            num_layers=gcn_layers-1,\n",
    "            in_dim=self.feat_dim,\n",
    "            num_hidden=n_hidden,\n",
    "            num_classes=nb_class,\n",
    "            heads=[heads] * (gcn_layers-1) + [1],\n",
    "            activation=F.elu,\n",
    "            feat_drop=dropout,\n",
    "            attn_drop=dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, g, idx):\n",
    "        input_ids, attention_mask = g.ndata['input_ids'][idx], g.ndata['attention_mask'][idx]\n",
    "        if self.training:\n",
    "            cls_feats = self.bert_model(input_ids, attention_mask)[0][:, 0]\n",
    "            g.ndata['cls_feats'][idx] = cls_feats\n",
    "        else:\n",
    "            cls_feats = g.ndata['cls_feats'][idx]\n",
    "        cls_logit = self.classifier(cls_feats)\n",
    "        cls_pred = nn.Softmax(dim=1)(cls_logit)\n",
    "        gcn_logit = self.gcn(g.ndata['cls_feats'], g)[idx]\n",
    "        gcn_pred = nn.Softmax(dim=1)(gcn_logit)\n",
    "        pred = (gcn_pred+1e-10) * self.m + cls_pred * (1 - self.m)\n",
    "        pred = torch.log(pred)\n",
    "        return pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finetune"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arguments:\n",
      "Namespace(batch_size=32, bert_init='roberta-base', bert_lr=0.0001, checkpoint_dir=None, dataset='R8', max_length=128, nb_epochs=10)\n",
      "checkpoints will be saved in ./checkpoint/roberta-base_R8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Train acc: 0.9639 loss: 0.1139  Val acc: 0.9507 loss: 0.1563  Test acc: 0.9456 loss: 0.1521\n",
      "New checkpoint\n",
      "Epoch: 2  Train acc: 0.9684 loss: 0.1044  Val acc: 0.9690 loss: 0.1253  Test acc: 0.9635 loss: 0.1349\n",
      "New checkpoint\n",
      "Epoch: 3  Train acc: 0.9648 loss: 0.1248  Val acc: 0.9562 loss: 0.1599  Test acc: 0.9502 loss: 0.1819\n",
      "Epoch: 4  Train acc: 0.9787 loss: 0.0847  Val acc: 0.9745 loss: 0.1099  Test acc: 0.9749 loss: 0.1052\n",
      "New checkpoint\n",
      "Epoch: 5  Train acc: 0.9885 loss: 0.0539  Val acc: 0.9763 loss: 0.1307  Test acc: 0.9712 loss: 0.1379\n",
      "New checkpoint\n",
      "Epoch: 6  Train acc: 0.9856 loss: 0.0530  Val acc: 0.9726 loss: 0.1374  Test acc: 0.9703 loss: 0.1451\n",
      "Epoch: 7  Train acc: 0.9550 loss: 0.0972  Val acc: 0.9361 loss: 0.1947  Test acc: 0.9370 loss: 0.1685\n",
      "Epoch: 8  Train acc: 0.9799 loss: 0.0919  Val acc: 0.9726 loss: 0.1159  Test acc: 0.9708 loss: 0.1312\n",
      "Epoch: 9  Train acc: 0.9864 loss: 0.0510  Val acc: 0.9726 loss: 0.0994  Test acc: 0.9653 loss: 0.1520\n",
      "Epoch: 10  Train acc: 0.9850 loss: 0.0655  Val acc: 0.9781 loss: 0.1060  Test acc: 0.9726 loss: 0.1240\n",
      "New checkpoint\n"
     ]
    },
    {
     "data": {
      "text/plain": "State:\n\titeration: 1550\n\tepoch: 10\n\tepoch_length: 155\n\tmax_epochs: 10\n\toutput: <class 'tuple'>\n\tbatch: <class 'list'>\n\tmetrics: <class 'dict'>\n\tdataloader: <class 'torch.utils.data.dataloader.DataLoader'>\n\tseed: <class 'NoneType'>\n\ttimes: <class 'dict'>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import dgl\n",
    "import torch\n",
    "import shutil\n",
    "import logging\n",
    "import argparse\n",
    "import warnings\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from ignite.engine import Events, create_supervised_evaluator, create_supervised_trainer, Engine\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--max_length', type=int, default=128, help='the input lenght for bert')\n",
    "parser.add_argument('--batch_size', type=int, default=32)\n",
    "parser.add_argument('--nb_epochs', type=int, default=10)\n",
    "parser.add_argument('--bert_lr', type=float, default=1e-4)\n",
    "parser.add_argument('--dataset', default='R8', choices=['20ng', 'R8', 'R52', 'ohsumed', 'mr'])\n",
    "parser.add_argument('--bert_init', type=str, default='roberta-base',\n",
    "                    choices=['roberta-base', 'roberta-large', 'bert-base-uncased', 'bert-large-uncased'])\n",
    "parser.add_argument('--checkpoint_dir', default=None, help='checkpoint directory, [bert_init]_[dataset] if not specified')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "max_length = args.max_length\n",
    "batch_size = args.batch_size\n",
    "nb_epochs = args.nb_epochs\n",
    "bert_lr = args.bert_lr\n",
    "dataset = args.dataset\n",
    "bert_init = args.bert_init\n",
    "checkpoint_dir = args.checkpoint_dir\n",
    "if checkpoint_dir is None:\n",
    "    ckpt_dir = './checkpoint/{}_{}'.format(bert_init, dataset)\n",
    "else:\n",
    "    ckpt_dir = checkpoint_dir\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "# shutil.copy(os.path.abspath(''), ckpt_dir)\n",
    "\n",
    "sh = logging.StreamHandler(sys.stdout)\n",
    "sh.setFormatter(logging.Formatter('%(message)s'))\n",
    "sh.setLevel(logging.INFO)\n",
    "fh = logging.FileHandler(filename=os.path.join(ckpt_dir, 'training.log'), mode='w')\n",
    "fh.setFormatter(logging.Formatter('%(message)s'))\n",
    "logger = logging.getLogger('Training Logger')\n",
    "logger.addHandler(sh)\n",
    "logger.addHandler(fh)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "cpu = torch.device('cpu')\n",
    "gpu = torch.device('cuda:0')\n",
    "\n",
    "logger.info('arguments:')\n",
    "logger.info(str(args))\n",
    "logger.info('checkpoints will be saved in {}'.format(ckpt_dir))\n",
    "\n",
    "# Data Preprocess\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size = load_corpus(dataset)\n",
    "# compute number of real train/val/test/word nodes and number of classes\n",
    "nb_node = adj.shape[0]\n",
    "nb_train, nb_val, nb_test = train_mask.sum(), val_mask.sum(), test_mask.sum()\n",
    "nb_word = nb_node - nb_train - nb_val - nb_test\n",
    "nb_class = y_train.shape[1]\n",
    "\n",
    "# instantiate model according to class number\n",
    "model = BertClassifier(pretrained_model=bert_init, nb_class=nb_class)\n",
    "# transform one-hot label to class id for pytorch computation\n",
    "y = torch.LongTensor((y_train + y_val + y_test).argmax(axis=1))\n",
    "label = {}\n",
    "label['train'], label['val'], label['test'] = y[:nb_train], y[nb_train:nb_train+nb_val], y[-nb_test:]\n",
    "\n",
    "# load documents and compute input encodings\n",
    "corpus_file = './data/corpus/'+dataset+'_shuffle.txt'\n",
    "with open(corpus_file, 'r') as f:\n",
    "    text = f.read()\n",
    "    text = text.replace('\\\\', '')\n",
    "    text = text.split('\\n')\n",
    "\n",
    "def encode_input(text, tokenizer):\n",
    "    input = tokenizer(text, max_length=max_length, truncation=True, padding=True, return_tensors='pt')\n",
    "    return input.input_ids, input.attention_mask\n",
    "\n",
    "input_ids, attention_mask = {}, {}\n",
    "input_ids_, attention_mask_ = encode_input(text, model.tokenizer)\n",
    "\n",
    "# create train/test/val datasets and dataloaders\n",
    "input_ids['train'], input_ids['val'], input_ids['test'] = input_ids_[:nb_train], input_ids_[nb_train:nb_train+nb_val], input_ids_[-nb_test:]\n",
    "attention_mask['train'], attention_mask['val'], attention_mask['test'] = attention_mask_[:nb_train], attention_mask_[nb_train:nb_train+nb_val], attention_mask_[-nb_test:]\n",
    "\n",
    "datasets = {}\n",
    "loader = {}\n",
    "for split in ['train', 'val', 'test']:\n",
    "    datasets[split] = Data.TensorDataset(input_ids[split], attention_mask[split], label[split])\n",
    "    loader[split] = Data.DataLoader(datasets[split], batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=bert_lr)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[30], gamma=0.1)\n",
    "\n",
    "def train_step(engine, batch):\n",
    "    global model, optimizer\n",
    "    model.train()\n",
    "    model = model.to(gpu)\n",
    "    optimizer.zero_grad()\n",
    "    (input_ids, attention_mask, label) = [x.to(gpu) for x in batch]\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(input_ids, attention_mask)\n",
    "    y_true = label.type(torch.long)\n",
    "    loss = F.cross_entropy(y_pred, y_true)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss = loss.item()\n",
    "    with torch.no_grad():\n",
    "        y_true = y_true.detach().cpu()\n",
    "        y_pred = y_pred.argmax(axis=1).detach().cpu()\n",
    "        train_acc = accuracy_score(y_true, y_pred)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "trainer = Engine(train_step)\n",
    "\n",
    "def test_step(engine, batch):\n",
    "    global model\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model = model.to(gpu)\n",
    "        (input_ids, attention_mask, label) = [x.to(gpu) for x in batch]\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(input_ids, attention_mask)\n",
    "        y_true = label\n",
    "        return y_pred, y_true\n",
    "\n",
    "evaluator = Engine(test_step)\n",
    "metrics = {\n",
    "    'acc': Accuracy(),\n",
    "    'nll': Loss(torch.nn.CrossEntropyLoss())\n",
    "}\n",
    "for n, f in metrics.items():\n",
    "    f.attach(evaluator, n)\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    evaluator.run(loader['train'])\n",
    "    metrics = evaluator.state.metrics\n",
    "    train_acc, train_nll = metrics[\"acc\"], metrics[\"nll\"]\n",
    "    evaluator.run(loader['val'])\n",
    "    metrics = evaluator.state.metrics\n",
    "    val_acc, val_nll = metrics[\"acc\"], metrics[\"nll\"]\n",
    "    evaluator.run(loader['test'])\n",
    "    metrics = evaluator.state.metrics\n",
    "    test_acc, test_nll = metrics[\"acc\"], metrics[\"nll\"]\n",
    "    logger.info(\n",
    "        \"\\rEpoch: {}  Train acc: {:.4f} loss: {:.4f}  Val acc: {:.4f} loss: {:.4f}  Test acc: {:.4f} loss: {:.4f}\"\n",
    "        .format(trainer.state.epoch, train_acc, train_nll, val_acc, val_nll, test_acc, test_nll)\n",
    "    )\n",
    "    if val_acc > log_training_results.best_val_acc:\n",
    "        logger.info(\"New checkpoint\")\n",
    "        torch.save(\n",
    "            {\n",
    "                'bert_model': model.bert_model.state_dict(),\n",
    "                'classifier': model.classifier.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'epoch': trainer.state.epoch,\n",
    "            },\n",
    "            os.path.join(\n",
    "                ckpt_dir, 'checkpoint.pth'\n",
    "            )\n",
    "        )\n",
    "        log_training_results.best_val_acc = val_acc\n",
    "    scheduler.step()\n",
    "\n",
    "log_training_results.best_val_acc = 0\n",
    "trainer.run(loader['train'], max_epochs=nb_epochs) "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pygeo",
   "language": "python",
   "display_name": "pygeo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}