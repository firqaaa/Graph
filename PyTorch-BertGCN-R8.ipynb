{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Utils"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import eigsh\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def load_data(dataset_str):\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(\"data/ind.{}.test.index\".format(dataset_str))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "    print_log(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
    "\n",
    "    if dataset_str == \"citeseer\":\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(\n",
    "            min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "    idx_test = test_idx_range.tolist()\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y)+500)\n",
    "\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "def load_corpus(dataset_str):\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'adj']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"./data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, adj = tuple(objects)\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    labels = np.vstack((ally, ty))\n",
    "\n",
    "    train_idx_orig = parse_index_file(\n",
    "        \"./data/{}.train.index\".format(dataset_str)\n",
    "    )\n",
    "    train_size = len(train_idx_orig)\n",
    "\n",
    "    val_size = train_size - x.shape[0]\n",
    "    test_size = tx.shape[0]\n",
    "\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y) + val_size)\n",
    "    idx_test = range(allx.shape[0], allx.shape[0] + test_size)\n",
    "\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "    return sparse_mx\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalized feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return features.A\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symetrically normalize adjacency matrix\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    return adj_normalized.A\n",
    "\n",
    "def construct_feed_dict(features, support, labels, labels_mask, placeholders):\n",
    "    \"\"\"Construct feed dictionary\"\"\"\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['labels']: labels})\n",
    "    feed_dict.update({placeholders['labels_mask']: labels_mask})\n",
    "    feed_dict.update({placeholders['features']: features})\n",
    "    feed_dict.update({placeholders['support'][i]: support[i] for i in range(len(len(support)))})\n",
    "    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n",
    "    return feed_dict\n",
    "\n",
    "def chebyshev_polynomials(adj, k):\n",
    "    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrix (tuple representation)\"\"\"\n",
    "    print_log(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\n",
    "\n",
    "    adj_normalized = normalize_adj(adj)\n",
    "    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n",
    "    largest_eigval, _ = eigsh(laplacian, 1, which='LM')\n",
    "    scaled_laplacian = (2. / largest_eigval[0]) * laplacian - sp.eye(adj.shape[0])\n",
    "\n",
    "    t_k = list()\n",
    "    t_k.append(sp.eye(adj.shape[0]).A)\n",
    "    t_k.append(scaled_laplacian.A)\n",
    "\n",
    "    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n",
    "        s_lap = sp.csr_matrix(scaled_lap, copy=True)\n",
    "        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n",
    "\n",
    "    for i in range(2, k+1):\n",
    "        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n",
    "\n",
    "    return t_k\n",
    "\n",
    "def loadWord2Vec(filename):\n",
    "    \"\"\"Read Word Vectors\"\"\"\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    word_vector_map = {}\n",
    "    file = open(filename, 'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        if (len(row) > 2):\n",
    "            vocab.append(row[0])\n",
    "            vector = row[1:]\n",
    "            length = len(vector)\n",
    "            for i in range(length):\n",
    "                vector[i] = float(vector[i])\n",
    "            embd.append(vector)\n",
    "            word_vector_map[row[0]] = vector\n",
    "    print_log('Loaded Word Vectors!')\n",
    "    file.close()\n",
    "    return vocab, embd, word_vector_map\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def print_log(msg='', end='\\n'):\n",
    "    now = datetime.datetime.now()\n",
    "    t = str(now.year) + '/' + str(now.month) + '/' + str(now.day) + ' '\\\n",
    "        + str(now.hour).zfill(2) + ':' + str(now.minute).zfill(2) + ':' + str(now.second).zfill(2)\n",
    "\n",
    "    if isinstance(msg, str):\n",
    "        lines = msg.split('\\n')\n",
    "    else:\n",
    "        lines = [msg]\n",
    "\n",
    "    for line in lines:\n",
    "        if line == lines[-1]:\n",
    "            print('[' + t + '] ' + str(line), end=end)\n",
    "        else:\n",
    "            print('[' + t + '] ' + str(line))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build Graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)\n"
     ]
    }
   ],
   "source": [
    "# build graph\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from math import log\n",
    "from sklearn import svm\n",
    "import scipy.sparse as sp\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "datasets = ['20ng', 'R8', 'R52', 'ohsumed', 'mr']\n",
    "dataset = \"R8\"\n",
    "\n",
    "if dataset not in datasets:\n",
    "    sys.exit(\"wrong dataset name\")\n",
    "\n",
    "word_embedding_dim = 300\n",
    "word_vector_map = {}\n",
    "\n",
    "# Shuffling\n",
    "doc_name_list = []\n",
    "doc_train_list = []\n",
    "doc_test_list = []\n",
    "\n",
    "with open(\"./data/\" + dataset + '.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        doc_name_list.append(line.strip())\n",
    "        temp = line.split(\"\\t\")\n",
    "        if temp[1].find(\"test\") != -1:\n",
    "            doc_test_list.append(line.strip())\n",
    "        elif temp[1].find('train') != -1:\n",
    "            doc_train_list.append(line.strip())\n",
    "\n",
    "doc_content_list = []\n",
    "with open(\"./data/corpus/\" + dataset + '.clean.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        doc_content_list.append(line.strip())\n",
    "\n",
    "train_ids = []\n",
    "for train_name in doc_train_list:\n",
    "    train_id = doc_name_list.index(train_name)\n",
    "    train_ids.append(train_id)\n",
    "# print(train_ids)\n",
    "random.shuffle(train_ids)\n",
    "\n",
    "train_ids_str = \"\\n\".join(str(index) for index in train_ids)\n",
    "with open(\"./data/\" + dataset + '.train.index', 'w') as f:\n",
    "    f.write(train_ids_str)\n",
    "\n",
    "test_ids = []\n",
    "for test_name in doc_test_list:\n",
    "    test_id = doc_name_list.index(test_name)\n",
    "    test_ids.append(test_id)\n",
    "# print(test_ids)\n",
    "random.shuffle(test_ids)\n",
    "\n",
    "test_ids_str = \"\\n\".join(str(index) for index in test_ids)\n",
    "with open(\"./data/\" + dataset + \".test.index\", \"w\") as f:\n",
    "    f.write(test_ids_str)\n",
    "\n",
    "ids = train_ids + test_ids\n",
    "# print(ids)\n",
    "# print(len(ids))\n",
    "\n",
    "shuffle_doc_name_list = []\n",
    "shuffle_doc_words_list = []\n",
    "for id in ids:\n",
    "    shuffle_doc_name_list.append(doc_name_list[int(id)])\n",
    "    shuffle_doc_words_list.append(doc_content_list[int(id)])\n",
    "shuffle_doc_name_str = \"\\n\".join(shuffle_doc_name_list)\n",
    "shuffle_doc_words_str = \"\\n\".join(shuffle_doc_words_list)\n",
    "\n",
    "with open(\"./data/\" + dataset + \"_shuffle.txt\", \"w\") as f:\n",
    "    f.write(shuffle_doc_name_str)\n",
    "\n",
    "with open(\"./data/corpus/\" + dataset + \"_shuffle.txt\", \"w\") as f:\n",
    "    f.write(shuffle_doc_words_str)\n",
    "\n",
    "# build vocab\n",
    "word_freq = {}\n",
    "word_set = set()\n",
    "for doc_words in shuffle_doc_words_list:\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_set.add(word)\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "\n",
    "vocab = list(word_set)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_doc_list = {}\n",
    "\n",
    "for i in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    appeared = set()\n",
    "    for word in words:\n",
    "        if word in appeared:\n",
    "            continue\n",
    "        if word in word_doc_list:\n",
    "            doc_list = word_doc_list[word]\n",
    "            doc_list.append(i)\n",
    "            word_doc_list[word] = doc_list\n",
    "        else:\n",
    "            word_doc_list[word] = [i]\n",
    "        appeared.add(word)\n",
    "\n",
    "word_doc_freq = {}\n",
    "for word, doc_list in word_doc_list.items():\n",
    "    word_doc_freq[word] = len(doc_list)\n",
    "\n",
    "word_id_map = {}\n",
    "for i in range(vocab_size):\n",
    "    word_id_map[vocab[i]] = i\n",
    "\n",
    "vocab_str = \"\\n\".join(vocab)\n",
    "\n",
    "with open(\"./data/corpus/\" + dataset + \"_vocab.txt\", \"w\") as f:\n",
    "    f.write(vocab_str)\n",
    "\n",
    "# Label List\n",
    "label_set = set()\n",
    "for doc_meta in shuffle_doc_name_list:\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label_set.add(temp[2])\n",
    "label_list = list(label_set)\n",
    "\n",
    "label_list_str = '\\n'.join(label_list)\n",
    "with open('./data/corpus/' + dataset + '_labels.txt', 'w') as f:\n",
    "    f.write(label_list_str)\n",
    "\n",
    "train_size = len(train_ids)\n",
    "val_size = int(0.1 * train_size)\n",
    "real_train_size = train_size - val_size\n",
    "\n",
    "real_train_doc_names = shuffle_doc_name_list[:real_train_size]\n",
    "real_train_doc_names_str = '\\n'.join(real_train_doc_names)\n",
    "\n",
    "with open('./data/' + dataset + '.real_train.name', 'w') as f:\n",
    "    f.write(real_train_doc_names_str)\n",
    "\n",
    "row_x = []\n",
    "col_x = []\n",
    "data_x = []\n",
    "for i in range(real_train_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embedding_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embedding_dim):\n",
    "        row_x.append(i)\n",
    "        col_x.append(j)\n",
    "        data_x.append(doc_vec[j] / doc_len)\n",
    "\n",
    "x = sp.csr_matrix((data_x, (row_x, col_x)), shape=(real_train_size, word_embedding_dim))\n",
    "\n",
    "y = []\n",
    "for i in range(real_train_size):\n",
    "    doc_meta = shuffle_doc_name_list[i]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    y.append(one_hot)\n",
    "y = np.array(y)\n",
    "# print(y)\n",
    "\n",
    "# tx: feature vectors of test docs, no initial features\n",
    "test_size = len(test_ids)\n",
    "\n",
    "row_tx = []\n",
    "col_tx = []\n",
    "data_tx = []\n",
    "for i in range(test_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embedding_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i + train_size]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embedding_dim):\n",
    "        row_tx.append(i)\n",
    "        col_tx.append(j)\n",
    "        data_tx.append(doc_vec[j] / doc_len)\n",
    "\n",
    "tx = sp.csr_matrix((data_tx, (row_tx, col_tx)),\n",
    "                   shape=(test_size, word_embedding_dim))\n",
    "\n",
    "ty = []\n",
    "for i in range(test_size):\n",
    "    doc_meta = shuffle_doc_name_list[i + train_size]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    ty.append(one_hot)\n",
    "ty = np.array(ty)\n",
    "# print(ty)\n",
    "\n",
    "# allx: the the feature vectors of both labeled and unlabeled training instances\n",
    "# (a superset of x)\n",
    "# unlabeled training instances -> words\n",
    "\n",
    "word_vectors = np.random.uniform(-0.01, 0.01, (vocab_size, word_embedding_dim))\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "    word = vocab[i]\n",
    "    if word in word_vector_map:\n",
    "        vector = word_vector_map[word]\n",
    "        word_vectors[i] = vector\n",
    "\n",
    "row_allx = []\n",
    "col_allx = []\n",
    "data_allx = []\n",
    "\n",
    "for i in range(train_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embedding_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "    for j in range(word_embedding_dim):\n",
    "        row_allx.append(int(i))\n",
    "        col_allx.append(j)\n",
    "        data_allx.append(doc_vec[j] / doc_len)\n",
    "for i in range(vocab_size):\n",
    "    for j in range(word_embedding_dim):\n",
    "        row_allx.append(int(i + train_size))\n",
    "        col_allx.append(j)\n",
    "        data_allx.append(word_vectors.item((i, j)))\n",
    "\n",
    "row_allx = np.array(row_allx)\n",
    "col_allx = np.array(col_allx)\n",
    "data_allx = np.array(data_allx)\n",
    "\n",
    "allx = sp.csr_matrix((data_allx, (row_allx, col_allx)), shape=(train_size + vocab_size, word_embedding_dim))\n",
    "\n",
    "ally = []\n",
    "for i in range(train_size):\n",
    "    doc_meta = shuffle_doc_name_list[i]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    ally.append(one_hot)\n",
    "\n",
    "for i in range(vocab_size):\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    ally.append(one_hot)\n",
    "\n",
    "ally = np.array(ally)\n",
    "print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
    "\n",
    "'''\n",
    "Doc word heterogeneous graph\n",
    "'''\n",
    "\n",
    "# word co-occurence with context windows\n",
    "window_size = 20\n",
    "windows = []\n",
    "\n",
    "for doc_words in shuffle_doc_words_list:\n",
    "    words = doc_words.split()\n",
    "    length = len(words)\n",
    "    if length <= window_size:\n",
    "        windows.append(words)\n",
    "    else:\n",
    "        for j in range(length - window_size + 1):\n",
    "            window = words[j: j + window_size]\n",
    "            windows.append(window)\n",
    "\n",
    "word_window_freq = {}\n",
    "for window in windows:\n",
    "    appeared = set()\n",
    "    for i in range(len(window)):\n",
    "        if window[i] in appeared:\n",
    "            continue\n",
    "        if window[i] in word_window_freq:\n",
    "            word_window_freq[window[i]] += 1\n",
    "        else:\n",
    "            word_window_freq[window[i]] = 1\n",
    "        appeared.add(window[i])\n",
    "\n",
    "word_pair_count = {}\n",
    "for window in windows:\n",
    "    for i in range(1, len(window)):\n",
    "        for j in range(0, i):\n",
    "            word_i = window[i]\n",
    "            word_i_id = word_id_map[word_i]\n",
    "            word_j = window[j]\n",
    "            word_j_id = word_id_map[word_j]\n",
    "            if word_i_id == word_j_id:\n",
    "                continue\n",
    "            word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n",
    "            # two orders\n",
    "            word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n",
    "\n",
    "row = []\n",
    "col = []\n",
    "weight = []\n",
    "\n",
    "# pmi as weight\n",
    "num_window = len(windows)\n",
    "\n",
    "for key in word_pair_count:\n",
    "    temp = key.split(',')\n",
    "    i = int(temp[0])\n",
    "    j = int(temp[1])\n",
    "    count = word_pair_count[key]\n",
    "    word_freq_i = word_window_freq[vocab[i]]\n",
    "    word_freq_j = word_window_freq[vocab[j]]\n",
    "    pmi = log((1.0 * count / num_window) /\n",
    "              (1.0 * word_freq_i * word_freq_j / (num_window * num_window)))\n",
    "    if pmi <= 0:\n",
    "        continue\n",
    "    row.append(train_size + i)\n",
    "    col.append(train_size + j)\n",
    "    weight.append(pmi)\n",
    "\n",
    "# word vector cosine similarity as weight\n",
    "\n",
    "# doc word frequency\n",
    "doc_word_freq = {}\n",
    "\n",
    "for doc_id in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[doc_id]\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_id = word_id_map[word]\n",
    "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
    "        if doc_word_str in doc_word_freq:\n",
    "            doc_word_freq[doc_word_str] += 1\n",
    "        else:\n",
    "            doc_word_freq[doc_word_str] = 1\n",
    "\n",
    "for i in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_word_set = set()\n",
    "    for word in words:\n",
    "        if word in doc_word_set:\n",
    "            continue\n",
    "        j = word_id_map[word]\n",
    "        key = str(i) + ',' + str(j)\n",
    "        freq = doc_word_freq[key]\n",
    "        if i < train_size:\n",
    "            row.append(i)\n",
    "        else:\n",
    "            row.append(i + vocab_size)\n",
    "        col.append(train_size + j)\n",
    "        idf = log(1.0 * len(shuffle_doc_words_list) /\n",
    "                  word_doc_freq[vocab[j]])\n",
    "        weight.append(freq * idf)\n",
    "        doc_word_set.add(word)\n",
    "\n",
    "node_size = train_size + vocab_size + test_size\n",
    "adj = sp.csr_matrix(\n",
    "    (weight, (row, col)), shape=(node_size, node_size))\n",
    "\n",
    "# dump objects\n",
    "with open(\"./data/ind.{}.x\".format(dataset), 'wb') as f:\n",
    "    pkl.dump(x, f)\n",
    "\n",
    "with open(\"./data/ind.{}.y\".format(dataset), 'wb') as f:\n",
    "    pkl.dump(y, f)\n",
    "\n",
    "with open(\"./data/ind.{}.tx\".format(dataset), 'wb') as f:\n",
    "    pkl.dump(tx, f)\n",
    "\n",
    "with open(\"./data/ind.{}.ty\".format(dataset), 'wb') as f:\n",
    "    pkl.dump(ty, f)\n",
    "\n",
    "with open(\"./data/ind.{}.allx\".format(dataset), 'wb') as f:\n",
    "    pkl.dump(allx, f)\n",
    "\n",
    "with open(\"./data/ind.{}.ally\".format(dataset), 'wb') as f:\n",
    "    pkl.dump(ally, f)\n",
    "\n",
    "with open(\"./data/ind.{}.adj\".format(dataset), 'wb') as f:\n",
    "    pkl.dump(adj, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from dgl.base import DGLError\n",
    "from dgl import function as fn\n",
    "from dgl.utils import expand_as_pair\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "\n",
    "class GraphConvEdgeWeight(GraphConv):\n",
    "\n",
    "    def forward(self, graph, feat, weight=None, edge_weights=None):\n",
    "        with graph.local_scope():\n",
    "            if not self._allow_zero_in_degree:\n",
    "                if (graph.in_degree() == 0).any():\n",
    "                    raise DGLError('There are 0-in-degree nodes in the graph, '\n",
    "                                   'output for those nodes will be invalid. '\n",
    "                                   'This is harmful for some applications, '\n",
    "                                   'causing silent performance regression. '\n",
    "                                   'Adding self-loop on the input graph by '\n",
    "                                   'calling `g = dgl.add_self_loop(g)` will resolve '\n",
    "                                   'the issue. Setting ``allow_zero_in_degree`` '\n",
    "                                   'to be `True` when constructing this module will '\n",
    "                                   'suppress the check and let the code run.')\n",
    "\n",
    "            # (BarclayII) For RGCN on heterogeneous graphs we need to support GCN on bipartite.\n",
    "            feat_src, feat_dst = expand_as_pair(feat, graph)\n",
    "            if self._norm == 'both':\n",
    "                degs = graph.out_degrees().float().clamp(min=1)\n",
    "                norm = th.pow(degs, -0.5)\n",
    "                shp = norm.shape + (1, ) * (feat_src.dim() - 1)\n",
    "                norm = th.reshape(norm, shp)\n",
    "                feat_src = feat_src * norm\n",
    "\n",
    "            if weight is not None:\n",
    "                if self.weight is not None:\n",
    "                    raise DGLError('External weight is provided while at the same time the'\n",
    "                                   ' module has defined its own weight parameter. Please'\n",
    "                                   ' create the module with flag weight=False.')\n",
    "            else:\n",
    "                weight = self.weight\n",
    "\n",
    "            if self._in_feats > self._out_feats:\n",
    "                # mult W first to reduce the feature size for aggregation\n",
    "                if weight is not None:\n",
    "                    feat_src = th.matmul(feat_src, weight)\n",
    "                graph.srcdata['h'] = feat_src\n",
    "                if edge_weights is None:\n",
    "                    graph.update_all(fn.copy_src(src='h', out='m'),\n",
    "                                     fn.sum(msg='m', out='h'))\n",
    "                else:\n",
    "                    graph.edata['a'] = edge_weights\n",
    "                    graph.update_all(fn.u_mul_e('h', 'a', 'm'),\n",
    "                                     fn.sum(msg='m', out='h'))\n",
    "                rst = graph.dstdata['h']\n",
    "            else:\n",
    "                # aggregate first then mult W\n",
    "                graph.srcdata['h'] = feat_src\n",
    "                if edge_weights is None:\n",
    "                    graph.update_all(fn.copy_src(src='h', out='m'),\n",
    "                                     fn.sum(msg='m', out='h'))\n",
    "                else:\n",
    "                    graph.edata['a'] = edge_weights\n",
    "                    graph.update_all(fn.u_mul_e('h', 'a', 'm'),\n",
    "                                     fn.sum(msg='m', out='h'))\n",
    "                rst = graph.dstdata['h']\n",
    "                if weight is not None:\n",
    "                    rst = th.matmul(rst, weight)\n",
    "\n",
    "            if self._norm != 'none':\n",
    "                degs = graph.is_degrees().float().clamp(min=1)\n",
    "                if self._norm == 'both':\n",
    "                    norm = th.pow(degs, -0.5)\n",
    "                else:\n",
    "                    norm = 1.0 / degs\n",
    "                shp = norm.shape + (1,) * (feat_dst.dim() - 1)\n",
    "                norm = th.reshape(norm, shp)\n",
    "                rst = rst * norm\n",
    "\n",
    "            if self.bias is not None:\n",
    "                rst = rst + self.bias\n",
    "\n",
    "            if self._activation is not None:\n",
    "                rst = self._activation(rst)\n",
    "\n",
    "            return rst"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_classes, n_layers, activation, dropout, normalization='none'):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        # input layer\n",
    "        self.layers.append(GraphConv(in_feats, n_hidden, activation=activation, norm=normalization))\n",
    "        # hidden layer\n",
    "        for i in range(n_layers -1):\n",
    "            self.layers.append(GraphConv(n_hidden, n_hidden, activation=activation, norm=normalization))\n",
    "        # output layer\n",
    "        self.layers.append(GraphConv(n_hidden, n_classes, norm=normalization))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, features, g, edge_weight):\n",
    "        h = features\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                h = self.dropout(h)\n",
    "            h = layer(g, h, edge_weights=edge_weight)\n",
    "        return h"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import dgl.function as fn\n",
    "from dgl.nn import GATConv\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, num_layers, in_dim, num_hidden, num_classes, heads, activation, feat_drop=0, attn_drop=0, negative_slope=0.2, residual=False):\n",
    "        super(GAT, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        self.activation = activation\n",
    "        # input projection (no residual)\n",
    "        self.gat_layers.append(GATConv(\n",
    "            in_dim, num_hidden, heads[0],\n",
    "            feat_drop, attn_drop, negative_slope, False, self.activation\n",
    "        ))\n",
    "        # hidden layers\n",
    "        for l in range(1, num_layers):\n",
    "            # due to multi-head, the in-dim = num_hidden * num_heads\n",
    "            self.gat_layers.append(GATConv(\n",
    "                num_hidden * heads[l-1], num_hidden, heads[l],\n",
    "                feat_drop, attn_drop, negative_slope, residual, self.activation\n",
    "            ))\n",
    "        # output projection\n",
    "        self.gat_layers.append(GATConv(\n",
    "            num_hidden * heads[-2], num_classes, heads[-1],\n",
    "            feat_drop, attn_drop, negative_slope, residual, None\n",
    "        ))\n",
    "\n",
    "    def forward(self, inputs, g):\n",
    "        h = inputs\n",
    "        for l in range(self.num_layers):\n",
    "            h = self.gat_layers[l](g, h).flatten(1)\n",
    "        # output projection\n",
    "        logits = self.gat_layers[-1](g, h).mean(1)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='roberta_base', nb_class=20):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.nb_class = nb_class\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "        self.bert_model = AutoModel.from_pretrained(pretrained_model)\n",
    "        self.feat_dim = list(self.bert_model.modules())[-2].out_features\n",
    "        self.classifier = nn.Linear(self.feat_dim, nb_class)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        cls_feats = self.bert_model(input_ids, attention_mask)[0][:, 0]\n",
    "        cls_logit = self.classifier(cls_feats)\n",
    "        return cls_logit\n",
    "\n",
    "class BertGCN(nn.Module):\n",
    "    def __init__(self, pretrained_model='roberta_base', nb_class=20, m=0.7, gcn_layers=2, n_hidden=200, dropout=0.5):\n",
    "        super(BertGCN, self).__init__()\n",
    "        self.m = m\n",
    "        self.nb_class = nb_class\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "        self.bert_model = AutoModel.from_pretrained(pretrained_model)\n",
    "        self.feat_dim = list(self.bert_model.modules())[-2].out_features\n",
    "        self.classifier = nn.Linear(self.feat_dim, nb_class)\n",
    "        self.gcn = GCN(\n",
    "            in_feats=self.feat_dim,\n",
    "            n_hidden=n_hidden,\n",
    "            n_classes=nb_class,\n",
    "            n_layers=gcn_layers-1,\n",
    "            activation=F.elu,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, g, idx):\n",
    "        input_ids, attention_mask = g.ndata['input_ids'][idx], g.ndata['attention_mask'][idx]\n",
    "        if self.training:\n",
    "            cls_feats = self.bert_model(input_ids, attention_mask)[0][:, 0]\n",
    "            g.ndata['cls_feats'][idx] = cls_feats\n",
    "        else:\n",
    "            cls_feats = g.ndata['cls_feats'][idx]\n",
    "        cls_logit = self.classifier(cls_feats)\n",
    "        cls_pred = nn.Softmax(dim=1)(cls_logit)\n",
    "        gcn_logit = self.gcn(g.ndata['cls_feats'], g, g.edata['edge_weight'])[idx]\n",
    "        gcn_pred = nn.Softmax(dim=1)(gcn_logit)\n",
    "        pred = (gcn_pred+1e-10) * self.m + cls_pred * (1 - self.m)\n",
    "        pred = torch.log(pred)\n",
    "        return pred\n",
    "\n",
    "class BertGAT(nn.Module):\n",
    "    def __init__(self, pretrained_model='roberta_base', nb_class=20, m=0.7, gcn_layers=2, heads=8, n_hidden=32, dropout=0.5):\n",
    "        super(BertGAT, self).__init__()\n",
    "        self.m = m\n",
    "        self.nb_class = nb_class\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "        self.bert_model = AutoModel.from_pretrained(pretrained_model)\n",
    "        self.feat_dim = list(self.bert_model.modules())[-2].out_features\n",
    "        self.classifier = nn.Linear(self.feat_dim, nb_class)\n",
    "        self.gcn = GAT(\n",
    "            num_layers=gcn_layers-1,\n",
    "            in_dim=self.feat_dim,\n",
    "            num_hidden=n_hidden,\n",
    "            num_classes=nb_class,\n",
    "            heads=[heads] * (gcn_layers-1) + [1],\n",
    "            activation=F.elu,\n",
    "            feat_drop=dropout,\n",
    "            attn_drop=dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, g, idx):\n",
    "        input_ids, attention_mask = g.ndata['input_ids'][idx], g.ndata['attention_mask'][idx]\n",
    "        if self.training:\n",
    "            cls_feats = self.bert_model(input_ids, attention_mask)[0][:, 0]\n",
    "            g.ndata['cls_feats'][idx] = cls_feats\n",
    "        else:\n",
    "            cls_feats = g.ndata['cls_feats'][idx]\n",
    "        cls_logit = self.classifier(cls_feats)\n",
    "        cls_pred = nn.Softmax(dim=1)(cls_logit)\n",
    "        gcn_logit = self.gcn(g.ndata['cls_feats'], g)[idx]\n",
    "        gcn_pred = nn.Softmax(dim=1)(gcn_logit)\n",
    "        pred = (gcn_pred+1e-10) * self.m + cls_pred * (1 - self.m)\n",
    "        pred = torch.log(pred)\n",
    "        return pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finetune"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import dgl\n",
    "import torch\n",
    "import shutil\n",
    "import logging\n",
    "import argparse\n",
    "import warnings\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from ignite.engine import Events, create_supervised_evaluator, create_supervised_trainer, Engine\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--max_length', type=int, default=128, help='the input lenght for bert')\n",
    "parser.add_argument('--batch_size', type=int, default=32)\n",
    "parser.add_argument('--nb_epochs', type=int, default=10)\n",
    "parser.add_argument('--bert_lr', type=float, default=1e-4)\n",
    "parser.add_argument('--dataset', default='R8', choices=['20ng', 'R8', 'R52', 'ohsumed', 'mr'])\n",
    "parser.add_argument('--bert_init', type=str, default='roberta-base',\n",
    "                    choices=['roberta-base', 'roberta-large', 'bert-base-uncased', 'bert-large-uncased'])\n",
    "parser.add_argument('--checkpoint_dir', default=None, help='checkpoint directory, [bert_init]_[dataset] if not specified')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "max_length = args.max_length\n",
    "batch_size = args.batch_size\n",
    "nb_epochs = args.nb_epochs\n",
    "bert_lr = args.bert_lr\n",
    "dataset = args.dataset\n",
    "bert_init = args.bert_init\n",
    "checkpoint_dir = args.checkpoint_dir\n",
    "if checkpoint_dir is None:\n",
    "    ckpt_dir = './checkpoint/{}_{}'.format(bert_init, dataset)\n",
    "else:\n",
    "    ckpt_dir = checkpoint_dir\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "# shutil.copy(os.path.abspath(''), ckpt_dir)\n",
    "\n",
    "sh = logging.StreamHandler(sys.stdout)\n",
    "sh.setFormatter(logging.Formatter('%(message)s'))\n",
    "sh.setLevel(logging.INFO)\n",
    "fh = logging.FileHandler(filename=os.path.join(ckpt_dir, 'training.log'), mode='w')\n",
    "fh.setFormatter(logging.Formatter('%(message)s'))\n",
    "logger = logging.getLogger('Training Logger')\n",
    "logger.addHandler(sh)\n",
    "logger.addHandler(fh)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "cpu = torch.device('cpu')\n",
    "gpu = torch.device('cuda:0')\n",
    "\n",
    "logger.info('arguments:')\n",
    "logger.info(str(args))\n",
    "logger.info('checkpoints will be saved in {}'.format(ckpt_dir))\n",
    "\n",
    "# Data Preprocess\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size = load_corpus(dataset)\n",
    "# compute number of real train/val/test/word nodes and number of classes\n",
    "nb_node = adj.shape[0]\n",
    "nb_train, nb_val, nb_test = train_mask.sum(), val_mask.sum(), test_mask.sum()\n",
    "nb_word = nb_node - nb_train - nb_val - nb_test\n",
    "nb_class = y_train.shape[1]\n",
    "\n",
    "# instantiate model according to class number\n",
    "model = BertClassifier(pretrained_model=bert_init, nb_class=nb_class)\n",
    "# transform one-hot label to class id for pytorch computation\n",
    "y = torch.LongTensor((y_train + y_val + y_test).argmax(axis=1))\n",
    "label = {}\n",
    "label['train'], label['val'], label['test'] = y[:nb_train], y[nb_train:nb_train+nb_val], y[-nb_test:]\n",
    "\n",
    "# load documents and compute input encodings\n",
    "corpus_file = './data/corpus/'+dataset+'_shuffle.txt'\n",
    "with open(corpus_file, 'r') as f:\n",
    "    text = f.read()\n",
    "    text = text.replace('\\\\', '')\n",
    "    text = text.split('\\n')\n",
    "\n",
    "def encode_input(text, tokenizer):\n",
    "    input = tokenizer(text, max_length=max_length, truncation=True, padding=True, return_tensors='pt')\n",
    "    return input.input_ids, input.attention_mask\n",
    "\n",
    "input_ids, attention_mask = {}, {}\n",
    "input_ids_, attention_mask_ = encode_input(text, model.tokenizer)\n",
    "\n",
    "# create train/test/val datasets and dataloaders\n",
    "input_ids['train'], input_ids['val'], input_ids['test'] = input_ids_[:nb_train], input_ids_[nb_train:nb_train+nb_val], input_ids_[-nb_test:]\n",
    "attention_mask['train'], attention_mask['val'], attention_mask['test'] = attention_mask_[:nb_train], attention_mask_[nb_train:nb_train+nb_val], attention_mask_[-nb_test:]\n",
    "\n",
    "datasets = {}\n",
    "loader = {}\n",
    "for split in ['train', 'val', 'test']:\n",
    "    datasets[split] = Data.TensorDataset(input_ids[split], attention_mask[split], label[split])\n",
    "    loader[split] = Data.DataLoader(datasets[split], batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=bert_lr)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[30], gamma=0.1)\n",
    "\n",
    "def train_step(engine, batch):\n",
    "    global model, optimizer\n",
    "    model.train()\n",
    "    model = model.to(gpu)\n",
    "    optimizer.zero_grad()\n",
    "    (input_ids, attention_mask, label) = [x.to(gpu) for x in batch]\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(input_ids, attention_mask)\n",
    "    y_true = label.type(torch.long)\n",
    "    loss = F.cross_entropy(y_pred, y_true)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss = loss.item()\n",
    "    with torch.no_grad():\n",
    "        y_true = y_true.detach().cpu()\n",
    "        y_pred = y_pred.argmax(axis=1).detach().cpu()\n",
    "        train_acc = accuracy_score(y_true, y_pred)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "trainer = Engine(train_step)\n",
    "\n",
    "def test_step(engine, batch):\n",
    "    global model\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model = model.to(gpu)\n",
    "        (input_ids, attention_mask, label) = [x.to(gpu) for x in batch]\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(input_ids, attention_mask)\n",
    "        y_true = label\n",
    "        return y_pred, y_true\n",
    "\n",
    "evaluator = Engine(test_step)\n",
    "metrics = {\n",
    "    'acc': Accuracy(),\n",
    "    'nll': Loss(torch.nn.CrossEntropyLoss())\n",
    "}\n",
    "for n, f in metrics.items():\n",
    "    f.attach(evaluator, n)\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    evaluator.run(loader['train'])\n",
    "    metrics = evaluator.state.metrics\n",
    "    train_acc, train_nll = metrics[\"acc\"], metrics[\"nll\"]\n",
    "    evaluator.run(loader['val'])\n",
    "    metrics = evaluator.state.metrics\n",
    "    val_acc, val_nll = metrics[\"acc\"], metrics[\"nll\"]\n",
    "    evaluator.run(loader['test'])\n",
    "    metrics = evaluator.state.metrics\n",
    "    test_acc, test_nll = metrics[\"acc\"], metrics[\"nll\"]\n",
    "    logger.info(\n",
    "        \"\\rEpoch: {}  Train acc: {:.4f} loss: {:.4f}  Val acc: {:.4f} loss: {:.4f}  Test acc: {:.4f} loss: {:.4f}\"\n",
    "        .format(trainer.state.epoch, train_acc, train_nll, val_acc, val_nll, test_acc, test_nll)\n",
    "    )\n",
    "    if val_acc > log_training_results.best_val_acc:\n",
    "        logger.info(\"New checkpoint\")\n",
    "        torch.save(\n",
    "            {\n",
    "                'bert_model': model.bert_model.state_dict(),\n",
    "                'classifier': model.classifier.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'epoch': trainer.state.epoch,\n",
    "            },\n",
    "            os.path.join(\n",
    "                ckpt_dir, 'checkpoint.pth'\n",
    "            )\n",
    "        )\n",
    "        log_training_results.best_val_acc = val_acc\n",
    "    scheduler.step()\n",
    "\n",
    "log_training_results.best_val_acc = 0\n",
    "trainer.run(loader['train'], max_epochs=nb_epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arguments:\n",
      "arguments:\n",
      "Namespace(batch_size=4, bert_init='roberta-base', bert_lr=1e-05, checkpoint_dir=None, dataset='R8', dropout=0.5, gcn_layers=2, gcn_lr=0.001, gcn_model='gcn', heads=8, m=0.7, max_length=128, n_hidden=200, nb_epochs=50, pretrained_bert_ckpt=None)\n",
      "Namespace(batch_size=4, bert_init='roberta-base', bert_lr=1e-05, checkpoint_dir=None, dataset='R8', dropout=0.5, gcn_layers=2, gcn_lr=0.001, gcn_model='gcn', heads=8, m=0.7, max_length=128, n_hidden=200, nb_epochs=50, pretrained_bert_ckpt=None)\n",
      "checkpoints will be saved in ./checkpoint/roberta-base_gcn_R8\n",
      "checkpoints will be saved in ./checkpoint/roberta-base_gcn_R8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph information:\n",
      "graph information:\n",
      "Graph(num_nodes=15362, num_edges=3504462,\n",
      "      ndata_schemes={'input_ids': Scheme(shape=(128,), dtype=torch.int64), 'attention_mask': Scheme(shape=(128,), dtype=torch.int64), 'label': Scheme(shape=(), dtype=torch.int64), 'train': Scheme(shape=(), dtype=torch.float32), 'val': Scheme(shape=(), dtype=torch.float32), 'test': Scheme(shape=(), dtype=torch.float32), 'label_train': Scheme(shape=(), dtype=torch.int64), 'cls_feats': Scheme(shape=(768,), dtype=torch.float32)}\n",
      "      edata_schemes={'edge_weight': Scheme(shape=(), dtype=torch.float32)})\n",
      "Graph(num_nodes=15362, num_edges=3504462,\n",
      "      ndata_schemes={'input_ids': Scheme(shape=(128,), dtype=torch.int64), 'attention_mask': Scheme(shape=(128,), dtype=torch.int64), 'label': Scheme(shape=(), dtype=torch.int64), 'train': Scheme(shape=(), dtype=torch.float32), 'val': Scheme(shape=(), dtype=torch.float32), 'test': Scheme(shape=(), dtype=torch.float32), 'label_train': Scheme(shape=(), dtype=torch.int64), 'cls_feats': Scheme(shape=(768,), dtype=torch.float32)}\n",
      "      edata_schemes={'edge_weight': Scheme(shape=(), dtype=torch.float32)})\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.50 GiB (GPU 0; 6.00 GiB total capacity; 2.74 GiB already allocated; 169.12 MiB free; 4.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 261>\u001B[1;34m()\u001B[0m\n\u001B[0;32m    258\u001B[0m         log_training_results\u001B[38;5;241m.\u001B[39mbest_val_acc \u001B[38;5;241m=\u001B[39m val_acc\n\u001B[0;32m    260\u001B[0m log_training_results\u001B[38;5;241m.\u001B[39mbest_val_acc \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m--> 261\u001B[0m g \u001B[38;5;241m=\u001B[39m \u001B[43mupdate_feature\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    262\u001B[0m trainer\u001B[38;5;241m.\u001B[39mrun(idx_loader, max_epochs\u001B[38;5;241m=\u001B[39mnb_epochs)\n",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36mupdate_feature\u001B[1;34m()\u001B[0m\n\u001B[0;32m    162\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, batch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataloader):\n\u001B[0;32m    163\u001B[0m     input_ids, attention_mask \u001B[38;5;241m=\u001B[39m [x\u001B[38;5;241m.\u001B[39mto(gpu) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m batch]\n\u001B[1;32m--> 164\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbert_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m][:, \u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    165\u001B[0m     cls_list\u001B[38;5;241m.\u001B[39mappend(output\u001B[38;5;241m.\u001B[39mcpu())\n\u001B[0;32m    166\u001B[0m cls_feat \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(cls_list, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\pygeo\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:848\u001B[0m, in \u001B[0;36mRobertaModel.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    839\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[0;32m    841\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[0;32m    842\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m    843\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    846\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[0;32m    847\u001B[0m )\n\u001B[1;32m--> 848\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    849\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    850\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    851\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    852\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    853\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    854\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    855\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    856\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    857\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    858\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    859\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    860\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    861\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\pygeo\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:524\u001B[0m, in \u001B[0;36mRobertaEncoder.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    515\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[0;32m    516\u001B[0m         create_custom_forward(layer_module),\n\u001B[0;32m    517\u001B[0m         hidden_states,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    521\u001B[0m         encoder_attention_mask,\n\u001B[0;32m    522\u001B[0m     )\n\u001B[0;32m    523\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 524\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    525\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    526\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    527\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    528\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    529\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    530\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    531\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    532\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    534\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    535\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\pygeo\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:451\u001B[0m, in \u001B[0;36mRobertaLayer.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    448\u001B[0m     cross_attn_present_key_value \u001B[38;5;241m=\u001B[39m cross_attention_outputs[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m    449\u001B[0m     present_key_value \u001B[38;5;241m=\u001B[39m present_key_value \u001B[38;5;241m+\u001B[39m cross_attn_present_key_value\n\u001B[1;32m--> 451\u001B[0m layer_output \u001B[38;5;241m=\u001B[39m \u001B[43mapply_chunking_to_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    452\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeed_forward_chunk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchunk_size_feed_forward\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mseq_len_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_output\u001B[49m\n\u001B[0;32m    453\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    454\u001B[0m outputs \u001B[38;5;241m=\u001B[39m (layer_output,) \u001B[38;5;241m+\u001B[39m outputs\n\u001B[0;32m    456\u001B[0m \u001B[38;5;66;03m# if decoder, return the attn key/values as the last output\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\pytorch_utils.py:243\u001B[0m, in \u001B[0;36mapply_chunking_to_forward\u001B[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001B[0m\n\u001B[0;32m    240\u001B[0m     \u001B[38;5;66;03m# concatenate output at same dimension\u001B[39;00m\n\u001B[0;32m    241\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcat(output_chunks, dim\u001B[38;5;241m=\u001B[39mchunk_dim)\n\u001B[1;32m--> 243\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minput_tensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:463\u001B[0m, in \u001B[0;36mRobertaLayer.feed_forward_chunk\u001B[1;34m(self, attention_output)\u001B[0m\n\u001B[0;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfeed_forward_chunk\u001B[39m(\u001B[38;5;28mself\u001B[39m, attention_output):\n\u001B[1;32m--> 463\u001B[0m     intermediate_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mintermediate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattention_output\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    464\u001B[0m     layer_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(intermediate_output, attention_output)\n\u001B[0;32m    465\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m layer_output\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\pygeo\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:362\u001B[0m, in \u001B[0;36mRobertaIntermediate.forward\u001B[1;34m(self, hidden_states)\u001B[0m\n\u001B[0;32m    360\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[0;32m    361\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdense(hidden_states)\n\u001B[1;32m--> 362\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mintermediate_act_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    363\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\pygeo\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\activations.py:56\u001B[0m, in \u001B[0;36mGELUActivation.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m---> 56\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mact\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 1.50 GiB (GPU 0; 6.00 GiB total capacity; 2.74 GiB already allocated; 169.12 MiB free; 4.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dgl\n",
    "import sys\n",
    "import torch\n",
    "import shutil\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator, Engine\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--max_length', type=int, default=128, help=\"the input length for bert\")\n",
    "parser.add_argument('--batch_size', type=int, default=4)\n",
    "parser.add_argument('-m', '--m', type=float, default=0.7, help=\"the factor balancing BERT and GCN prediction\")\n",
    "parser.add_argument('--nb_epochs', type=int, default=50)\n",
    "parser.add_argument('--bert_init', type=str, default='roberta-base', choices=['roberta-base', 'roberta-large', 'bert-base-uncased', 'bert-large-uncased'])\n",
    "parser.add_argument('--pretrained_bert_ckpt', default=None)\n",
    "parser.add_argument('--dataset', default='R8', choices=['20ng', 'R8', 'R52', 'ohsumed', 'mr'])\n",
    "parser.add_argument('--checkpoint_dir', default=None, help='checkpoint directory, [bert_init]_[gcn_model]_[dataset] if not specified')\n",
    "parser.add_argument('--gcn_model', type=str, default='gcn', choices=['gcn', 'gat'])\n",
    "parser.add_argument('--gcn_layers', type=int, default=2)\n",
    "parser.add_argument('--n_hidden', type=int, default=200, help='the dimension of gcn hidden layer, the dimension for gat is n_hidden * heads')\n",
    "parser.add_argument('--heads', type=int, default=8, help='the number of attention heads for gat')\n",
    "parser.add_argument('--dropout', type=float, default=0.5)\n",
    "parser.add_argument('--gcn_lr', type=float, default=1e-3)\n",
    "parser.add_argument('--bert_lr', type=float, default=1e-5)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "max_length = args.max_length\n",
    "batch_size = args.batch_size\n",
    "m = args.m\n",
    "nb_epochs = args.nb_epochs\n",
    "bert_init = args.bert_init\n",
    "pretrained_bert_ckpt = args.pretrained_bert_ckpt\n",
    "dataset = args.dataset\n",
    "checkpoint_dir = args.checkpoint_dir\n",
    "gcn_model = args.gcn_model\n",
    "gcn_layers = args.gcn_layers\n",
    "n_hidden = args.n_hidden\n",
    "heads = args.heads\n",
    "dropout = args.dropout\n",
    "gcn_lr = args.gcn_lr\n",
    "bert_lr = args.bert_lr\n",
    "\n",
    "if checkpoint_dir is None:\n",
    "    ckpt_dir = './checkpoint/{}_{}_{}'.format(bert_init, gcn_model, dataset)\n",
    "else:\n",
    "    ckpt_dir = checkpoint_dir\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "sh = logging.StreamHandler(sys.stdout)\n",
    "sh.setFormatter(logging.Formatter('%(message)s'))\n",
    "sh.setLevel(logging.INFO)\n",
    "fh = logging.FileHandler(filename=os.path.join(ckpt_dir, 'training.log'), mode='w')\n",
    "fh.setFormatter(logging.Formatter('%(message)s'))\n",
    "fh.setLevel(logging.INFO)\n",
    "logger = logging.getLogger('training logger')\n",
    "logger.addHandler(sh)\n",
    "logger.addHandler(fh)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "cpu = torch.device('cpu')\n",
    "gpu = torch.device('cuda:0')\n",
    "\n",
    "logger.info('arguments:')\n",
    "logger.info(str(args))\n",
    "logger.info('checkpoints will be saved in {}'.format(ckpt_dir))\n",
    "\n",
    "# Model\n",
    "\n",
    "# Data Preprocess\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size = load_corpus(dataset)\n",
    "'''\n",
    "adj: n*n sparse adjacency matrix\n",
    "y_train, y_val, y_test: n*c matrices\n",
    "train_mask, val_mask, test_mask: n-d bool array\n",
    "'''\n",
    "# compute number of real train/val/test/word nodes and number of classes\n",
    "nb_node = features.shape[0]\n",
    "nb_train, nb_val, nb_test = train_mask.sum(), val_mask.sum(), test_mask.sum()\n",
    "nb_word = nb_node - nb_train - nb_val - nb_test\n",
    "nb_class = y_train.shape[1]\n",
    "\n",
    "# instantiate model according to class number\n",
    "if gcn_model == \"gcn\":\n",
    "    model = BertGCN(nb_class=nb_class, pretrained_model=bert_init, m=m, gcn_layers=gcn_layers,\n",
    "                    n_hidden=n_hidden, dropout=dropout)\n",
    "else:\n",
    "    model = BertGAT(nb_class=nb_class, pretrained_model=bert_init, m=m, gcn_layers=gcn_layers,\n",
    "                    heads=heads, n_hidden=n_hidden, dropout=dropout)\n",
    "\n",
    "if pretrained_bert_ckpt is not None:\n",
    "    ckpt = torch.load(pretrained_bert_ckpt, map_location=gpu)\n",
    "    model.bert_model.load_state_dict(ckpt['bert_model'])\n",
    "    model.classifier.load_state_dict(ckpt['classifier'])\n",
    "\n",
    "# load documents and compute input encodings\n",
    "corpse_file = \"./data/corpus/\" + dataset + '_shuffle.txt'\n",
    "with open(corpse_file, 'r') as f:\n",
    "    text = f.read()\n",
    "    text = text.replace(\"\\\\\", '')\n",
    "    text = text.split(\"\\n\")\n",
    "\n",
    "def encode_input(text, tokenizer):\n",
    "    input = tokenizer(text, max_length=max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
    "    return input.input_ids, input.attention_mask\n",
    "\n",
    "input_ids, attention_mask = encode_input(text, model.tokenizer)\n",
    "input_ids = th.cat([input_ids[:-nb_test], th.zeros((nb_word, max_length), dtype=th.long), input_ids[-nb_test:]])\n",
    "attention_mask = th.cat([attention_mask[:-nb_test], th.zeros((nb_word, max_length), dtype=th.long), attention_mask[-nb_test:]])\n",
    "\n",
    "# transform one-hot label to class id for pytorch computation\n",
    "y = y_train + y_test + y_val\n",
    "y_train = y_train.argmax(axis=1)\n",
    "y = y.argmax(axis=1)\n",
    "\n",
    "# document mask used for update feature\n",
    "doc_mask = train_mask + val_mask + test_mask\n",
    "\n",
    "# build DGL Graph\n",
    "adj_norm = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "g = dgl.from_scipy(adj_norm.astype('float32'), eweight_name='edge_weight')\n",
    "g.ndata['input_ids'], g.ndata['attention_mask'] = input_ids, attention_mask\n",
    "g.ndata['label'], g.ndata['train'], g.ndata['val'], g.ndata['test'] =\\\n",
    "    th.LongTensor(y), th.FloatTensor(train_mask), th.FloatTensor(val_mask), th.FloatTensor(test_mask)\n",
    "g.ndata['label_train'] = th.LongTensor(y_train)\n",
    "g.ndata['cls_feats'] = th.zeros((nb_node, model.feat_dim))\n",
    "\n",
    "logger.info('graph information:')\n",
    "logger.info(str(g))\n",
    "\n",
    "# create index loader\n",
    "train_idx = Data.TensorDataset(torch.arange(0, nb_train, dtype=torch.long))\n",
    "val_idx = Data.TensorDataset(torch.arange(nb_train, nb_train+nb_val, dtype=torch.long))\n",
    "test_idx = Data.TensorDataset(torch.arange(nb_node-nb_test, nb_node, dtype=torch.long))\n",
    "doc_idx = Data.ConcatDataset([train_idx, val_idx, test_idx])\n",
    "\n",
    "idx_loader_train = Data.DataLoader(train_idx, batch_size=batch_size, shuffle=True)\n",
    "idx_loader_val = Data.DataLoader(val_idx, batch_size=batch_size)\n",
    "idx_loader_test = Data.DataLoader(test_idx, batch_size=batch_size)\n",
    "idx_loader = Data.DataLoader(doc_idx, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training\n",
    "def update_feature():\n",
    "    global model, g, doc_mask\n",
    "    # no gradient needed, uses a large batchsize to speed up the process\n",
    "    dataloader = Data.DataLoader(\n",
    "        Data.TensorDataset(g.ndata['input_ids'][doc_mask], g.ndata['attention_mask'][doc_mask]),\n",
    "        batch_size=1024\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        model = model.to(gpu)\n",
    "        model.eval()\n",
    "        cls_list = []\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            input_ids, attention_mask = [x.to(gpu) for x in batch]\n",
    "            output = model.bert_model(input_ids=input_ids, attention_mask=attention_mask)[0][:, 0]\n",
    "            cls_list.append(output.cpu())\n",
    "        cls_feat = torch.cat(cls_list, axis=0)\n",
    "    g = g.to(cpu)\n",
    "    g.ndata['cls_feats'][doc_mask] = cls_feat\n",
    "    return g\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.bert_model.parameters(), 'lr': bert_lr},\n",
    "    {'params': model.classifier.parameters(), 'lr': bert_lr},\n",
    "    {'params': model.gcn.parameters(), 'lr': gcn_lr},\n",
    "], lr=1e-3)\n",
    "\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[30], gamma=0.1)\n",
    "\n",
    "def train_step(engine, batch):\n",
    "    global model, g, optimizer\n",
    "    model.train()\n",
    "    model = model.to(gpu)\n",
    "    g = g.to(gpu)\n",
    "    optimizer.zero_grad()\n",
    "    train_mask = g.ndata['train'][idx].type(torch.BoolTensor)\n",
    "    y_pred = model(g, idx)[train_mask]\n",
    "    y_true = g.ndata['label_train'][idx][train_mask]\n",
    "    loss = F.nll_loss(y_pred, y_true)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    g.ndata['cls_feats'].detach_()\n",
    "    train_loss = loss.item()\n",
    "    with torch.no_grad():\n",
    "        if train_mask.sum() > 0:\n",
    "            y_true = y_true.detach().cpu()\n",
    "            y_pred = y_pred.argmax(axis=1).detach().cpu()\n",
    "            train_acc = accuracy_score(y_true, y_pred)\n",
    "        else:\n",
    "            train_acc = 1\n",
    "    return train_loss, train_acc\n",
    "\n",
    "trainer = Engine(train_step)\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def reset_graph(trainer):\n",
    "    scheduler.step()\n",
    "    update_feature()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def test_step(engine, batch):\n",
    "    global model, g\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model = model.to(gpu)\n",
    "        g = g.to(gpu)\n",
    "        (idx, ) = [x.to(gpu) for x in batch]\n",
    "        y_pred = model(g, idx)\n",
    "        y_true = g.ndata['label'][idx]\n",
    "        return y_pred, y_true\n",
    "\n",
    "evalutor = Engine(test_step)\n",
    "metrics={\n",
    "    'acc': Accuracy(),\n",
    "    'nll': Loss(torch.nn.NLLLoss())\n",
    "}\n",
    "for n, f in metrics.items():\n",
    "    f.attach(evaluator, n)\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    evaluator.run(idx_loader_train)\n",
    "    metrics = evaluator.state.metrics\n",
    "    train_acc, train_nll = metrics[\"acc\"], metrics[\"nll\"]\n",
    "    evaluator.run(idx_loader_val)\n",
    "    metrics = evaluator.state.metrics\n",
    "    val_acc, val_nll = metrics[\"acc\"], metrics[\"nll\"]\n",
    "    evaluator.run(idx_loader_test)\n",
    "    metrics = evaluator.state.metrics\n",
    "    test_acc, test_nll = metrics[\"acc\"], metrics[\"nll\"]\n",
    "    logger.info(\n",
    "        \"Epoch: {}  Train acc: {:.4f} loss: {:.4f}  Val acc: {:.4f} loss: {:.4f}  Test acc: {:.4f} loss: {:.4f}\"\n",
    "        .format(trainer.state.epoch, train_acc, train_nll, val_acc, val_nll, test_acc, test_nll)\n",
    "    )\n",
    "    if val_acc > log_training_results.best_val_acc:\n",
    "        logger.info(\"New checkpoint\")\n",
    "        torch.save(\n",
    "            {\n",
    "                'bert_model': model.bert_model.state_dict(),\n",
    "                'classifier': model.classifier.state_dict(),\n",
    "                'gcn': model.gcn.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'epoch': trainer.state.epoch,\n",
    "            },\n",
    "            os.path.join(\n",
    "                ckpt_dir, 'checkpoint.pth'\n",
    "            )\n",
    "        )\n",
    "        log_training_results.best_val_acc = val_acc\n",
    "\n",
    "log_training_results.best_val_acc = 0\n",
    "g = update_feature()\n",
    "trainer.run(idx_loader, max_epochs=nb_epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pygeo",
   "language": "python",
   "display_name": "pygeo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}